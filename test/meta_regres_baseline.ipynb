{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meta machine_learning baseline:\n",
    "\n",
    "1.针对回归\n",
    "\n",
    "2.包含逻辑回归，决策树，随机森林，xgboost模型\n",
    "\n",
    "3.可根据需求自动调整超参数\n",
    "\n",
    "待改进：\n",
    "1.加入特征工程\n",
    "3.加入回归分析\n",
    "4.加入神经网络方法(tensorflow框架)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以bmi预测为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.utils import column_or_1d\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalu(y_test, y_pred):\n",
    "    print(\"explained_variance_score:\", explained_variance_score(y_test,y_pred),\"越接近1越好\")\n",
    "    print(\"mean_absolute_error:\", mean_absolute_error(y_test,y_pred))\n",
    "    print(\"mean_squared_error:\", mean_squared_error(y_test,y_pred))\n",
    "    print(\"median_absolute_error:\", median_absolute_error(y_test,y_pred))\n",
    "    print(\"r2_score:\", r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_table(\"./merge.metaphlan_tables.tree.merge.metadata.new.noLD16_2\",index_col=0,header='infer')\n",
    "data = pd.read_table(\"./merge.metaphlan_tables.tree.merge.metadata.new.noLD16_2\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>812</th>\n",
       "      <th>813</th>\n",
       "      <th>814</th>\n",
       "      <th>815</th>\n",
       "      <th>816</th>\n",
       "      <th>817</th>\n",
       "      <th>818</th>\n",
       "      <th>819</th>\n",
       "      <th>820</th>\n",
       "      <th>821</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>project</td>\n",
       "      <td>age</td>\n",
       "      <td>bmi</td>\n",
       "      <td>gender</td>\n",
       "      <td>whr</td>\n",
       "      <td>dis_CRC</td>\n",
       "      <td>dis_HBV</td>\n",
       "      <td>dis_T2D</td>\n",
       "      <td>Cholesterol</td>\n",
       "      <td>TG</td>\n",
       "      <td>...</td>\n",
       "      <td>k__Viruses|p__Viruses_noname|c__Viruses_noname...</td>\n",
       "      <td>k__Viruses|p__Viruses_noname|c__Viruses_noname...</td>\n",
       "      <td>k__Viruses|p__Viruses_noname|c__Viruses_noname...</td>\n",
       "      <td>k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactob...</td>\n",
       "      <td>k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactob...</td>\n",
       "      <td>k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactob...</td>\n",
       "      <td>k__Bacteria|p__Proteobacteria|c__Gammaproteoba...</td>\n",
       "      <td>k__Bacteria|p__Proteobacteria|c__Gammaproteoba...</td>\n",
       "      <td>k__Bacteria|p__Candidatus_Saccharibacteria|c__...</td>\n",
       "      <td>k__Bacteria|p__Candidatus_Saccharibacteria|c__...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>austria.crc.advanced_adenoma</td>\n",
       "      <td>78</td>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "      <td>0.87</td>\n",
       "      <td>advanced_adenoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>austria.crc.advanced_adenoma</td>\n",
       "      <td>48</td>\n",
       "      <td>25.61</td>\n",
       "      <td>male</td>\n",
       "      <td>0.87</td>\n",
       "      <td>advanced_adenoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>austria.crc.advanced_adenoma</td>\n",
       "      <td>67</td>\n",
       "      <td>27.14</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>advanced_adenoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>137</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>austria.crc.advanced_adenoma</td>\n",
       "      <td>61</td>\n",
       "      <td>22.8</td>\n",
       "      <td>female</td>\n",
       "      <td>0.88</td>\n",
       "      <td>advanced_adenoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.08264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 822 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0    1      2       3     4                 5    \\\n",
       "0                       project  age    bmi  gender   whr           dis_CRC   \n",
       "1  austria.crc.advanced_adenoma   78     24  female  0.87  advanced_adenoma   \n",
       "2  austria.crc.advanced_adenoma   48  25.61    male  0.87  advanced_adenoma   \n",
       "3  austria.crc.advanced_adenoma   67  27.14    male   NaN  advanced_adenoma   \n",
       "4  austria.crc.advanced_adenoma   61   22.8  female  0.88  advanced_adenoma   \n",
       "\n",
       "       6        7            8    9    \\\n",
       "0  dis_HBV  dis_T2D  Cholesterol   TG   \n",
       "1      NaN      NaN          NaN  144   \n",
       "2      NaN      NaN          NaN   75   \n",
       "3      NaN      NaN          NaN  137   \n",
       "4      NaN      NaN          NaN   82   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "2                        ...                           \n",
       "3                        ...                           \n",
       "4                        ...                           \n",
       "\n",
       "                                                 812  \\\n",
       "0  k__Viruses|p__Viruses_noname|c__Viruses_noname...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "                                                 813  \\\n",
       "0  k__Viruses|p__Viruses_noname|c__Viruses_noname...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "                                                 814  \\\n",
       "0  k__Viruses|p__Viruses_noname|c__Viruses_noname...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "                                                 815  \\\n",
       "0  k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactob...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "                                                 816  \\\n",
       "0  k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactob...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "                                                 817  \\\n",
       "0  k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactob...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                            0.08264   \n",
       "\n",
       "                                                 818  \\\n",
       "0  k__Bacteria|p__Proteobacteria|c__Gammaproteoba...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "                                                 819  \\\n",
       "0  k__Bacteria|p__Proteobacteria|c__Gammaproteoba...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "                                                 820  \\\n",
       "0  k__Bacteria|p__Candidatus_Saccharibacteria|c__...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "                                                 821  \n",
       "0  k__Bacteria|p__Candidatus_Saccharibacteria|c__...  \n",
       "1                                                  0  \n",
       "2                                                  0  \n",
       "3                                                  0  \n",
       "4                                            0.00051  \n",
       "\n",
       "[5 rows x 822 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"===Done!\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_title_index=8\n",
    "y_title = data.loc[0,y_title_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df=data.loc[1:,31:]\n",
    "y_df=data.loc[1:,y_title_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_x_y(x_df, y_df):\n",
    "    y_data=y_df[y_df.isnull().values==False]\n",
    "    x_tmp=x_df.loc[y_df.isnull().values==False,:]\n",
    "    x_tmp=pd.DataFrame(x_tmp,dtype=np.float)\n",
    "    x_data=x_tmp.loc[:,(x_tmp==0).sum(axis=0)/x_tmp.shape[0]<0.5]\n",
    "    if len(y_data.unique()) * 20 > x_data.shape[0]:\n",
    "        lable_type = \"regress\"\n",
    "    else:\n",
    "        lable_type = \"classify\"\n",
    "    if lable_type == \"regress\":\n",
    "        y_data = pd.DataFrame(y_data,dtype=np.float)\n",
    "    return x_data, y_data, lable_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data, lable_type= filter_x_y(x_df, y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129c03278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#pyplot.figure(figsize=(15.0, 10.0))\n",
    "if lable_type == \"classify\":   \n",
    "    sns.countplot(y_data)\n",
    "    pyplot.xlabel(y_title+\" sample\");\n",
    "    pyplot.savefig(\"./temp.png\")\n",
    "else:\n",
    "    sns.distplot(y_data, hist=True, kde=True)\n",
    "    pyplot.xlabel(y_title+\" sample\");\n",
    "    pyplot.savefig(\"./temp.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=column_or_1d(y_train, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelencoder=LabelEncoder()\n",
    "# labelencoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: 0.04833719130325154 越接近1越好\n",
      "mean_absolute_error: 7.3562821474942295\n",
      "mean_squared_error: 96.311764999435\n",
      "median_absolute_error: 6.020542144775391\n",
      "r2_score: -0.008609261953968161\n"
     ]
    }
   ],
   "source": [
    "evalu(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: -0.5598673067283451 越接近1越好\n",
      "mean_absolute_error: 1.0570588235294118\n",
      "mean_squared_error: 1.8553235294117647\n",
      "median_absolute_error: 0.8399999999999999\n",
      "r2_score: -0.6106079522093957\n"
     ]
    }
   ],
   "source": [
    "y_pred = tree_model.predict(X_test)\n",
    "tree_model.score(X_test, y_pred)\n",
    "evalu(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model2 = DecisionTreeRegressor()\n",
    "tuned_parameters= { 'max_features': [\"auto\",\"sqrt\",\"log2\"],\n",
    "                  'min_samples_leaf': range(1,10,1) , 'max_depth': range(1,10,1)\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "TM = GridSearchCV(tree_model2, tuned_parameters,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_features': ['auto', 'sqrt', 'log2'], 'min_samples_leaf': range(1, 10), 'max_depth': range(1, 10)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 1, 'max_features': 'log2', 'min_samples_leaf': 7}\n"
     ]
    }
   ],
   "source": [
    "print(TM.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: -0.5598673067283451 越接近1越好\n",
      "mean_absolute_error: 1.0570588235294118\n",
      "mean_squared_error: 1.8553235294117647\n",
      "median_absolute_error: 0.8399999999999999\n",
      "r2_score: -0.6106079522093957\n"
     ]
    }
   ],
   "source": [
    "y_pred = tree_model.predict(X_test)\n",
    "tree_model.score(X_test, y_pred)\n",
    "evalu(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_model=RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: -0.5598673067283451 越接近1越好\n",
      "mean_absolute_error: 1.0570588235294118\n",
      "mean_squared_error: 1.8553235294117647\n",
      "median_absolute_error: 0.8399999999999999\n",
      "r2_score: -0.6106079522093957\n"
     ]
    }
   ],
   "source": [
    "y_pred = tree_model.predict(X_test)\n",
    "tree_model.score(X_test, y_pred)\n",
    "evalu(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = {'min_samples_leaf': range(10,100,10), 'n_estimators' : range(10,100,10),\n",
    "                    'max_features':['auto','sqrt','log2']\n",
    "                    }\n",
    "RR = GridSearchCV(rr_model, tuned_parameters,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'min_samples_leaf': range(10, 100, 10), 'n_estimators': range(10, 100, 10), 'max_features': ['auto', 'sqrt', 'log2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RR.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: -0.5598673067283451 越接近1越好\n",
      "mean_absolute_error: 1.0570588235294118\n",
      "mean_squared_error: 1.8553235294117647\n",
      "median_absolute_error: 0.8399999999999999\n",
      "r2_score: -0.6106079522093957\n"
     ]
    }
   ],
   "source": [
    "y_pred = tree_model.predict(X_test)\n",
    "tree_model.score(X_test, y_pred)\n",
    "evalu(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import preprocessing\n",
    "#lbl = preprocessing.LabelEncoder()\n",
    "#train_x['acc_id1'] = lbl.fit_transform(train_x['acc_id1'].astype(str))#将提示的包含错误数据类型这一列进行转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "XGB_model=XGBRegressor(max_depth=5,objective='reg:gamma')  \n",
    "# XGB_model=XGBClassifier(objective= 'multi:softprob') # 多分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class XGBRegressor in module xgboost.sklearn:\n",
      "\n",
      "class XGBRegressor(XGBModel, sklearn.base.RegressorMixin)\n",
      " |  Implementation of the scikit-learn API for XGBoost regression.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  max_depth : int\n",
      " |      Maximum tree depth for base learners.\n",
      " |  learning_rate : float\n",
      " |      Boosting learning rate (xgb's \"eta\")\n",
      " |  n_estimators : int\n",
      " |      Number of boosted trees to fit.\n",
      " |  silent : boolean\n",
      " |      Whether to print messages while running boosting.\n",
      " |  objective : string or callable\n",
      " |      Specify the learning task and the corresponding learning objective or\n",
      " |      a custom objective function to be used (see note below).\n",
      " |  booster: string\n",
      " |      Specify which booster to use: gbtree, gblinear or dart.\n",
      " |  nthread : int\n",
      " |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      " |  n_jobs : int\n",
      " |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      " |  gamma : float\n",
      " |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      " |  min_child_weight : int\n",
      " |      Minimum sum of instance weight(hessian) needed in a child.\n",
      " |  max_delta_step : int\n",
      " |      Maximum delta step we allow each tree's weight estimation to be.\n",
      " |  subsample : float\n",
      " |      Subsample ratio of the training instance.\n",
      " |  colsample_bytree : float\n",
      " |      Subsample ratio of columns when constructing each tree.\n",
      " |  colsample_bylevel : float\n",
      " |      Subsample ratio of columns for each split, in each level.\n",
      " |  reg_alpha : float (xgb's alpha)\n",
      " |      L1 regularization term on weights\n",
      " |  reg_lambda : float (xgb's lambda)\n",
      " |      L2 regularization term on weights\n",
      " |  scale_pos_weight : float\n",
      " |      Balancing of positive and negative weights.\n",
      " |  base_score:\n",
      " |      The initial prediction score of all instances, global bias.\n",
      " |  seed : int\n",
      " |      Random number seed.  (Deprecated, please use random_state)\n",
      " |  random_state : int\n",
      " |      Random number seed.  (replaces seed)\n",
      " |  missing : float, optional\n",
      " |      Value in the data which needs to be present as a missing value. If\n",
      " |      None, defaults to np.nan.\n",
      " |  importance_type: string, default \"gain\"\n",
      " |      The feature importance type for the feature_importances_ property: either \"gain\",\n",
      " |      \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      " |  \\*\\*kwargs : dict, optional\n",
      " |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      " |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      " |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      " |      will result in a TypeError.\n",
      " |  \n",
      " |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      " |  \n",
      " |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      " |          passed via this argument will interact properly with scikit-learn.\n",
      " |  \n",
      " |  Note\n",
      " |  ----\n",
      " |  A custom objective function can be provided for the ``objective``\n",
      " |  parameter. In this case, it should have the signature\n",
      " |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      " |  \n",
      " |  y_true: array_like of shape [n_samples]\n",
      " |      The target values\n",
      " |  y_pred: array_like of shape [n_samples]\n",
      " |      The predicted values\n",
      " |  \n",
      " |  grad: array_like of shape [n_samples]\n",
      " |      The value of the gradient for each sample point.\n",
      " |  hess: array_like of shape [n_samples]\n",
      " |      The value of the second derivative for each sample point\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      XGBRegressor\n",
      " |      XGBModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods inherited from XGBModel:\n",
      " |  \n",
      " |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, importance_type='gain', **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  apply(self, X, ntree_limit=0)\n",
      " |      Return the predicted leaf every tree for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      " |          For each datapoint x in X and for each tree, return the index of the\n",
      " |          leaf x ends up in. Leaves are numbered within\n",
      " |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      " |  \n",
      " |  evals_result(self)\n",
      " |      Return the evaluation results.\n",
      " |      \n",
      " |      If **eval_set** is passed to the `fit` function, you can call\n",
      " |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      " |      When **eval_metric** is also passed to the `fit` function, the\n",
      " |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      evals_result : dictionary\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      " |      \n",
      " |          clf = xgb.XGBModel(**param_dist)\n",
      " |      \n",
      " |          clf.fit(X_train, y_train,\n",
      " |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      " |                  eval_metric='logloss',\n",
      " |                  verbose=True)\n",
      " |      \n",
      " |          evals_result = clf.evals_result()\n",
      " |      \n",
      " |      The variable **evals_result** will contain:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      " |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)\n",
      " |      Fit the gradient boosting model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Feature matrix\n",
      " |      y : array_like\n",
      " |          Labels\n",
      " |      sample_weight : array_like\n",
      " |          instance weights\n",
      " |      eval_set : list, optional\n",
      " |          A list of (X, y) tuple pairs to use as a validation set for\n",
      " |          early-stopping\n",
      " |      sample_weight_eval_set : list, optional\n",
      " |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      " |          instance weights on the i-th validation set.\n",
      " |      eval_metric : str, callable, optional\n",
      " |          If a str, should be a built-in evaluation metric to use. See\n",
      " |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      " |          signature is func(y_predicted, y_true) where y_true will be a\n",
      " |          DMatrix object such that you may need to call the get_label\n",
      " |          method. It must return a str, value pair where the str is a name\n",
      " |          for the evaluation and value is the value of the evaluation\n",
      " |          function. This objective is always minimized.\n",
      " |      early_stopping_rounds : int\n",
      " |          Activates early stopping. Validation error needs to decrease at\n",
      " |          least every <early_stopping_rounds> round(s) to continue training.\n",
      " |          Requires at least one item in evals.  If there's more than one,\n",
      " |          will use the last. Returns the model from the last iteration\n",
      " |          (not the best one). If early stopping occurs, the model will\n",
      " |          have three additional fields: bst.best_score, bst.best_iteration\n",
      " |          and bst.best_ntree_limit.\n",
      " |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      " |          and/or num_class appears in the parameters)\n",
      " |      verbose : bool\n",
      " |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      " |          metric measured on the validation set to stderr.\n",
      " |      xgb_model : str\n",
      " |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      " |          loaded before training (allows training continuation).\n",
      " |      callbacks : list of callback functions\n",
      " |          List of callback functions that are applied at end of each iteration.\n",
      " |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      " |          Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      " |  \n",
      " |  get_booster(self)\n",
      " |      Get the underlying xgboost Booster of this model.\n",
      " |      \n",
      " |      This will raise an exception when fit was not called\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      booster : a xgboost booster of underlying model\n",
      " |  \n",
      " |  get_params(self, deep=False)\n",
      " |      Get parameters.\n",
      " |  \n",
      " |  get_xgb_params(self)\n",
      " |      Get xgboost type parameters.\n",
      " |  \n",
      " |  load_model(self, fname)\n",
      " |      Load the model from a file.\n",
      " |      \n",
      " |      The model is loaded from an XGBoost internal binary format which is\n",
      " |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      " |      the Python Booster object (such as feature names) will not be loaded.\n",
      " |      Label encodings (text labels to numeric labels) will be also lost.\n",
      " |      **If you are using only the Python interface, we recommend pickling the\n",
      " |      model object for best results.**\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or a memory buffer\n",
      " |          Input file name or memory buffer(see also save_raw)\n",
      " |  \n",
      " |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)\n",
      " |      Predict with `data`.\n",
      " |      \n",
      " |      .. note:: This function is not thread safe.\n",
      " |      \n",
      " |        For each booster object, predict can only be called from one thread.\n",
      " |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      " |        of model object and then call ``predict()``.\n",
      " |      \n",
      " |      .. note:: Using ``predict()`` with DART booster\n",
      " |      \n",
      " |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      " |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      " |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      " |        a nonzero value, e.g.\n",
      " |      \n",
      " |        .. code-block:: python\n",
      " |      \n",
      " |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : DMatrix\n",
      " |          The dmatrix storing the input.\n",
      " |      output_margin : bool\n",
      " |          Whether to output the raw untransformed margin value.\n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      " |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      " |      validate_features : bool\n",
      " |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      " |          Otherwise, it is assumed that the feature_names are the same.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : numpy array\n",
      " |  \n",
      " |  save_model(self, fname)\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      The model is saved in an XGBoost internal binary format which is\n",
      " |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      " |      the Python Booster object (such as feature names) will not be loaded.\n",
      " |      Label encodings (text labels to numeric labels) will be also lost.\n",
      " |      **If you are using only the Python interface, we recommend pickling the\n",
      " |      model object for best results.**\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Output file name\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      " |      the full range of xgboost parameters that are not defined as member variables\n",
      " |      in sklearn grid search.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from XGBModel:\n",
      " |  \n",
      " |  coef_\n",
      " |      Coefficients property\n",
      " |      \n",
      " |      .. note:: Coefficients are defined only for linear learners\n",
      " |      \n",
      " |          Coefficients are only defined when the linear model is chosen as base\n",
      " |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      " |          as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Feature importances property\n",
      " |      \n",
      " |      .. note:: Feature importance is defined only for tree boosters\n",
      " |      \n",
      " |          Feature importance is only defined when the decision tree model is chosen as base\n",
      " |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      " |          as linear learners (`booster=gblinear`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array of shape ``[n_features]``\n",
      " |  \n",
      " |  intercept_\n",
      " |      Intercept (bias) property\n",
      " |      \n",
      " |      .. note:: Intercept is defined only for linear learners\n",
      " |      \n",
      " |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      " |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      " |          as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(XGBRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, importance_type='gain',\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='reg:gamma', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: -0.19059431799435478 越接近1越好\n",
      "mean_absolute_error: 0.9234981486376594\n",
      "mean_squared_error: 1.4469258676091545\n",
      "median_absolute_error: 0.9356154060363768\n",
      "r2_score: -0.25607759061173163\n"
     ]
    }
   ],
   "source": [
    "y_pred = XGB_model.predict(X_test)\n",
    "XGB_model.score(X_test, y_pred)\n",
    "evalu(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00153561 0.00527742 0.01120412 0.02320008 0.00365687 0.00863486\n",
      " 0.0315629  0.02448498 0.00824199 0.01315442 0.02825086 0.00389032\n",
      " 0.00782227 0.02494201 0.00560086 0.03832502 0.00326854 0.00961272\n",
      " 0.02290437 0.02951537 0.00982624 0.02673565 0.00667023 0.01993603\n",
      " 0.02356114 0.00302439 0.01299371 0.01602833 0.00965167 0.01373242\n",
      " 0.00233139 0.0248308  0.02015337 0.00485    0.0009356  0.01589436\n",
      " 0.02464838 0.02012677 0.02653887 0.03397303 0.01272395 0.01324642\n",
      " 0.00488051 0.00677778 0.00993506 0.02151003 0.02870334 0.06596873\n",
      " 0.00670753 0.01223959 0.00942217 0.01191725 0.01335743 0.00804259\n",
      " 0.02240744 0.01190659 0.0068285  0.02663179 0.03610566 0.01109584\n",
      " 0.00178001 0.00620313 0.01693356 0.01314718]\n"
     ]
    }
   ],
   "source": [
    "print(XGB_model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEWdJREFUeJzt3H+o3Xd9x/Hny1uLrorRNZPQpEsGQQmy/iDEiCKzoiRR\nzD/7owWtK45Qlg4FwcUNBv7Xv0QLJSGr1RWdRfyxXWqw1F+IsGpSrbVpmnmXdSQhLpFh3SzYRd/7\n43yDx2Pa+7255957zvk8H3C45/v5fL73vr/3x+t+zuf7Pd9UFZKkdrxkrQuQJK0ug1+SGmPwS1Jj\nDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUmKvWuoDLufbaa2vz5s1rXYYkTY3HHnvsZ1W1vs/Y\niQz+zZs3c+zYsbUuQ5KmRpL/7DvWpR5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtS\nYwx+SWrMRL5zV9J02nzgq7+z/czd71qjSvRinPFLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8\nktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMb0Cv4ku5KcTLKQ5MBl+pPknq7/iSQ3\nD/WtS/LFJE8nOZHkTeM8AEnS0iwa/EnmgHuB3cA24LYk20aG7Qa2do99wMGhvk8CX6uq1wM3ACfG\nULck6Qr1mfHvABaq6lRVPQ88COwdGbMXeKAGHgXWJdmQ5FXAW4FPAVTV81X18zHWL0laoj7Bfx1w\nemj7TNfWZ8wW4ALw6SQ/THJfkmsu90WS7EtyLMmxCxcu9D4ASdLSrPTJ3auAm4GDVXUT8Evg984R\nAFTV4araXlXb169fv8JlSVK7+gT/WWDT0PbGrq3PmDPAmar6Xtf+RQb/CCRJa6RP8B8FtibZkuRq\n4FZgfmTMPHB7d3XPTuDZqjpXVT8FTid5XTfu7cBT4ypekrR0Vy02oKouJrkLeBiYA+6vquNJ7uz6\nDwFHgD3AAvAccMfQp/hr4HPdP41TI32SpFW2aPADVNURBuE+3HZo6HkB+19g38eB7cuoUZI0Rr5z\nV5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfgl\nqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNaZX8CfZleRkkoUkBy7T\nnyT3dP1PJLl5qO+ZJD9O8niSY+MsXpK0dFctNiDJHHAv8A7gDHA0yXxVPTU0bDewtXu8ETjYfbzk\nbVX1s7FVLUm6Yn1m/DuAhao6VVXPAw8Ce0fG7AUeqIFHgXVJNoy5VknSGPQJ/uuA00PbZ7q2vmMK\n+HqSx5Lsu9JCJUnjsehSzxi8parOJvkj4JEkT1fVd0YHdf8U9gFcf/31q1CWJLWpz4z/LLBpaHtj\n19ZrTFVd+nge+AqDpaPfU1WHq2p7VW1fv359v+olSUvWJ/iPAluTbElyNXArMD8yZh64vbu6Zyfw\nbFWdS3JNklcCJLkGeCfw5BjrlyQt0aJLPVV1McldwMPAHHB/VR1PcmfXfwg4AuwBFoDngDu63V8L\nfCXJpa/1T1X1tbEfhSSpt15r/FV1hEG4D7cdGnpewP7L7HcKuGGZNUqSxsh37kpSYwx+SWqMwS9J\njTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQY\ng1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUmF7Bn2RXkpNJFpIcuEx/ktzT9T+R\n5OaR/rkkP0zy0LgKlyRdmUWDP8kccC+wG9gG3JZk28iw3cDW7rEPODjS/0HgxLKrlSQtW58Z/w5g\noapOVdXzwIPA3pExe4EHauBRYF2SDQBJNgLvAu4bY92SpCvUJ/ivA04PbZ/p2vqO+QTwEeA3L/ZF\nkuxLcizJsQsXLvQoS5J0JVb05G6SdwPnq+qxxcZW1eGq2l5V29evX7+SZUlS0/oE/1lg09D2xq6t\nz5g3A+9J8gyDJaJbknz2iquVJC1bn+A/CmxNsiXJ1cCtwPzImHng9u7qnp3As1V1rqo+WlUbq2pz\nt983q+q94zwASdLSXLXYgKq6mOQu4GFgDri/qo4nubPrPwQcAfYAC8BzwB0rV7IkaTkWDX6AqjrC\nINyH2w4NPS9g/yKf49vAt5dcocZm84Gv/s72M3e/a40qkbSWfOeuJDXG4Jekxhj8ktQYg1+SGmPw\nS1JjDH5JakyvyzklqUWzegm0M35JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGC/nlKbQrF5mqNXh\njF+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMZ4Hf8yeT21pGnTa8afZFeSk0kWkhy4TH+S\n3NP1P5Hk5q79ZUm+n+RHSY4n+di4D0CStDSLBn+SOeBeYDewDbgtybaRYbuBrd1jH3Cwa/8VcEtV\n3QDcCOxKsnNMtUuSrkCfGf8OYKGqTlXV88CDwN6RMXuBB2rgUWBdkg3d9v92Y17aPWpcxUuSlq5P\n8F8HnB7aPtO19RqTZC7J48B54JGq+t6VlytJWq4Vv6qnqn5dVTcCG4EdSd5wuXFJ9iU5luTYhQsX\nVrosSWpWn6t6zgKbhrY3dm1LGlNVP0/yLWAX8OToF6mqw8BhgO3bt7scNOG8mkmaXn1m/EeBrUm2\nJLkauBWYHxkzD9zeXd2zE3i2qs4lWZ9kHUCSlwPvAJ4eY/2SpCVadMZfVReT3AU8DMwB91fV8SR3\ndv2HgCPAHmABeA64o9t9A/CP3ZVBLwG+UFUPjf8wJEl99XoDV1UdYRDuw22Hhp4XsP8y+z0B3LTM\nGiVNIJf7ppfv3J0g/iFJWg3eq0eSGmPwS1JjDH5JaozBL0mN8eSuVpQnrJfP7+HqGf1ezypn/JLU\nGINfkhpj8EtSYwx+SWqMwS9JjfGqnhnhlR8rY/j7uhLf05X+/NLlGPwrwBCWNMlc6pGkxhj8ktQY\nl3o0kVwuk1aOwd8xaCS1wuBXL159Is0Og1/STPHV++I8uStJjTH4JakxLvVIWpTLJ7Ol14w/ya4k\nJ5MsJDlwmf4kuafrfyLJzV37piTfSvJUkuNJPjjuA5AkLc2iwZ9kDrgX2A1sA25Lsm1k2G5ga/fY\nBxzs2i8CH66qbcBOYP9l9pUkraI+M/4dwEJVnaqq54EHgb0jY/YCD9TAo8C6JBuq6lxV/QCgqv4H\nOAFcN8b6JUlL1Cf4rwNOD22f4ffDe9ExSTYDNwHfW2qRkqTxWZWTu0leAXwJ+FBV/eIFxuxjsEzE\n9ddfvxplaYJMyxvEVvokpydRtRr6zPjPApuGtjd2bb3GJHkpg9D/XFV9+YW+SFUdrqrtVbV9/fr1\nfWqXJF2BPsF/FNiaZEuSq4FbgfmRMfPA7d3VPTuBZ6vqXJIAnwJOVNXHx1q5JOmKLLrUU1UXk9wF\nPAzMAfdX1fEkd3b9h4AjwB5gAXgOuKPb/c3A+4AfJ3m8a/vbqjoy3sOQJPXVa42/C+ojI22Hhp4X\nsP8y+30XyDJrlCSNkbdskKTGGPyS1BiDX5IaY/BLUmMMfklqjLdllqRVMinvUHfGL0mNccavmTMp\nsyppUhn80irxBmyaFC71SFJjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMZ4Oaea5KWVapnBL82oWfzn\nNovHtBYM/jU0+kuspTMINM3W6vfX4JdmhLeqUF8Gv6RV5z+ptWXwS5pI41wGcUnwd3k5pyQ1xuCX\npMb0WupJsgv4JDAH3FdVd4/0p+vfAzwH/EVV/aDrux94N3C+qt4wxtqvmC/7JF2pWciPRWf8SeaA\ne4HdwDbgtiTbRobtBrZ2j33AwaG+zwC7xlGsJGn5+iz17AAWqupUVT0PPAjsHRmzF3igBh4F1iXZ\nAFBV3wH+e5xFS5KuXJ/gvw44PbR9pmtb6pgXlWRfkmNJjl24cGEpu0qSlmBiLuesqsPAYYDt27fX\nGpejKzALa58a8F3ls61P8J8FNg1tb+zaljpGU2LWAnzWjmcW+TNaXX2Weo4CW5NsSXI1cCswPzJm\nHrg9AzuBZ6vq3JhrlSSNwaLBX1UXgbuAh4ETwBeq6niSO5Pc2Q07ApwCFoB/AP7q0v5JPg/8K/C6\nJGeSfGDMxyBJWoJea/xVdYRBuA+3HRp6XsD+F9j3tuUUOGm8x4ikaTcxJ3dnnWuYkiaFwT+F/Cci\n/ZavwpfOe/VIUmMMfklqjEs9kprU8pKpwa9V1/If3CRa6Z+HP+/JY/BL0gqY5H94rvFLUmOc8c8w\nL3OTdDlNBL8BKEm/1UTwSxqY5HVnrR6DX1omw1TTxpO7ktQYg1+SGuNSz4uYhJfwk1CDpNnijF+S\nGuOMv3HT9IpimmqVJpkzfklqjDN+qeMrCrXCGb8kNcYZvyQt07S9Wpy54J+2H4AkrbZeSz1JdiU5\nmWQhyYHL9CfJPV3/E0lu7ruvJGl1LTrjTzIH3Au8AzgDHE0yX1VPDQ3bDWztHm8EDgJv7LmvNHN8\n5alJ1mfGvwNYqKpTVfU88CCwd2TMXuCBGngUWJdkQ899JUmrqE/wXwecHto+07X1GdNnX0nSKkpV\nvfiA5M+BXVX1l932+4A3VtVdQ2MeAu6uqu92298A/gbYvNi+Q59jH7Cv23wdcHJ5h8a1wM+W+TnW\nyjTXDta/1qa5/mmuHda2/j+uqvV9Bva5qucssGloe2PX1mfMS3vsC0BVHQYO96inlyTHqmr7uD7f\naprm2sH619o01z/NtcP01N9nqecosDXJliRXA7cC8yNj5oHbu6t7dgLPVtW5nvtKklbRojP+qrqY\n5C7gYWAOuL+qjie5s+s/BBwB9gALwHPAHS+274ociSSpl15v4KqqIwzCfbjt0NDzAvb33XeVjG3Z\naA1Mc+1g/Wttmuuf5tphSupf9OSuJGm2eJM2SWrMzAX/tN0iIsn9Sc4neXKo7TVJHknyk+7jq9ey\nxheSZFOSbyV5KsnxJB/s2qel/pcl+X6SH3X1f6xrn4r6L0kyl+SH3WXVU1V/kmeS/DjJ40mOdW1T\nUX+SdUm+mOTpJCeSvGlaap+p4B+6RcRuYBtwW5Jta1vVoj4D7BppOwB8o6q2At/otifRReDDVbUN\n2Ans777f01L/r4BbquoG4EZgV3dV2rTUf8kHgRND29NW/9uq6sahyyCnpf5PAl+rqtcDNzD4GUxH\n7VU1Mw/gTcDDQ9sfBT661nX1qHsz8OTQ9klgQ/d8A3ByrWvseRz/wuC+TFNXP/AHwA8Y3Gtqaupn\n8N6YbwC3AA9N2+8P8Axw7UjbxNcPvAr4D7rzpNNUe1XN1oyf2blFxGtr8D4IgJ8Cr13LYvpIshm4\nCfgeU1R/t0zyOHAeeKSqpqp+4BPAR4DfDLVNU/0FfD3JY92792E66t8CXAA+3S2z3ZfkGqaj9pkL\n/plTg6nDRF96leQVwJeAD1XVL4b7Jr3+qvp1Vd3IYOa8I8kbRvontv4k7wbOV9VjLzRmkuvvvKX7\n/u9msFT41uHOCa7/KuBm4GBV3QT8kpFlnQmufeaCv8/tJabBf3V3N6X7eH6N63lBSV7KIPQ/V1Vf\n7pqnpv5LqurnwLcYnG+ZlvrfDLwnyTMM7nx7S5LPMj31U1Vnu4/nga8wuKPvNNR/BjjTvUIE+CKD\nfwTTUPvMBf+s3CJiHnh/9/z9DNbOJ06SAJ8CTlTVx4e6pqX+9UnWdc9fzuD8xNNMSf1V9dGq2lhV\nmxn8rn+zqt7LlNSf5Jokr7z0HHgn8CRTUH9V/RQ4neR1XdPbgaeYgtqB2Tq5251Q2QP8G/DvwN+t\ndT096v08cA74PwaziA8Af8jghN1PgK8Dr1nrOl+g9rcweCn7BPB499gzRfX/KfDDrv4ngb/v2qei\n/pFj+TN+e3J3KuoH/gT4Ufc4funvdYrqvxE41v3+/DPw6mmp3XfuSlJjZm2pR5K0CINfkhpj8EtS\nYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TG/D9/nXmnR8DXtAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x128f62780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "from matplotlib import pyplot\n",
    "pyplot.bar(range(len(XGB_model.feature_importances_)), XGB_model.feature_importances_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXd4lFXah+8nJEAg0qQTunSUpsJ+KmKDdUXsImJFXFAX\nRUSEVQRsNFGEVUEFQUBRXFxsoCjEXhABxQKo9F4DCSXt+f543oQxTgowk8nMnPu65mLmLec9P0bn\ncJ4qqorD4XA4HLmJCfUEHA6Hw1E8cQuEw+FwOPziFgiHw+Fw+MUtEA6Hw+Hwi1sgHA6Hw+EXt0A4\nHA6Hwy9ugXA4jgMRmSQiQ0M9D4cjmIjLg3AUJSKyDqgGZPocbqyqW05gzE7ATFVNPLHZhSciMg3Y\npKoPhXoujsjC7SAcoeBSVU3weR334hAIRCQ2lM8/EUSkRKjn4Ihc3ALhKDaISAcR+VJE9onICm9n\nkH3uVhH5RUQOiMgfItLHO14WmA/UFJEU71VTRKaJyGM+93cSkU0+n9eJyAMi8gOQKiKx3n3/FZGd\nIrJWRO7OZ64542ePLSKDRGSHiGwVkctF5B8islpE9ojIv33uHS4ib4rI656e70Wklc/5ZiKS5P09\n/CQi3XI993kReV9EUoHbgJ7AIE/7O951g0Xkd2/8n0XkCp8xbhGRz0XkSRHZ62m92Od8JRF5WUS2\neOf/53Ouq4gs9+b2pYicVugv2BF2uAXCUSwQkVrAe8BjQCVgIPBfEaniXbID6AqUA24FnhaRtqqa\nClwMbDmOHUkP4BKgApAFvAOsAGoBFwD9RaRLIceqDpT27n0YeBG4AWgHnAMMFZH6PtdfBszxtL4K\n/E9E4kQkzpvHh0BVoB8wS0Sa+Nx7PfA4cBLwCjALGONpv9S75nfvueWBEcBMEanhM0Z7YBVQGRgD\nTBER8c7NAMoALbw5PA0gIm2AqUAf4GRgMvC2iJQq5N+RI8xwC4QjFPzP+xfoPp9/nd4AvK+q76tq\nlqouBL4D/gGgqu+p6u9qfIL9gJ5zgvOYoKobVfUQcAZQRVUfUdU0Vf0D+5G/rpBjpQOPq2o6MBv7\n4X1GVQ+o6k/Az0Arn+uXquqb3vVPYYtLB++VAIzy5rEIeBdbzLKZp6pfeH9Ph/1NRlXnqOoW75rX\ngTXAmT6XrFfVF1U1E5gO1ACqeYvIxUBfVd2rqune3zfAP4HJqvqNqmaq6nTgiDdnRwQStrZXR1hz\nuap+lOtYXeAaEbnU51gcsBjAM4EMAxpj/7ApA/x4gvPYmOv5NUVkn8+xEsBnhRxrt/djC3DI+3O7\nz/lD2A//X56tqlme+atm9jlVzfK5dj22M/E3b7+IyE3AAKCedygBW7Sy2ebz/IPe5iEB29HsUdW9\nfoatC9wsIv18jpX0mbcjwnALhKO4sBGYoaq35z7hmTD+C9yE/es53dt5ZJtE/IXipWKLSDbV/Vzj\ne99GYK2qNjqeyR8HtbPfiEgMkAhkm8Zqi0iMzyJRB1jtc29uvX/6LCJ1sd3PBcBXqpopIss5+veV\nHxuBSiJSQVX3+Tn3uKo+XohxHBGAMzE5igszgUtFpIuIlBCR0p7zNxH7V2opYCeQ4e0mOvvcux04\nWUTK+xxbDvzDc7hWB/oX8PxvgQOe4zrem0NLETkjYAr/TDsRudKLoOqPmWq+Br4BDmJO5zjPUX8p\nZrbKi+1AA5/PZbFFYyeYgx9oWZhJqepWzOn/nIhU9ObQ0Tv9ItBXRNqLUVZELhGRkwqp2RFmuAXC\nUSxQ1Y2Y4/bf2A/bRuB+IEZVDwB3A28AezEn7ds+9/4KvAb84fk1amKO1hXAOsxf8XoBz8/EnOCt\ngbXALuAlzMkbDOYB3TE9NwJXevb+NGxBuNibw3PATZ7GvJgCNM/26ajqz8A44Cts8TgV+OIY5nYj\n5lP5FQsO6A+gqt8BtwP/8eb9G3DLMYzrCDNcopzDUcSIyHDgFFW9IdRzcTjyw+0gHA6Hw+EXt0A4\nHA6Hwy/OxORwOBwOv7gdhMPhcDj8EtZ5EBUqVNBTTjkl1NMoElJTUylbtmyopxF0okUnRI/WaNEJ\ngdX6448/EhMTg4ggIjRr1oxNmzaxb98+YmJiKFWqFHXr1iU29th/xpcuXbpLVasUeKGqBvUFNMFi\n0rNf+4H+3rlKwEKsDMBCoOKxjN24cWONFhYvXhzqKRQJ0aJTNXq0RotO1cBqrVu3ru7cufNPxz74\n4ANNT09XVdVBgwbpoEGDjmts4DstxG9s0E1MqrpKVVuramuscNlB4C3v9GDgY7Xs1Y+9zw6Hw+Hw\nQ+fOnXN2DB06dGDTpk0F3HFiFKmTWkQ6A8NU9Szv8yqgk6pu9YqEJalqk3wH8aFOg1M05tpngjTb\n4sV9p2Yw7sewtggWimjRCdGjNVp0wrFrXTfqEgDq1avHSSedRIkSJYiNjeW7776jatWq7N+/nyNH\njjBkyBCeeOKJP9176aWX0r17d2644djTaURkqaqeXuB1RbFAiEgFLCv1QuAwcIWqfuXVs18HNMMq\nTX6kqhUKGOufWFVJqlSp0u6NN94I5tSLDSkpKSQkJBR8YZgTLToherRGi044fq3XXXcdkydPpnz5\no4n7y5cvp1KlSowePZrk5GTuv/9+WrWygsAzZ85k1apVPPLIIxyt0l54zjvvvEItEEH3QXgL0HSs\nhvwurChZBe/4fsxHkQScDuw9lnGdDyLyiBadqtGjNVp0qh6/Vn/+hmzOPfdcvf3223Xs2LGqqvry\nyy9rhw4dNDU19XinWXx8EF4BtY5YeeHvVXWTHq0SudVbJMAakOwI9nwcDkd4k5mZSZs2bejatSsA\n999/P02bNuW0007jiiuuYN++3EVoiz8iwoUXXki7du144YUXSE1N5cCBA4Dp/frrr2nZsiULFixg\nzJgxvP3225QpU6aAUQMwLw2yiUlEWmOFybK7WS0FLlTVVLFm6+dj0UypwAeqelMB4zkTUwQTLToh\nerQGWucbb7zBqlWrOHjwICNHjmTJkiW0bduWEiVKMHnyZAD69OkTsOcdC8erdefOnVSpUoW9e/cy\ncOBArr/+el599VUANm/eTJcuXbj33nvp2bMn6enplCtXDoDmzZszYMCAY35esTExYRUrFUjGFoIN\nwH+8c4ux0NeDWPvDz49lbGdiijyiRadq9GgNpM6NGzfq+eefrx9//LFecsklfzk/d+5cvf766wP2\nvGMlEFqHDRuWY05SNRPTkiVLTnhcXyguJiash3CKqpZX1T1Y6eULvHNpwEisFv90b/FwOBwOv/Tv\n358xY8YQE+P/p2vq1KlcfPHFRTyrE8PXnJSamsqHH35Iy5aFat8RdIrCxNQMMyudh7WIXAXsVNW2\n3rkPsMbo+4EzVHV9AePlmJgqV67S7uHxLwZz+sWGavGw/VDB14U70aITokfrseo8tdbRSJ7MzEz6\n9u1L5cqV6datG3PnzmXnzp2sX7+eU089lQkTJuRce6KRPYHgeExMW7ZsYejQoYDpvfDCC7nhhhv4\n7LPPmDBhAsnJySQkJNCwYUPGjh0bkHkWCxMTFsL6I+aMzgRSsAYme4AsrN/vbqybVgqQ4V2/FDi/\noPGdiSnyiBadqtGj9UR0jhs3Tnv06KGXXHKJDh48WKtVq6Y1a9bUuLg4LVWqlPbs2VNVAxPZEwjC\n5TulGJmYzlPVGqpaQlUTgP8BE4BPgTZAZVUtBZwDHFLVU4GbsY5gDocjStm0aRPvvfcevXv3BmDk\nyJFs27aNzZs307x5c8444wxmzpxZ5JE90USRpDeKSFVV3SEidYBzgQ5AJ6y15LlYHkQljjZm/wmI\nF5FSqnokr3EPpWdSb/B7wZx6seG+UzO4JQq0RotOiB6teenMziIGM62cfvrp1KpVi3fffZc9e/bQ\nrl07SpYsyf3330/lypXzHP9f//oXR44c4aKLLgKsBMWkSZMCLyQKCZoPQkSmYjuBNMx0dABbkA4D\n+7Cw10ewnrZx3ud9WETTXOAcVb3Qz7guzDWCiRadED1aC6Mzd+jqww8/zK5du3juued48sknWbZs\nGbNmzcq5vn///txxxx00aVLoyjxFQrh8pyH3QWDJcV2AlZgTegVwHxDrnd8ATPfe3wW87L0/B/NJ\nnFLQM5wPIvKIFp2q0aO1IJ3+QlcrVqyoNWrU0Lp162qVKlVURHL8DarBCf0MBOHynRJqH4SqfopF\nLKGqO7AKrqKqGd4l+72FA6A5sEhEEoEpmKO6UrDm5nA4/srhw4c588wzadWqFS1atGDYsGEADB8+\nnFq1atG6dWtat27N+++/H9Dn+gtdzcrKYsuWLaxbt47XX3+dEiVKMHPmzIA+11EwQfNBiEhZoKz3\nfgVQG7heRK4BhmOLwmfe5SuAq4AxQAmgIdAVy4/IPa6viYmkpKRgSShWpKSkRIXWaNEJxU+rqjJi\nxAji4+PJyMigX79+VKtWjXXr1tGtWze6d++ec+2xzDs/nV999RXp6ekcOHCA5cuXs3v3bpKSksjI\nyMi5Z8WKFagqSUlJfwr97Ny5c0BDPwNBcftOT5jCbDOO5wU0AH4G0jHfwq/e8X6YP0KxENcPsIVq\nGUczrg9491XP7xnOxBR5RItO1eKtNTU1Vdu0aaNff/31XzJ7j5X8dA4ePFhr1aqldevW1WrVqml8\nfLz27NlTGzdurFu2bFFV1S1btmi4/L9enL9TXygGJqY/gF6YP+FK4Dfv1AHML/EZ0EVVu6iZnd4A\n/q2q5TET0zdA/WDNz+Fw/JXMzExat25N1apVueiii2jfvj0AEydO5LTTTqNXr17s3bs3YM8bOXIk\nmzZtYt26dcyePZvzzz+fmTNn0q1bN6ZPnw7A9OnTueyyywL2TEfhCWomtYi8DzQGegMDgf8AT2Gh\nrXOAgar6nYiU8Y5/jiXSDcXakM5X1TdzjekyqSOYaNEJodHqm6Wcm7S0NO655x7S0tJIT08nLS2N\nkSNHsmDBAj7++GPKly/Pnj17aNiwIU8++WShn1nYyJ7ly5fz+uuvM3LkSJKTkxkxYgQ7duygWrVq\nDBs2LKdAXXEm0qKYghnm+gnW46EkIFi4awkspHUrUB7YjJmVFPM77MZyIG4DHsbPAuFLkyZNdNWq\nVUGZf3EjKSmJTp06hXoaQSdadELx06qqpKamkpCQQHp6OvXr1+eKK67g5JNPJiEhgYEDB7Ju3Tq6\ndu3KypUrCz1ucdMZTMJFa2E7ygUzk/pLYC+2COzyjr2J9Xz4P+A7LKO6pao2A74CVqvqhWr1mBK9\nex0ORxGwa9cuMjIsyHD//v3s27ePevXq5RSSA3jrrbeKTSE5R/AJWhSTqg4BhgCISCfMxDQdqKeq\n671iWt/o0bDX/wIjRKQUUBNohJ8oJl9cJnXkES06IX+tmpHGtlcfQDPSISuLMk3OosI5Pck8dIBd\n80aTsX87seWqUfnywZQoXXiThm/2cm62bt3KzTffzK+//sqRI0f4v//7P+677z5atWrFL7/8wtCh\nQ6lSpQoffPDBMWt1hCfBzqTuiu0Y/oUtELUwn0QctjglY4vAUGAhVsCvDLbj6K2q8/2M6zKpI5ho\n0Qn5a1VVDh8+/KeQ0379+vHpp59Srly5nIYyBw4cCEpznJSUFIYOHcrdd99N+fLlKV++PCLC1KlT\n2b17Nw888MAxjeW+0+JFccmkbgus9D6XxMpoDPdz7XCsiJ9gJTd24GVc5/cKl9C3QBAu4XMnSrTo\nVC28Vt+Q06IM/xwxYsRfwlvXrl2rLVq0OKZx3Hda/KAYhLl+ipX1zuZizDmd4nudiNyCleS43pt4\nacxp7XBEPBs3buS8886jefPmtGjRgmeeeQaw5LAOHToQHx9PuXLl6NixI+3bt2f79u3UqGHde6tX\nr8727dsDNpedO3fm9HM+dOgQCxcupGnTpmzdujXnGueDiC6CmUk9FegGZO+3emDJb6NE5FFsNzEJ\nuAKLWFoqIvUx89MIPeqbyD2uy6SOYKJFJ5jWb7/9lh49etC4cWMOHjxInz59KF++PKNGjaJv376M\nGjWKuXPnMnv2bFq1avWnDGOwvIVA/X39/vvvjBo1iqysLLKysujUqRMJCQncfPPN/Pbbb4gI1atX\nZ8CAAQHLpI40Ik5rYbYZx/PCTEyXYNVby2IhrM8A92PRU49j9Zg2YjWbVmILxqVYSGzpgp7hTEyR\nR7ToVPWvtVu3bvrhhx9quXLlNCsrS1VVN2zYoJUrV9axY8eGZYZxtH+nxRGKiYkp2XufqqonY2Gv\nqqpZwIvABlWtrapNVLWlqvYF3sV8EW2CNTeHoziybt06li1bRvv27WncuDGvvvoqAK+++ip79uyh\nadOmLsPYUaQEO5P6bOAjVS3tfX4SuAZbOA4Bm1T1Ks+0tFFVM0SkL7bTqKWqu/yM6TKpI5hI1JlX\n9nJKSkpO/4Pdu3ezfft2zj//fB544AHmzJnDpEmTUFVKlChBTEwMH3zwQVhmGIdLZE8gCBetxSGK\n6TUsGkkxk9EGrGhfJrAF6x63Hlju/ZmGmaOOAH0L84xw2F4HinDZup4o0aJT1bRu2bJFv/76a+3c\nubM+8cQT2qhRI/3pp5/09NNP16SkJFVVffzxx7VmzZohnu3xE23faThAMTAx9QDOBH5S1ZKqWgc4\nGWsz+jegM3BAVVural3gZSzC6XxVdf0CHVFB9erVefbZZ2nWrBlDhgyhWbNmbN68mV9//ZWOHTuS\nlZXF0qVLs//R5XAUKUXSk9qHa4Df1TKp78Uc04hIBeBW4F5V/aKwg7lM6sgjEnWuG3UJGzdu5Kab\nbmL79u2ICP/85z9p1aoV06ZNY8aMGZQuXZrJkyeTkZHBjTfeSOXKlalVqxYJCQkkJiayf//+UMtw\nRCHBzKR+DegEVMbMTL9g0UxlsdBXAZYCl2ONgnphZb7jgKbAo6o63M+4LpM6golUnbt372b37t1/\nCmcdMmQIL7/8MldffTWnnXYavXv3Ji4ujmnTprFhwwYmTpxIcnIyZ511FnPnzmXevHmhlnFcROp3\n6o9w0VpYH0QwazH1ABCRAVhV1/JAe2A25pweJSKDgcFABjBIVceJyJtYo6GUPMZ9AXgBrJprOFRO\nDAThUiXyRIkWnW3btuXgwYNUqlSJxMRExo8fz3nnnZeTfwBw0003AbB69Wp++umnsP17iZbvFCJP\na1BNTF6P6UuwnIfRwPfARdjOAqx4XxJQAWgnIpcDa4HUYM7L4ShqfE1M6enp7Nq1i3/+85/Ur1+f\na6+9FoASJUowZ84cAHbs2EHVqlXJysriscceo2/fvqGcviNKCXaY65vASOAWoC8W1VRJVeO988Ox\nLOqDwB+YeekMrLBfiqr+pSuJMzFFNpGqM9vEVLt2bfr168e+fft45JFHePDBB9m3bx8NGjRgz549\n7N+/n8cff5xNmzblmJTOOeccbr/9drwKyGFHpH6n/ggXrcUhzLUr8BzQEliHhbFWwsptnKJHi/Rl\nYIvHk8C1PscHFvQMF+YaeUSyzrS0NO3cubOOGzdOu3XrpmPHjtXY2Fh97bXXVFV11qxZGhsbG+JZ\nBp5I/k5zEy5aCXWYK3AWVotpEVDVWwgmYElyN3vXJAB71MJa2wNjRGQd0B/4t4j8K4jzcziKDFXl\ntttuo1mzZlx55ZUsW7aMZs2aUadOHe6++25q167N3XffTdOmTUM9VYcjh6A3DBKRZsCHmOP5n1iv\n6QuwHhCtgVIi8gPWYa6bqu71TE8pqvqf/J7hwlwjj3DWmVc46z333MMFF1zA4sWLKVWqFBMmTCA2\nNpYff/yRNm3asGzZMhISEjh48CDx8fGhluFw5BBUHwSAiDQBPsXCXQ9hfSFSsaZAW4ErMR/FWOBV\nVe3ps0A4H4RHuNg2T5Rw1+kvnPXRRx+lXr16ZGRkMGTIENLT02nTpg1XXXUV1113He+88w4igqrS\ntWtX3nsvPBfIvAj37/RYCBetIfdB+HsBJbDw1Yd8jtUGPgA2Ab8cy3jOBxF5RJrO7OqsWVlZeuON\nN+rdd9+tiYmJunr1al28eLE2bdo0R/NHH32kbdu2De2Eg0Ckfaf5ES5aKaQPokgyqUWkqqruAK7D\nSn3/xzteA3gaGAQspoAe1A5HcSEvUxLAxIkTefbZZ8nMzGT79u3MmDGDL774ghkzZtCgQQN2797N\nNddcw3XXXceLL77IPffcQ0ZGBqVLl+aFF14IsTKH4yjBzKSuDbwCVAPqAgcw89IaIBGrydQQy7Je\nBzTybl3l/fm1Wvnv3OM6E1MEEy468zIl7d27l5kzZ/Lwww9z//33c8UVV3DxxRfn3Pf0009Tq1Yt\nrr322rDReqJEi04IH60hNzFhvaXbeu9Pwqq37sV8DQOBMsA3QHnvGmdiyodw2bqeKOGqM9uUdM01\n1+j8+fNzwll9SU9P16pVq+rGjRtVNXy1HivRolM1fLQS6jBXVd2qqt977w94i8PvHM2SbgjUB1Z4\noa3VgYYiUj1Yc3I4jpW8ekYPHz6cWrVq0bp1a5o3b85XX31F+/btWbVqFQ888AArVqxg3rx5LFmy\nJGesjz76iKZNm5KYmBgqOQ7HMVFUUUxvAU2wPhBVsUViE9YbohqWYd0cyAJ+xXIlHlLVz/yM5xoG\nRTDFTWfN0hl+TUlJSUnEx8fTrVs37rnnHm644QY6duxIjx492LZtGw0aNCAtLY1t27bxyCOP8Le/\n/Y1Ro0bRvHlzunXrBoSPOeJEiRadED5aQ25iyn5hyXDLMB9ERWwxqI85qz8FvvWuWwc08t63w3pV\nl8tvbGdiijyKu85sU9KwYcN01KhRfzEldenSRRctWpTzuUGDBrpjxw6/YxV3rYEiWnSqho9WQm1i\nAhCROOC/wAxVPQmrs7RGVdeq9aX+HgtzRVXrqeoa7/1SzBzVOJjzcziOBd+e0arKI488wtKlS1m5\nciV79+4F4PLLL2fx4sWAVWFNS0ujcuXKoZy2w3HcBC3MVayy2BTM8fyUd/g6wDcLqClQzsukXgn0\nU9XdItIAi2r6I79nuEzqyKModGbs38mu954iK3UfICS07sKej15g6NChzJs3j5iYGKpWrcq0adOo\nWbMmYKaDq666ivHjx1OuXDnatWvHwYMHadiwIe+99x7z5s1jxowZ9OrVi169etGyZUtKlizJ9OnT\nw7bInsMRzDDXd7FS30cwv4JgP/rpQDngN2wB6IVlVL+BlQJP8D7foarv+BnXhblGMEWhM68Q1SpV\nqlC2bFkA/vvf/7J+/XoGDBiQkwF9xhln5JTm9mXbtm05zX+OBfedRh7hojXkPgigI9AWWOl9vgz4\nAnNWJwGn57q+HuacnkMhKrmq80FEJKHQme1X8OWJJ57Qvn375mRA33PPPX86v2XLlpz3Tz31lHbv\n3v2Yn+u+08gjXLQS6kxqVf1UROr5HOoBvKSqq7K33CJSQ1W3eueHYlFOPwVrTg4H+G/eM2PGDIYO\nHcrkyZPZt28fJUuW5KuvvsrJgD711FNp3bo1AE888QSvvfYay5cvR0SoV68ekydPDrEqhyPwBLth\n0GnAZ8A2LO/h71gk0zQgHpgP1MHMT7WwLOq2WGTT2XmM6UxMEUxRmph8m/c8+eSTfzIxDRkyhD17\n9gT1h999p5FHuGgNuYnJW3jeBDZ770tirUWbYRnUS/HMTFizoBuBs4F3gc8LM74zMUUeRaUzd/Oe\n3CamQYMGacWKFYM6B/edRh7hopVQm5hEpDzWBGivtxClYV3l9onIIaz8Rjbtgau991WBGBH5lxbQ\nD8LhOB42bNjAmWeeSVpaGn/88UeOiemuu+5izpw57Nu3DxGhY8eOoZ6qwxFSghnFdBnwGrZzSMNC\nah8CXsIimMoAK4C/qzUJuggYBZyCJdFdpqqL/IzrMqkjmEDpPLVW+TzPff755wwdOpR69eqxZcsW\nMjMz6devH99++y1btmwhJiaGPXv2ULFiRaZOnXrik8mDcDFHnCjRohPCR2vITUxYjwfFWo1uwsJd\n38UyqjOwRWM9MNq7vg1QE/gflkC3uaBnOBNT5OFMTJFHtOhUDR+tFINM6puB9aoai+U6rMac0Vuw\n7OkvsZ3A5d5CtUxVt3gLxJdAvIiUCuL8HFGK6l/7Q7dv3541a9bw4IMPUrt2bV555RXOOuusUE/V\n4QgpwQxz3SYiG71ifddhtZV+Bs5S1a1eqOturFhfbhoC36vqkfye4TKpI49A6Vw36pI8z82dO5cZ\nM2ZQsmRJJkyYQMWKFfn888+57777WLt2LSJCTEwMNWrUOOF5OBzhTLDDXFtj5TbaYg2CmmDmphKY\nbyIdiFHVOK9u014s/DUGa03aXlV/zjWmC3ONYIoqzHXHjh1MnTqV1q1bs2DBgr9kUk+bNo25c+fy\n9ttvB20e7juNPMJFa8h9ENkvbIHYCrzrfd4DPO69fxzY7b2/HpiHmaLOx6q71stvbOeDiDyKQmfu\n7OhsH8Tq1atzrunatas2aNAgqPNw32nkES5aKQY+CEQkEavH9Ep+l3l/xgOdgH9j0U1pwP5gzs8R\nGeTV1GfOnDm0aNGCmJgYvvvuu5zrs7OjFy1aRPPmzZk/fz7JyckMHjyYKlWqULJkSZKSknjrrbdC\nJcnhKBYEM8x1KrYriAO6AXcA3wEPYw2DSmI+iQbe6zusHpN6ry1AO1XdkWtcZ2KKYI5HZ17F90QE\nEeGpp57ijjvuoEmTJn+679ChQ39q9uPLrFmzSEtL49Zbbz1hTXnhvtPII1y0htzEBDyAVWhdie0M\n3gWGA4dyXbcXKIstIN8Az2HJcquABvk9w5mYIo9A6MwdtnruuefqkiVL/nSNb5irP9avX68tWrQ4\n4bnkh/tOI49w0UoxMDFVAM7Bmv7MxvwKVwIHRKQGWLE+YIeqpgItMdNSltqu4Qug4BXOEfX4mpga\nNWrEp59+Svv27XNMTJ988gk//3w01kF9wlwHDBiQc3zNmjU57+fNm0fTpk2LVIfDUdwoqmJ9yViR\nvtc5anaKBV4E9qnqIBF5AOjL0bakpYBLVfWHXGO6TOoIJi+d+WVG51V8L9vEdOedd3L33XfTuXNn\nAH788UegHCndAAAgAElEQVTuvvtuGjRokNPMp3fv3rz//vts3LiRmJgYqlWrxr333kuVKlWCohPC\nxxxxokSLTggfrSE3MXkLz5vAZszE9B7WMOj/gK+wENedQCXv2tOBg1iU0xpgF1Aiv/GdiSnyOF6d\n+WVGly9fXqdPnx6gGQYO951GHuGilVCbmHyL9alqkqpeoqprVPVLVf0bVs01WVX3eLdc5C0ir6pq\nI+/8mcGanyNyUPWfGe1wOE6MoGVSA/WxTOm6IrIM+8F/QlWz+0xXBn73ub4WtnvIZpN3LE9cJnXk\nkZfO48mMXrx4MePHjycjI4N+/foxa9YsPvjgg2BO3+GIKIIZ5roQuND7mAFswMJbq2OlvuMwp/Sl\nqrpFRFKB0t71mdiCMk5V38w1rgtzjWCON8zVX2Z0Xj6I4oL7TiOPcNEach8EthCs997HYTkPXwDl\nvGNJwFhgkvf5YeBB730NrPrr2fk9w/kgIo/j0ZlXZnQ2zgcRWqJFp2r4aCXUPghV3QZs8Ir1xWHR\nSWtV1Tc7Oh5LigP4L3CtV8H1FKxe07fBmp8j/MgrY3r+/PnMmDGD559/noSEhJzM6LfeeovExET2\n79/PvffeS5cuXUKswOEIL4Id5toW2zWUxnYEa7BKrTFYGGsm8AeQ5b1PA1pgvpHHVPVhP2M6E1ME\nk5/OvDKmFyxYQLly5bjiiiu46aabaNKkCY899ljOff379/ebSR1q3HcaeYSL1pCbmHxfWNLcYuA0\nYBtQ1zv+IrZAlPI+V/X+bIbtHkrnN64zMUUex6Iz25TUuHFjXb9+vXbu3FmHDRumuf+78JdJXRxw\n32nkES5aCXVP6lyL0D4RWQz8C/hdVdd7p2pgu5gj3nU7vD9/EZEULLv6O39jOqKPjRs3ctNNN7F9\n+3bS09Nzeklv3bqV9u3bc+jQITIzM9m2bVuop+pwRATBjGKqgiXDVcNqMjXETEklgcGqOl5E9nuf\nM71re6rqeyJSF0umO01Vd+Ua12VSRzD1y5co0MSUO2O6d+/eZGZm0qBBA/bt28eePXsYOXIk6enp\nTJgwgeTkZBISEmjYsCFjx44tYkV5Ey7miBMlWnRC+GgNuYkJMyctA37ACvaNwLKjD2OF+H7A+lNP\nxUp+v48tFMuxntSXF/QMZ2KKPArS6S9jOi4uTpctW6aqqsuWLdO4uLgimOmJ477TyCNctFIMoph+\nUNU2qnqaqrb0fvTXA0tVtYmqnoY5sGd4E/4M6yJ3kaq2VdX/BWtujvBE1X/GtIiwYMECwCKasusr\nORyOE6NIfBAePbBaS6/7HPsf8IiI1AMOYQvErr/cmQcukzrymPZ3a/nZq1cv3n33XapWrcrKlSsB\nmD59OjNmzKBUqVJMnDiRxMREPv/8c0qVKsXChQuZMmUKdevWpXTp0vk9wuFwFJKgLRBew6CuWGXW\n9litpTigiohMwOosTQXOBmpizYJSgOUiMlxV/bbzyhXmyhveD0qkk5KSkvPjGcmkpKSQlJREq1at\n6NChAyNHjiQpKQmAUaNG8eSTTzJ79mwSEhKoXbs2ZcqUoUKFCtx5552cfPLJ7N69m9WrV+fcU5zJ\n1hrpRItOiECthbFDHc8L6Ai0BVZ6ny/DTEpNsCzq032uLYPVblqJRTbtAGILeobzQUQevjrXrl37\np6Y9J510Uk7G9IYNG7RZs2aqqjpw4EAdOXKkqqqOHDlS77///iKd8/ESjd9ppBMuWikGPohP+XPx\nvR7AS6q6KvuAiDTyrj2I7TZ+xZLqgpe95yhW9OrVi6pVq9KyZcucY8uXL6dDhw784x//4Pfff+fb\nby2hvk6dOjm9pDt06MCqVat4//33GTx4MAsXLqRRo0Z89NFHDB48OFRyHI6IoqgaBm3Dwlz/DnQB\n7sL6RJTBmgllYOGudb0/N3jv26rq8lxjukzqCGLFihXEx8czcuRIXn75ZVJSUhgxYgRXX301devW\npX///lSvXp3x48ezYcMGJk6cSHJyMmeddRZz585l3rx5oZZw3ETqd5qbaNEJ4aM15GGu3sLzJrDZ\ne18Sy6juDHyCNQgaDYzOdU8z4Efgj4LGdyamyMDXlLR48WLt3Lmzzp49W9euXauJiYnao0ePv9yz\natUqPeOMM4p6qgElkr9TX6JFp2r4aCXUmdS+DYO8hSgNq7X0oYj827vsa+DqXAvWLyJyEvBxsObm\nKN6MHz+eLl265GRLjxw5EoAdO3ZQtWpVsrKyeOyxx+jbt2+IZ+pwRDZF3TDoHlVN9bmmF/C6iNQH\nNqpqhpdFnQi87G/QXJnUTJwVviaGY6FaPGGrNb9+0qNHj+aLL77g8OHDJCUlkZKSwqOPPsquXbtI\nS0sjMzOTU045hf79+3Po0KEck9I555xD/fr1wzpiJOIiXvIgWnRCBGotzDbjeF5YRzj1XpuAhViU\nUiZHq7fuxbKobwS2YxVfjwDbC/MMZ2IKfz755BN95513tFSpUqpqOmNjY/W9995TVdV3331XS5Qo\nEcopBo1I/U5zEy06VcNHK6GOYsIK820FflLVROARzA/xALZz+Bao5U12KebILgfMBGJFpEQQ5+Yo\nJnTs2JEKFSr86VipUqX4+uuvAfjyyy8pV65cKKbmcEQ9x2xiEpGKQG1V/SG/61T1f56vIfv//guA\nnVgeRC/gXLXwVrAcidlYwb6/Az9hiXRf5fcMl0kdHqwbdYnfzOju3bvz4YcfcuDAATIzMylZsiT3\n3HMPkyZNolevXowaNQqwntMOh6PoKVSYq4gkAd2wBWUplsj2haoOyOee0ljP6UaYc3odVlrjPszs\nlI6FuU7HMqhXAn2wIn+rgIc1Vz9qb1wX5hqG5A5n9WXbtm306dOHK6+8kquuuoqpU6fSqlUrzj33\nXBYvXsy7777LuHHjQjTz4BHu32lhiRadED5aAxrmCizz/uwNjPDe/1DAPQI0x37444BvgIuxVqJ1\ngN+xhaEy8B/gNqzsRl+sf/XVBc3L+SDCi9yZ0dn88ccfGhsbq6tXr9bFixdruXLlNCsrS1Wt3/RJ\nJ51U1FMtEiLhOy0M0aJTNXy0EmAfRKyI1ACuBd4tzA3eJLJNSHHea4+qZgJPYT6Kkt75zVg3uc+x\ncuAJ3jFHhNCrVy/atWvHb7/9lnOse/futG7dmk6dOpGZmck111wDQM2aNfnkk08AWLRoEY0aNQrJ\nnB2OaKewJqZrgKGYWekOEWkAjFXVq/K5pzbWKKg9ZlJajEUr9cd2IhWxiKXaWFOhVzG/w1DgQWCQ\nqv6lu4szMYUnK1asIDU1leHDh/Phhx/mHH/00Uf54osvOHLkCGXKlOG2226jUaNGTJw4Mccv0b9/\n/2LXTzoQhPt3WliiRSeEj9aQZ1IDb2FO6XQsmukIVqTvELAGK+u9DajsXf8gZnY6BGwBBhb0DGdi\nCi8+++yznHDWbNLT07Vq1apavXr1HBNTtBAtWqNFp2r4aCWQJiYRaSwiH4vISu/zaSLyUAELzxWq\nWkVV41S1BvAH5qg+gJmbEjD/w/ciUl1VH8cc2IuxvAlHFPDRRx9Ro0YNatWq5UxJDkcxo7Ampk+A\n+4HJqtrGO7ZSrVNcXvdUAdJVdZ+INMEimm5U1Tne+SSgMV7faRFJwJLppmEO6zdU9Uk/47qe1MWU\n/DKme/Towfbt21FVKleuzC233MJ3333H0qVLSU9PJzY2lmrVqjF+/Piw2KIHgnAxR5wo0aITwkdr\noKOYlqhPNJP3fnkB95wOpGImoyzgR+/4NVieg2ImqGwT01PetRne9SlA8/ye4UxM4UPujOlssk1M\nt99+u44YMSLsdR4L0aI1WnSqho9WAhzFtEtEGno/6ojI1ZhfIT+WArWAT4FBwEER6YCFvV6Jlfke\noKrZLUa7eYvDJswMVRpLrnNEAP4ypsFMTE2bNmX+/Pn06NEjBDNzOBx5UdhM6ruAF4CmIrIZWAv0\nLMR9E4BfgOeA67Ho118Af43lPwC+VtUZIjLcG/+b/AZ3mdTFi7wypgHatWvHihUryMzM5KSTTmL8\n+PHcdtttzJ49m9NPP53U1FQaNWrE5s0uutnhKC4U6IMQkRgsae0NESkLxKjqgQIHFjkfK9mtWNJc\nKpZH0QQYheVAZGLmq7+JyEtY6e912M6jMhbJNC7XuC7MtRjjL2N62bJlzJw5k3vuuYdhw4bx1FNP\nUbFixZx7nn76aWrVqsW1114bNjoDQbRojRadED5aA+2DKJS9Ktc9AiR47ysD+7FdxIdYRnUScDeQ\n5F0TCzwNLAcWYQvK5fk9w/kgiie5M6avueYaXbhwod9M6mwfxMaNG1U1vHSeKNGiNVp0qoaP1sL+\nphfWB/GRiAwUkdoiUin7VcDCo6qa4n08iDmdO2A7iuzynAlYzgOqmqGq96pqa6yR0F5gdSHn5ygm\n+MuYXr16NePGjaNJkyb8/PPP3HjjjTnnsn0QiYmJoZiuw+HIh8KGua71c1hVtUE+93QAXgROwcxJ\nAryE1WK6yfszDThDVX8UkTLAqcB4LAIqHaikqodzjetMTMUYfxnTPXr0YO/evZQpU4bk5GQABgwY\nwCWXXMKoUaNo3rw53bp1A8JHZyCIFq3RohPCR2txyKQ+DVgG/IA5qjOAizDH824ss/owcMi7vqH3\neTO222gFlMjvGc7EVDzJnTFdrVo1HTt2bM7nBg0a6I4dO/zeG046T5Ro0RotOlXDRyuB7EktIjfl\nsbi8ks/C8wOQnVTXGds9tAKaAhVUVUXkCqxeE5jz+k0sN2Knqq4ozNwcxZ/Y2Fjefvtt5syZQ1ZW\nFikpKVSuXDnU03I4HAVQ2DDXM3zeZ+cnfA/kuUD4ZlJjIatpwK+Yz+FczEk9iKNlNRpj/ok7gN9E\nJENVx/gZ1/WkDjGFzZiuUqUKt9xyC7Gxsfz444+kpJhLKjY2lqSkJH+hzpHX0zcfokVrtOiECNRa\nmG1G7hfWJW5BAddchCW8HcZ++D/yjp+NJdHt945f4B1/BltEDmJmqVXZ5/J6ORNT8cNfxvQZZ5yh\nbdq00cOHD6uqat26dZ2JSaNHa7ToVA0frQS5J3UqUL+Aa1ZiO4XuWNhqHRFprtbzYRZWTmMjVqMJ\nb9GYraplgC5YLkTBThRHscJfxnRmZibNmzenVKlSrF69mszMTGdicjjCgML6IN7BK7MBxGCd4ubk\nd4+qbgW2isggYCZwOVBLROoAw7HSGtN8bnkH6O9FM5XFKr7+nN8zXCZ1aFg36pI8zzVo0IB169ah\nqiQmJjJixAjS09NZtGgRMTH275ELLrjAr3nJ4XAULwob5nquz8cMYL2q5luS22sYNBM4C8uOPhmo\ni/kg4jFzUkls19BTRHoCT2LNg7J/Pdqo6vJc47ow12JMXmGumZmZzJgxgz/++IPhw4cze/bsPH0Q\n4aAzEESL1mjRCeGjNdCZ1KMLcyzX+RpAWywZbhm2MLTFHNWVvGuSgQl+7u2G+S5K5/cM54Monrgw\n18IRLVqjRadq+GglwD6Ii/wcu7iAhWcr8CPwX2AGsARoB1TCmgSt8xaP20Skeq7bO2Dd5vLsN+Eo\nnvTq1YuuXbuSlpaWcyw2NpZJkyYRHx9PfHw827Ztcz4IhyMMyNcHISJ3AHcCDUTkB59TJwFfFHBv\nbWxRiMMc2lWwntRLgElYuGwdYKKqbhORNlgtptMxB/ZhzDSVe1xfE1NkhZTlQ7iEzy1fvpzMzMw/\nhblmZGSwadMmqlevTlZWFgcOHHBhrkSP1mjRCRGoNb/tBVAeqAe8hvkPsl+VCtqaAJdhju2VWOhq\nOtCHo8X6HsQWgS+863tjZcS3Y36OfAv1qTMxFVucialwRIvWaNGpGj5aCUQmtaomY36CHgAiUhX7\nl3+CiCSo6oZ87p0nIiWBd4GpWMjrH96icSmWZd0HC2lFVV8CXhKRD4Byqvq/QqxvjjDAZVI7HOFJ\nYaOYLsVagtYEdmC7iF9UtUU+9wgwHdiDFeD7FPMpdMcaCO3wLv0/VV3v3ROD1Wl6R1X9lvdwPalD\nz7H2np4zZw7Jycl/yqR+//33XRRTlGiNFp0QPloDHcW0AgtTXeZ9Pg+YUsA9Z2O7hWTMp5AGDMCS\n7DK8c1uATd71JYH5WBG/XUCngublTEzFD5dJXXiiRWu06FQNH60EOIopXVV3AzEiEqOqiykgy1kt\nY3oGli09EItYmuotFi2AT7Bw1uzeELdjkUt9gHnAOG9H4QgjXCa1wxE5FLZY3z4RSQA+A2aJyA5s\nJ5AnIlIeuALbaTzlHU4TkU1YjgRYEcA13vvmWEmOEsAhYB+2CH2b1zNcJnVoyKv39PDhwxkzZgxp\naWk5i8Do0aNJT0/nhx9+oEyZMpQoUYLRo0e7TGqHIwworA+iLPajHYNVZi0PzPJ2FXndMwAYh+0Y\nxLv/BsxBfQNHGwY9oKoTPN/C01i0U7aRu7+qPpNrXJdJXQzw13t62rRpxMfHc+655zJkyJCc47fe\neitt2rShX79+/PrrrzzyyCO8+uqrzgcRJVqjRSeEj9aANwzCHNMXeu/LACcVcH0XzNfQHsub2As8\nj9VhGoiV+z7d5/rsntT7sKZBv+B6UudQHG2buXtMDxs2TMeOHfuX4126dNFFixblfHZhrka0aI0W\nnarho5VA+iBE5Hasmc9k71AtoKAw1BWYA/obVT2A5UO0zWehysD8Ei96r3hcT+pii7/e00lJSQwe\nPJgmTZrwyy+/8PrrrwNw+eWXs3jxYsD6U6elpTkfhMMRBhTWxLQcOBP4RlWzu8T9qKqnFnDfYWA9\nkIk1BHoW8z9cjmVYf4rtEvaKSA1sUdmJLUBZqlrJz5jOxFQM8FeU7/nnn2fp0qXs3buXvXv3UrJk\nSfr160fnzp0ZM2YMv/32G3FxcfTt25e2bf3/W6G46Qwm0aI1WnRC+GgNdJjrN96f2WGuscAPhbhv\nC9b85yDWi7oi8G8sWikL80tkh7l+7H3+Bcuo3gfUy298Z2IKLbkzprNNTKp/NT8VluKoM1hEi9Zo\n0akaPloJcJjrJyLybyBeRC7CekG8U4j70rCchodUtb2q7lXVJ1S1OrZ7uNZbCMB2DYqZliphfotr\nCzk/RzHgwIEDTJw4kdNOO43u3bvTuHHjUE/J4XCcAIU1McUAtwGdsYikD4CXNJ+bvUzq/Vi9pQ3A\nZFV9QURqqOpWEUnC+lrXVNXrRCQOy5u4AItimqeq1/gZ12VSFxGn1irP6NGj+frrr6lQoUJOVBLA\no48+ypdffsnhw4c5+eSTufXWW/n222/ZsGEDIkJaWhqNGjVi2LBhx/TMcNmiB4Jo0RotOiF8tAbE\nxATUKcw2JI9738V2BIcxB/UhbAHY5x0/iDmla2BZ2kuxENfngLFYKY4G+T3DmZiCzyeffKJLly79\ni7low4YNes4552hcXJzu3LnzL/c5E1PBRIvWaNGpGj5aCZCJKSdSSUT+W/j1CYAxWP+H31S1JTAa\n2IqFvX6C+RruU+sbke3Mnok5p+/3FhjXkzrEdOzYkUqV/hIrwL333svgwYP/dGzr1q0579966y1a\ntnTtPByOcKagTGrfTKYGxzj2UqxUeHaiXWfgEVVdlTtBSlVTReQboCuwwru+A1bkL09cJnVgyK/H\ndK9evZg3bx6pqUcT5+fNm8ePP/7IddddR3p6Oi1btuTxxx8nKSmJ5cuXIyLUq1ePyZMn5zmuw+Eo\n/uTrgxCR71W1be73hRpYZA5WaiMG2y28iuU1jANqY0l0e4Dl2MIwzbu+JFbR9UlVHetnXBfmWoTk\nDmc9fPgw9957Lw888ADPPvss33//PdOmTaN27doBeV642HADQbRojRadED5aA+WDyMQczQewH/T9\nPp/3F3BvR+AS4LDPsWZAE8wPcaPP8euB2cAtWLe5dRQQ4qrOB1Fk+Iaz/vDDD1qlShUtU6aM1qhR\nQwGtVauWbt26NSDPChcbbiCIFq3RolM1fLQSoIZBJY5lVcp176cicnauY78A/mrwKFAW223EYuGx\n+4/32Y7A0atXL+bOnZvTY/rUU0/lxRdfZNGiRdSpU4eBAwfy5ptvUr167rbiDocj3ClsNddjRkSm\nYhnTJX2ODcfKep8EPCoiu1X1fayMx03AC1gRv81YlJO/cX3DXJk4a16wJBQrqsUTNK35NQDK3WP6\nhhtuYMGCBTkmJoDvv/+ew4cPB2QuEdfTNx+iRWu06IQI1FqYbcbxvICPMB+DApuwPIrZmHkpC/Mz\nfKBHzVGZWHOhVCzc9e8FPcOZmIoGZ2IKDtGiNVp0qoaPVgKcSX08C8+FWEOgI6qaqKpTgF+Bx7As\n6i6q2sW7/H7ga1Utr6plsXyJcv7GdYSWbBNT79692bJlCyVKlODjjz92JiaHIwIJmokpH/phiXFD\nReQWVd2L7TKqicgHQDUseS7fEFdwYa6BIq8GQEOHDmXixImkpKSQmZlJjRo1ePjhh5k2bVpOgT6H\nwxG5FKrUxnENLPIacCFQGYuA2glMAfpjDmnFnNE1gDuBB7H2pLFYzsULqtrXz7guzDUI+GsAlJqa\nStmyZdm2bRt33XUXZ511Fpdffjn33XcfpUqVAmDnzp1UrlyZ559/3m9C3bESLmGCgSBatEaLTggf\nrQFvGHQ8L8z5vNl7XxKogCXMxWJJdDuxDOvrgOk+96wE3ilofOeDCCx5lcdYu3atVq1aVfv27fuX\nc3Xr1vVbauN4CRcbbiCIFq3RolM1fLQSah+E15O6PdZJDlVNA+JV9UO15kBXYD6JRKz436kici2W\nA1EGq8XkKCL8NQAaOnQopUuXpn79+uzYsYO5c+cyZcqUEM7S4XAUJcE0MS0GzuVouY4/sJ4QZwA1\nMbMTwDhVfURExmHmp3Tv3Ex1JqYcisLElLsBULaJCWDIkCHs2bMn6OUzwmWLHgiiRWu06ITw0Rpy\nExN596T+ELgY8zl8CSR5178C9PHePwckF/QMZ2IKLLkbAPkyaNAgrVixYtDnEC5b9EAQLVqjRadq\n+Ggl1CYm8u5JrcClWP2lyVjXOYD6wBARWYeV3jhJRPoHcX4OH3r16kXXrl1zMqYB7rrrLqpWrUrJ\nkiUZP3487dq1C+EMHQ5HURNME1NpzAm9GSuhUQdbEJpgu4t0LDnuTFX9UURKeudPx4r5pahqop9x\nXcOg4yS/jOkBAwawZs0aUlJSqFy5MrfccgtffvklW7ZsISYmhj179lCxYkWmTp16YpMogHDZogeC\naNEaLTohfLQWBxOTAH/DusYdBFKAi7w/d2FVXJOBA971dwEvAy2wbOutQEx+z3AmpsDiTExFS7Ro\njRadquGjlVCbmLInge0iRmClvvdju4Yqqtoaq95a2rulObAMeAurAvszrmFQSFmzZg0PPvggtWvX\n5pVXXuGss84K9ZQcDkcREsxifQJMxfwOZwHPquo3IrIFi25KAgZhdZrAFpBHgN6YX6IdZmr6Nq9n\nuEzqYyO7MZC/rOlmzZqxZs0aMjMziY+PZ8yYMSQlJbFq1SoqVqxIeno6p5xyygnrcDgc4UMwfRBn\nA59hZqTSQBzWLCgRuBKr2qrARar6iYg8BdzrHVNsp3G7qk7PNa4Lcz1B/GVNL1myhJo1a/LQQw/R\noUMHAPr06ZNzz/bt2xk8eHDO9cEiXGy4gSBatEaLTggfrSH3QXgLz3Sgt/d+BPAQlkndC/gKWzBG\ne+frASt97v0SaJ7f+M4Hcfz4y5rOPjZ37ly9/vrrdfXq1TnnJkyYoFdddVVA5+CPcLHhBoJo0Rot\nOlXDRyuBaBh0IohIA6ATcIuIxAPnY2U1YoCBmJnpbOBq75bSeEl1InIRkKGqPwdrftGMvz7Tviam\nq6++mp49ezJ48GBWrVpFTEwMdevWZdKkSSGctcPhKGqCaWK6Bngd6/0QAxwCqgK/YQX8YrHeD9+o\n6gUicjUwBzMvZQEf69Fy4L7jOhPTCeIva3rJkiW0bduW1157jQULFnD22WfTt+9fEtmDTrhs0QNB\ntGiNFp0QPlpDbmLCIpAU6Ox9fgZ4lKN9qf8AFnN0kWoM/OK9b4dVdi2X3zOcien48RfS+vLLL2uH\nDh30tdde0+uvvz6gzyss4bJFDwTRojVadKqGj1ZCHeaKRSdlYnkQYFVa26r1pf4b1hPiIW+yYKW/\nM71Faynwu7doOAKMv6zpq6++mj59+pCcnEz//v3p2LFjCGfocDiKA8E0MU0FbgWOAD9hoau/Yr0g\nbsPqNL2pqjeLSD3vXBy2UOzHzFJNVHVPrnFdJnUhONas6ZdffpmYmBgyMzM5cOAAderU4aWXXgrA\nzI+NcNmiB4Jo0RotOiF8tBYHE1NH4A7M9/ATFu46GMuq3ohlVP8GTMIimDZgi8Ry4Bcswc6ZmDyc\niSnyiBat0aJTNXy0EmoTk6p+CswHflfVFsDTWGOgbqpaG8uyvk6PlvTer6pNVbW1qjbzFhVnYioi\nFixYwJgxY3j77beZNWsWF198cain5HA4Qkwww1zLYuak7PedsbLf54jI45ijujm2UADUF5Hs+kzP\nAo0wR3aeuEzqvMmrz/ScOXPo3bs3+/fvByAxMZERI0YwcuRIjhw5wmmnncbBgwepWbMmN9xwQ1C0\nOByO8CCYPog5WNe4GMxk9CpWWgMgHvM3gPWH6AbM9P4U79VTVf8Sw+rCXAuPv4zp9evXIyKMGjWK\n5ORkZs2alXP9ggULeOeddxg3bhylS5fOa9igEi423EAQLVqjRSeEj9bi4oO4BDjsc2wBcJ73fhyw\nB6iC7SRWAKWwvhCHsDLg+T7D+SAKJq8+0+3bt9cGDRrkfJ4/f742a9ZMd+zYcbxTDAjhYsMNBNGi\nNVp0qoaPVoqJDyI51+H/Aed5hfyuxyKcdnnvX1fVIxzdQVQI1twijV69elG1alVatmyZc2zPnj1c\ndNFFdOrUiXXr1rF3796ccz169GDZsmWsX7+exMREpkyZwr/+9S8OHDjARRddROvWrUOSJOdwOIoX\nwTevdxIAABjkSURBVDQxzcNai8bhhbQCN2NVW2t4x3cA27DeD+0x05N61ydgeRPLc43rTEy58GdK\nmjRpEuXKleP888/nzjvvpEuXLn8qvte/f3/uuOMOmjRpUiTzLyzhskUPBNGiNVp0QvhoLQ4mphqY\niWkl5pxejVd8D+tNfR9mZnoY+A9wg8+9c4FtBT3DmZiOktuU1LhxY92yZYuuXbtWmzRporn/rs49\n91xdsmRJMKZ6QoTLFj0QRIvWaNGpGj5aCXWxPlXdKiI/ee8PiMgvQC0RWY2V+26HVXQ9HyvYV9vn\n9taYv8JRCPwV39u2bRs33XQTa9asYdeuXZQoUSKEM3Q4HOFIsHtSf42Fs24EqmGLwCQsWmkrVryv\nGVAReB9rNRrnHVMKMDFFUyZ1/fIl8jUx5S6+17lzZ+rUqcPevXvZu3cvqsrAgQMpV64cEyZMIDk5\nmYSEBBo2bMjYsWOLUkq+hMsWPRBEi9Zo0Qnho7U4mJhew/wL6Vj5jG1AB2AW0BczM70FTPKufxCr\nv7Qeq+P0e0HPcCamo+TOjI6Li9Nly5apquqyZcs0Li4umNMLGOGyRQ8E0aI1WnSqho9WikEUUw9s\nx7AIGIrVYlJV7Qm8hJmZfsV2Cqjq46raEPM//AbMDtbcogERYcECs9LNnz8fCxxzOByOwlPkPam9\n0zOA8sClwHk+98QA12JRTP8q6BnRlEk97e9leeaZZ3jxxRdRVW6//Xb69+8PWNjqxx9/zJEjR3Iy\no0uVKsXChQuZMmUKdevWDVnim8PhCF+CtkBgi8INWC5ELDDQWzRmARcAu7G+1AOAId49k7BmQgC1\nsAioP5ErzJU3/l42iBKKDz/99BPjx4/n+eefJy4ujkGDBlGlShVq1apFnz59uOyyyxgyZEhOmGuF\nChW48847Ofnkk9m9ezerV68mKSkptCIKQUpKSljMMxBEi9Zo0QmRpzWYUUyfi8grwGeq+pKIjMD8\nES8B16jqJyIyECu/MUREmmO5EJOxBLrnRKSxqmbmGvcF4AWAJk2aaKdOnYIloViRlJTE+eefz9//\n/ncALr/8cjZv3kzPnj0BWLduHWXLliX776N79+6sWbOGq666ilGjRnHdddcRDn9XSUlJYTHPQBAt\nWqNFJ0Se1qD5IHx6Uk/x6Um9HGgKfOpdFo/1oga4DGtRehXmwP4NODNY8ws36tevzzvvvEPTpk1p\n1qwZU6ZMYePGjYCZmP72t7+xatWqnMzowYMHs3DhQho1asRHH33E4MGDQ6zA4XCEG8HuST0d26XE\nYruCf2CRS/FY3+kMbw4JIvIccDZwChbJlAKMVtU3c40blZnUP/30E8OGDaNixYrEx8ezdetW2rVr\nF3E//OESJhgIokVrtOiE8NFa2DDXYPog1mK7g5Gq+qCITMRKb2wEDmO7l41YGXCAhkCqqpYRkTLA\ndqyQ35+IZhPTZZddxpQpUwDo2LEj8fHxEbWdhcjboudHtGiNFp0QeVqD2ZM6Gesx/ZD3+Q2gBVAH\nq9TaDngKK8wHVtm1gojEYjuMGCwvIip5+umnadGiBS1btqRHjx4kJiayePFidu/ezapVq1iyZAlV\nqvxl/XQ4HI6AEcwdxFgsSmmfiPyB5TscwXYVf4hIMlAGQETivPeNsd1FFrbAfJx70FyZ1EycNS+I\nEoqeU2uVZ+fOnYwZM4Zp/9/e2Qd3VV55/HNIECLvaAoYMKBVuyzyamorFJECssJYa0NbgUI7y3S1\nbVqcskKdosBU4+qMLbuj09VWbeuWl7WKrrstUpSCLlGEYkVLrJpUXoKkaBBCAgk5+8d5EmM2hEjy\nyy/33vOZ+c3v/u597r3Pd5jJw3NeH3mEbt26sWzZMnbt2sWJEyc477zz6NKlC2PGjOG9996LVcQE\nxC8KpCWSojUpOiF+WlO5QPw3MBPr8ZCBdZc7AGwBPosV89sGnAPMCtdfBXpjCXY1QC5Nuso1NTEV\nzPlCCiWkh3379pGZmUleXh69e/emZ8+e5OTksHfv3oYxt956K4MHD47Vdhbit0VviaRoTYpOiJ/W\nVJqY/gtbEN5U1ZHA1zGfwu9UtYeqdgf+ETMjKeaD+DGQh/kvngVOXyskhuTk5LBo0SLOP/98Bg0a\nRJ8+fcjLy+PgwYMAvPPOOzz++OPMnj07zTN1HCfOpDIP4oCI7OfDxj+fB8qB74nIPKwX9dlYctxj\nwGIsB+JkOP4WcHdL74hyJnXpXTMoLi7mK1/5SsO5t99+mxUrVjB//nyefPJJSkpK6Nu3L7NmzWLD\nhg0sXbqUQ4cO0bVrV+677z769vWeSo7jpI5UV3N9BbgIK9ZXilVx/TcsPyIDay16CdZm9A5gFOag\nPgtYrar/77/IcQ1zPXnyJLNmzeL+++9n9+7dvPTSS9xyi7XwXr9+Pa+88krD7zgTlTDB9iApWpOi\nE6KjtTNUcxWs1/QurIT3i1g1197h+lCsgN9PgfuwaKfz9MOGQe+d7h1xqua6fv16veKKK1RVtaio\nSIcPH66VlZVaV1en8+bN04KCgjTPsGOISjXM9iApWpOiUzU6WukE1VwVOBZ+dg2f/qr6QTj3Rczk\npMA7wIWqul9EemDZ1hki0i1V8+toKioqyM/Pb8iE3rp160eur169mhtuuAGAyy+/nPz8fMaOHcul\nl15KXV0dM2fOTMe0HcdJMKk0Ma3CKrUOCKeOAs9jkUkXYgtGFWZWOgA8gvkpsrCFpUQtV6LpcyNp\nYiosLGTkyJHMmDGDmpoajh8/3rAVrampIT8/n4cffpj+/fs3e39Utq5tJSk6ITlak6IToqM17Sam\nsPD8AliAOao3YeGtdwNLwvXfAc+H428DD2PJdKVYyGuXlp4fFRNTRUWFDh06VOvq6pq9vm7dOp06\ndWqLz4jK1rWtJEWnanK0JkWnanS0km4Tk4j0ASYCP1fVCixsdTxWlO8XYdgPsbBWMH/FH7FaTXOA\ng8QkzLWkpITs7Gy+8Y1vMGbMGBYsWPCR/tGrVq1qMC85juN0FlKZKDcWK5/xsIiMBgZiu4RBqloW\nxnyWD0ttvAGswHYc+4FxWMLcS40f2lkzqS/N6XPKa8XFxWzfvp2amhoOHTrEmjVr2LdvH4sXL6aq\nqorf/va3zJ07t8UMzLhlaJ6KpOiE5GhNik6IodbWbDPO5AN8FXNAH8VKbBzDOszVhOMqoAI4Esbf\nG8bXYbkQJ4D5Lb0jKiamsrIy7dGjhz744IOqqrpx48bTmpSaEpWta1tJik7V5GhNik7V6Ggl3SYm\nzOdQCSxU1W7ADKxL3AlggapmYTuKjDD+X4HXVLWLqmZgiXTbUji/DiMrK4va2lomTJgAwObNmxk1\nalSaZ+U4jtMyqTQxVYXnbwm/rwT+hBXkmwn8OnyXhuvdCeYmEZkK1Krq6y2+oBNlUpfeNeOU10pK\nShg2bBijRo2ipqaGjIwMHnvssVOOdxzH6QykMsz109gu4iys5tMHWITSvwCzscWgDpioqi+IyJVh\nvIbPalWd08xzIxfmWlxczE033cTs2bNZsGABK1eupFu3btx4442tfkZUwufaSlJ0QnK0JkUnREdr\n2sNcsQikWqzPdFcs1+EhYB3Wk5pwXBqO5wGPh+Px4d4RLb0jKj6I4uJizcjIaAhz3bx5s15zzTUf\n6xlRsW22laToVE2O1qToVI2OVlrpg0iliWkvsFdVXwwd4g5jdZdGYFnUYOGu9baZGqBraBi0G/NV\nfCKF80sJQ4cOpVevXmRkZJCZmcnLL7/MsWPHyMrK4vrrr6e0tBQRYeLEiemequM4ToukupprmYh8\nAPTEGgG9jJXR+KuIVGBO6/3hlhewgn3VmPnpGLCz6XObmJg6XUhZdXU1K1eupE8fC3vdtGkTxcXF\nVFZWsmPHDjIzM6mqqqKsrOxjzT124XOnICk6ITlak6ITYqi1NduMM/0AT2FO6F3A37D+0zcD24Ey\nzJH9SKOxJ7BF4fVwfGFLz++MJqbc3FwtLy//yLmysjLNzc1t+O0mplOTFJ2qydGaFJ2q0dFKusNc\nQyb1pcAwVR2BhbGOVNUfYyW/S7EmQl3DLcOAh1R1tKoODwvKl1M1v1QhIkyZMoVx48bxwAMPADBw\n4ECGDBlCcXExABs3bmT48OHpnKbjOM5p6fBMahGZDtyChb0+BKwJ498Arg4+iIuxIn/Hmz60M2RS\nt5Q1ffToUUSEuro6CgoKqKqqYtSoUcyfP59rr72W2tpaBg0axOLFi93E1AxJ0QnJ0ZoUnRBDra3Z\nZpzJB3gOC1c9jpmYtgKvhd8nsCilk1g/iHOwfIm6cP4I8Gfgupbe0dlNTLfffrvec8897fLcqGxd\n20pSdKomR2tSdKpGRyvpNjEBKzE/w1/UTEy3YCGt3bAdwEHgTlW9EXNM/wBrM/pTVe0FvI/tKiJD\nZWUldXV1DcfPPPMMI0aMSPOsHMdxzoxURjGtE5Fb+WhP6tcbmZgU+GUYWykiO7CKrp0+k7o+a/rk\nyZNcdtll5OTk8PTTT/Puu+9y4MABhgwZAsDVV1/N9OnTO3x+juM47UEqM6mHAE9jjuoT2G7gSixC\nKRurwfQG8Jyq3igiQ7HaS30xM9ReYLSqVjd5bqfJpF67di3FxcUcO3aMwsJCAMrLy8nOzub9999n\n0aJFfPe7322XuktRydBsK0nRCcnRmhSdEB2tnSGTehCWBLcL6IUtBsOBv8PqML0JXNZofCawB1gV\nfp8DZLT0jnT6IPbs2aOTJ0/WjRs36owZM5od4z6Ij09SdKomR2tSdKpGRyvp9kGo9Xx4LRzXO51z\ngL9g5qbyJrdMCwvEoXDPIVU9mar5tZWFCxdSWFjIN7/5TbZts6KzlZWVHDlypOHYfRCO40SZVJqY\nugNFWHmNPVjY6hBgMbAQOBuYp6q/CuPvxUxHZ2GRTP+pqvObeW7aTUxbt26lqKiInJwcioqKKCkp\n4YknnmD//v0sXboUMP/ElClTmDt3bru8Mypb17aSFJ2QHK1J0QnR0doZTEyrsAJ9NZgP4gDwGayl\n6O1Ys6CvNRr/Phb2ehSLfqoBprb0jnSZmJYsWaIDBw7U7t27a79+/bRLly46Z86clL4zKlvXtpIU\nnarJ0ZoUnarR0UonMDHdgO0YngWWYjWXVFW/qKrLm7nlJuBRVe0JXIE5qseman5tobCwkPHjx/P8\n889z2223ce655/Loo4+me1qO4zjtSsrCXEVEsEzpsVj57vtU9cUWblkPLBOR14HzgbcwB3fT53ZY\nJvWpMqa3bt1KTU0NFRUV/OhHP+Lw4cMpz56MXYbmKUiKTkiO1qTohBhqbc0240w+wPVYrkN1+BzH\ndgk3h2PFTE+/b3TPGj7MtD4EdG/pHek0MeXk5Gi/fv20e/fubmJqR5KiUzU5WpOiUzU6Wkm3iQkr\nrTFOVbtjeQ9HsIJ8s7GIpT8Ad4VxiMhwrBR4b8yxnQWMTOH8zpjCwkKKiooYM2YMd9xxh5uYHMeJ\nJaks1lcLvN3oWLC2oxcDm8P5F4F7MB/F14G1qnpcROqwNqX9W3pBqjOpdy/7PBMnTuT48ePU1taS\nn5/P8uXmPlm4cCF33303W7ZsOc1THMdxokkqF4hc4A8i0gX7Y69Yye+FWIRSF2BCo/GfBcaLyA/D\n2HKs0dBHaNowaO30HikTsHXrVpYvX05WVha1tbUUFBQwYMAADh8+TE1NTUPOwwUXXOA+iHYiKToh\nOVqTohNiqLU1dqgz+WA7hp7hsx3LpP4MMA54Jpz7H6A6jP8FsKfR/T8H8lt6R0f6ICorK3XMmDFa\nVFTU4IPIzc3VAQMGaFZWlvsg2omk6FRNjtak6FSNjlbS7YMIkzgO/AZYi+U3qKpuV9VpqjoOy64+\nHG45wIfNgwAGA/tSNb/WUF1dTV5eHllZWfTs2ZOsrCwuv/xyCgsL2bt3L6WlpaxevZrJkye7D8Jx\nnNiRykxqwaq1Tsd8CSWq+kkRuQp4FHNcdwFWqur3ReQ7WKe5aqwvxHEgW5uU2+jITGpVpbq6mqys\nLCoqKpgzZw4333wzU6ZMaRizc+dO1qxZ01CsL1VEJUOzrSRFJyRHa1J0QnS0doZM6gmYL2E/ljVd\ni4W57sYqtb4BvIr1iACr4nonlv+wB8uq7tfSOzraxDRw4EAtKCjosHc2Jipb17aSFJ2qydGaFJ2q\n0dFKJzAxPY9lUv8Zy4l4C+gB/BX4vqpeDPx7WBhQ1QpVvVVVLwQmhgXi4lTNrzWUl5dz6NAhRo8e\nTXZ2NpmZmUybNi2dU3Icx+kwUmliygZ+BqzASnevAuZjO4kHMCd2L+B/VfWaMP6T4Z6hmKnpIlV9\nr8lzG2dSj7vtJw+2ea6nyph+6623uOuuu6irq6O2tpYTJ05w5513MmzYsDa/8+MSla1rW0mKTkiO\n1qTohOhoba2JKZULxHewonxl2EJwQlUvEZE3sYqtFVjGdJWqfk5EvoQtJjXheiYwUps0DGrMJZdc\nosXFxSmZf3OsWLGCs88+m0WLFnXYO+vZtGkTkyZN6vD3djRJ0QnJ0ZoUnRAdrSLSqgUilZnUOZij\nuTeWFT1EROqd07mqOhLIA0YBqOpvVPXvVXW0qg7H/BRpbaZQXl5ORUUFAFVVVWzYsIFPfepT6ZyS\n4zhOh5FKH8QPVHWwqg4Fvgo8q6pzMaf1lWHYZKyBECIyTEQyw3EuVnajNFXzaw1lZWVcddVVjBw5\nkry8PKZOncrMmTPTOSXHcZwOI2Umpo+8RGQSsEhVZ4rIBGAlZkKqBr6lqttF5GvAEszEVAesUNV1\np3nuEaDjbEzp5Vzgb+meRAeQFJ2QHK1J0QnR0ZqrqtmnG9QhC0SqEJGXW2NHiwNJ0ZoUnZAcrUnR\nCfHTmkofhOM4jhNhfIFwHMdxmiXqC8QD6Z5AB5IUrUnRCcnRmhSdEDOtkfZBOI7jOKkj6jsIx3Ec\nJ0X4AuE4juM0S2QXCBGZLiLFIvKmiCxJ93zaExF5SEQOisiuRuf6i8gGEflL+O6Xzjm2ByIyRESe\nE5HXReQ1EfleOB8rrSLSXUReEpFXgs7l4XysdDZGRDJE5I8i8nT4HTutIlIqIq+KyE4ReTmci5XO\nSC4QIpIB3Af8AzAcuEFEhqd3Vu3KI1gfjcYsATaq6kXAxvA76tRilX2HY90Gvx3+HeOm9TgwWVVH\nAaOB6SLyGeKnszHfwyo51xNXrVeF8kD1uQ+x0hnJBQL4NPCmqr6tqieA1cAX0jyndkNVNwPvNTn9\nBawtK+H7ug6dVApQ1TJV3RGOj2B/UHKImdZQgv9o+Nk1fJSY6axHRAYDM7DKzPXEUmszxEpnVBeI\nHKypUD17w7k4M0BVy8LxAWBAOifT3ojIUGAM8CIx1BpMLjuBg8AGVY2lzsBPgFuwkjn1xFGrAr8X\nke2hDQHETGdmuifgfHxUVUUkNvHJItIT612+UFU/sG61Rly0qrXOHS0ifYEnRGREk+ux0CkiM4GD\nob7apObGxEUrMEFV94nIJ4ANIrK78cU46IzqDmIf1q2unsHhXJx5V0QGAYTvg2meT7sgIl2xxeE/\nVPXxcDqWWsE6JwLPYT6mOOocD1wrIqWY6XdyKPMfO62qui98HwSewEzfsdIZ1QViG3BRKBF+FlZO\n/Kk0zynVPIV15CN8P5nGubQLYluFnwN/VtV7G12KlVYRyQ47B0QkC5iK9WaPlU5oscx/rLSKSA8R\n6VV/DEwDdhE3nVHNpBaRazBbZwbwkKrekeYptRsisgqYhJUOfhfrzLcOWAucj/X1/nLTdqxRI5R+\n3wK8yof26lsxP0RstIrISMxhmYH9p2ytqq4QkXOIkc6mNCnzHyutInIBtmsAM9X/WlXviJ3OqC4Q\njuM4TmqJqonJcRzHSTG+QDiO4zjN4guE4ziO0yy+QDiO4zjN4guE4ziO0yyeSe04zSAiJ7Hw23qu\nU9XSNE3HcdKCh7k6TjOIyFFV7dmB78tU1dqOep/jtAY3MTnOGSAig0Rkc+gFsEtEPhfOTxeRHaH3\nw8Zwrr+IrBORP4lIUUicQ0SWicivROQF4FehoN89IrItjP2nNEp0HDcxOc4pyArVVwFKVPWLTa7P\nBtaH7NkM4GwRyQYeBCaqaomI9A9jlwN/VNXrRGQy8EusLwRYP5MJqloVKoIeVtU8EekGvCAiz6hq\nSSqFOs6p8AXCcZqnSlVHt3B9G/BQKDa4TlV3htISm+v/oDcqsTAB+FI496yInCMivcO1p1S1KhxP\nA0aKSH743Qe4CPAFwkkLvkA4zhmgqptFZCLWGOcREbkXeP8MHlXZ6FiAAlVd3x5zdJy24j4IxzkD\nRCQXeFdVH8Q6p40FioCJIjIsjKk3MW0B5oRzk4C/qeoHzTx2PXBT2JUgIheHSqGOkxZ8B+E4Z8Yk\n4J9FpAY4CsxT1fLgR3hcRLpgvQCmAsswc9SfgGN8WA66KT8DhgI7Qin0ciLestKJNh7m6jiO4zSL\nm5gcx3GcZvEFwnEcx2kWXyAcx3GcZvEFwnEcx2kWXyAcx3GcZvEFwnEcx2kWXyAcx3GcZvk/nf0h\n2gkhMp0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1284deb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot feature importance using built-in function\n",
    "from xgboost import plot_importance\n",
    "plot_importance(XGB_model)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function plot_importance in module xgboost.plotting:\n",
      "\n",
      "plot_importance(booster, ax=None, height=0.2, xlim=None, ylim=None, title='Feature importance', xlabel='F score', ylabel='Features', importance_type='weight', max_num_features=None, grid=True, show_values=True, **kwargs)\n",
      "    Plot importance based on fitted trees.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    booster : Booster, XGBModel or dict\n",
      "        Booster or XGBModel instance, or dict taken by Booster.get_fscore()\n",
      "    ax : matplotlib Axes, default None\n",
      "        Target axes instance. If None, new figure and axes will be created.\n",
      "    grid : bool, Turn the axes grids on or off.  Default is True (On).\n",
      "    importance_type : str, default \"weight\"\n",
      "        How the importance is calculated: either \"weight\", \"gain\", or \"cover\"\n",
      "    \n",
      "        * \"weight\" is the number of times a feature appears in a tree\n",
      "        * \"gain\" is the average gain of splits which use the feature\n",
      "        * \"cover\" is the average coverage of splits which use the feature\n",
      "          where coverage is defined as the number of samples affected by the split\n",
      "    max_num_features : int, default None\n",
      "        Maximum number of top features displayed on plot. If None, all features will be displayed.\n",
      "    height : float, default 0.2\n",
      "        Bar height, passed to ax.barh()\n",
      "    xlim : tuple, default None\n",
      "        Tuple passed to axes.xlim()\n",
      "    ylim : tuple, default None\n",
      "        Tuple passed to axes.ylim()\n",
      "    title : str, default \"Feature importance\"\n",
      "        Axes title. To disable, pass None.\n",
      "    xlabel : str, default \"F score\"\n",
      "        X axis title label. To disable, pass None.\n",
      "    ylabel : str, default \"Features\"\n",
      "        Y axis title label. To disable, pass None.\n",
      "    show_values : bool, default True\n",
      "        Show values on plot. To disable, pass False.\n",
      "    kwargs :\n",
      "        Other keywords passed to ax.barh()\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    ax : matplotlib Axes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(plot_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True, False, False,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "        True, False,  True,  True, False,  True,  True,  True,  True,\n",
       "       False,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True, False,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor()\n",
    "gbdt_RFE = RFE(model, int(0.8*X_train.shape[1]))\n",
    "gbdt_RFE.fit(X_train, y_train)\n",
    "gbdt_RFE.ranking_\n",
    "#特征选择输出结果\n",
    "#gbdt_RFE.support_\n",
    "#输出结果为：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 64)"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>61</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>95</th>\n",
       "      <th>98</th>\n",
       "      <th>111</th>\n",
       "      <th>...</th>\n",
       "      <th>610</th>\n",
       "      <th>611</th>\n",
       "      <th>619</th>\n",
       "      <th>624</th>\n",
       "      <th>625</th>\n",
       "      <th>627</th>\n",
       "      <th>685</th>\n",
       "      <th>690</th>\n",
       "      <th>702</th>\n",
       "      <th>726</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>0.08763</td>\n",
       "      <td>0.00227</td>\n",
       "      <td>0.00337</td>\n",
       "      <td>0.00912</td>\n",
       "      <td>0.92729</td>\n",
       "      <td>0.07266</td>\n",
       "      <td>0.30569</td>\n",
       "      <td>0.02557</td>\n",
       "      <td>0.03173</td>\n",
       "      <td>0.26319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.83851</td>\n",
       "      <td>0.16932</td>\n",
       "      <td>0.27568</td>\n",
       "      <td>0.65651</td>\n",
       "      <td>0.42383</td>\n",
       "      <td>2.21426</td>\n",
       "      <td>0.49426</td>\n",
       "      <td>0.75520</td>\n",
       "      <td>13.60813</td>\n",
       "      <td>0.02305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>0.11185</td>\n",
       "      <td>0.00464</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.17030</td>\n",
       "      <td>1.00166</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.33259</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.35661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.48900</td>\n",
       "      <td>8.13686</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.28955</td>\n",
       "      <td>5.02893</td>\n",
       "      <td>0.06792</td>\n",
       "      <td>0.16304</td>\n",
       "      <td>9.56091</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.01344</td>\n",
       "      <td>0.00286</td>\n",
       "      <td>0.02261</td>\n",
       "      <td>0.10007</td>\n",
       "      <td>3.92172</td>\n",
       "      <td>0.08675</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01937</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.36165</td>\n",
       "      <td>...</td>\n",
       "      <td>2.02262</td>\n",
       "      <td>1.06370</td>\n",
       "      <td>5.25088</td>\n",
       "      <td>0.55415</td>\n",
       "      <td>0.67356</td>\n",
       "      <td>1.45788</td>\n",
       "      <td>0.38973</td>\n",
       "      <td>0.30366</td>\n",
       "      <td>16.02550</td>\n",
       "      <td>0.06081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>0.00655</td>\n",
       "      <td>0.00300</td>\n",
       "      <td>0.04929</td>\n",
       "      <td>0.39836</td>\n",
       "      <td>0.80423</td>\n",
       "      <td>0.01613</td>\n",
       "      <td>0.28416</td>\n",
       "      <td>0.02062</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.28807</td>\n",
       "      <td>...</td>\n",
       "      <td>4.94547</td>\n",
       "      <td>1.50510</td>\n",
       "      <td>14.74455</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.39289</td>\n",
       "      <td>6.21633</td>\n",
       "      <td>0.00328</td>\n",
       "      <td>0.12309</td>\n",
       "      <td>8.45865</td>\n",
       "      <td>0.03540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>0.48082</td>\n",
       "      <td>0.09798</td>\n",
       "      <td>0.29394</td>\n",
       "      <td>0.55871</td>\n",
       "      <td>2.58449</td>\n",
       "      <td>0.11017</td>\n",
       "      <td>0.26504</td>\n",
       "      <td>1.04471</td>\n",
       "      <td>0.17303</td>\n",
       "      <td>0.14684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.16139</td>\n",
       "      <td>0.24277</td>\n",
       "      <td>2.22713</td>\n",
       "      <td>1.98828</td>\n",
       "      <td>1.50117</td>\n",
       "      <td>7.18742</td>\n",
       "      <td>0.10728</td>\n",
       "      <td>0.16467</td>\n",
       "      <td>8.81397</td>\n",
       "      <td>0.02787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>0.03204</td>\n",
       "      <td>0.04425</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10824</td>\n",
       "      <td>0.86262</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03526</td>\n",
       "      <td>0.05876</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.18182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12252</td>\n",
       "      <td>0.00038</td>\n",
       "      <td>3.94376</td>\n",
       "      <td>0.02213</td>\n",
       "      <td>0.48827</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00654</td>\n",
       "      <td>0.15705</td>\n",
       "      <td>18.24170</td>\n",
       "      <td>0.02871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>0.62166</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01723</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>0.93107</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.09023</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.08427</td>\n",
       "      <td>...</td>\n",
       "      <td>6.69592</td>\n",
       "      <td>1.02801</td>\n",
       "      <td>10.82138</td>\n",
       "      <td>0.40760</td>\n",
       "      <td>2.40580</td>\n",
       "      <td>1.55032</td>\n",
       "      <td>0.10949</td>\n",
       "      <td>0.17668</td>\n",
       "      <td>3.61774</td>\n",
       "      <td>0.34012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10927</td>\n",
       "      <td>0.00122</td>\n",
       "      <td>0.01962</td>\n",
       "      <td>0.07067</td>\n",
       "      <td>0.02285</td>\n",
       "      <td>0.44075</td>\n",
       "      <td>0.04095</td>\n",
       "      <td>0.01397</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.28673</td>\n",
       "      <td>0.26287</td>\n",
       "      <td>10.17593</td>\n",
       "      <td>0.60657</td>\n",
       "      <td>0.98747</td>\n",
       "      <td>2.51809</td>\n",
       "      <td>0.00495</td>\n",
       "      <td>0.08544</td>\n",
       "      <td>18.55156</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>0.08739</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06191</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.36653</td>\n",
       "      <td>0.00192</td>\n",
       "      <td>0.20238</td>\n",
       "      <td>0.19706</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.17877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01116</td>\n",
       "      <td>4.06874</td>\n",
       "      <td>9.49282</td>\n",
       "      <td>1.23557</td>\n",
       "      <td>0.39896</td>\n",
       "      <td>0.60494</td>\n",
       "      <td>0.22398</td>\n",
       "      <td>0.59859</td>\n",
       "      <td>4.55491</td>\n",
       "      <td>0.39903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>0.16399</td>\n",
       "      <td>0.79895</td>\n",
       "      <td>0.02749</td>\n",
       "      <td>2.04479</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03040</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03699</td>\n",
       "      <td>0.01799</td>\n",
       "      <td>0.70545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.60606</td>\n",
       "      <td>0.46186</td>\n",
       "      <td>6.79872</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.62241</td>\n",
       "      <td>1.74928</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.02290</td>\n",
       "      <td>8.94006</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06799</td>\n",
       "      <td>0.03393</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.28796</td>\n",
       "      <td>0.52041</td>\n",
       "      <td>17.19084</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.99326</td>\n",
       "      <td>3.54834</td>\n",
       "      <td>0.97580</td>\n",
       "      <td>1.59456</td>\n",
       "      <td>1.86928</td>\n",
       "      <td>0.20449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.57254</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10735</td>\n",
       "      <td>0.04081</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00103</td>\n",
       "      <td>0.06429</td>\n",
       "      <td>8.20110</td>\n",
       "      <td>0.66091</td>\n",
       "      <td>0.46247</td>\n",
       "      <td>3.41952</td>\n",
       "      <td>0.01496</td>\n",
       "      <td>0.11130</td>\n",
       "      <td>2.60901</td>\n",
       "      <td>0.00958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>0.21623</td>\n",
       "      <td>0.07427</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.48242</td>\n",
       "      <td>1.78546</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.11366</td>\n",
       "      <td>0.06596</td>\n",
       "      <td>0.04611</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06108</td>\n",
       "      <td>0.47826</td>\n",
       "      <td>7.84393</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.73410</td>\n",
       "      <td>3.10690</td>\n",
       "      <td>0.16846</td>\n",
       "      <td>1.76587</td>\n",
       "      <td>1.55576</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>0.10642</td>\n",
       "      <td>0.03069</td>\n",
       "      <td>0.00520</td>\n",
       "      <td>0.12162</td>\n",
       "      <td>1.07610</td>\n",
       "      <td>0.02478</td>\n",
       "      <td>2.11103</td>\n",
       "      <td>0.13262</td>\n",
       "      <td>0.07617</td>\n",
       "      <td>0.03921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.14875</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.36625</td>\n",
       "      <td>4.50622</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06290</td>\n",
       "      <td>20.08717</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>0.03959</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.67538</td>\n",
       "      <td>0.34223</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.24728</td>\n",
       "      <td>0.10485</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.92156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.08811</td>\n",
       "      <td>1.27991</td>\n",
       "      <td>1.04462</td>\n",
       "      <td>0.77945</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.12353</td>\n",
       "      <td>0.07120</td>\n",
       "      <td>27.61270</td>\n",
       "      <td>0.05802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03388</td>\n",
       "      <td>0.02393</td>\n",
       "      <td>0.00055</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07239</td>\n",
       "      <td>0.18708</td>\n",
       "      <td>0.03324</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57961</td>\n",
       "      <td>1.90755</td>\n",
       "      <td>1.48208</td>\n",
       "      <td>0.28730</td>\n",
       "      <td>1.61595</td>\n",
       "      <td>0.09017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00317</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.08281</td>\n",
       "      <td>1.23321</td>\n",
       "      <td>0.01195</td>\n",
       "      <td>0.10511</td>\n",
       "      <td>0.20621</td>\n",
       "      <td>0.03924</td>\n",
       "      <td>0.04807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49849</td>\n",
       "      <td>1.60531</td>\n",
       "      <td>18.60599</td>\n",
       "      <td>0.21709</td>\n",
       "      <td>0.76444</td>\n",
       "      <td>0.63998</td>\n",
       "      <td>0.01888</td>\n",
       "      <td>0.03818</td>\n",
       "      <td>7.99493</td>\n",
       "      <td>0.36140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00556</td>\n",
       "      <td>0.00451</td>\n",
       "      <td>0.03041</td>\n",
       "      <td>1.42271</td>\n",
       "      <td>0.07010</td>\n",
       "      <td>0.41342</td>\n",
       "      <td>0.12524</td>\n",
       "      <td>0.02375</td>\n",
       "      <td>0.45324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.55632</td>\n",
       "      <td>1.49514</td>\n",
       "      <td>9.96385</td>\n",
       "      <td>0.00287</td>\n",
       "      <td>2.09317</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.07726</td>\n",
       "      <td>0.63980</td>\n",
       "      <td>12.13454</td>\n",
       "      <td>0.08526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00744</td>\n",
       "      <td>0.04946</td>\n",
       "      <td>0.88820</td>\n",
       "      <td>2.53431</td>\n",
       "      <td>0.07960</td>\n",
       "      <td>0.30497</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.19612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02549</td>\n",
       "      <td>0.04222</td>\n",
       "      <td>19.33150</td>\n",
       "      <td>0.00909</td>\n",
       "      <td>0.31635</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06417</td>\n",
       "      <td>0.21777</td>\n",
       "      <td>12.71725</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>0.52340</td>\n",
       "      <td>0.00060</td>\n",
       "      <td>0.01474</td>\n",
       "      <td>0.71126</td>\n",
       "      <td>1.69977</td>\n",
       "      <td>0.04472</td>\n",
       "      <td>0.86150</td>\n",
       "      <td>0.10590</td>\n",
       "      <td>0.13163</td>\n",
       "      <td>0.57698</td>\n",
       "      <td>...</td>\n",
       "      <td>1.75720</td>\n",
       "      <td>0.16714</td>\n",
       "      <td>0.14412</td>\n",
       "      <td>0.14576</td>\n",
       "      <td>2.96856</td>\n",
       "      <td>9.51526</td>\n",
       "      <td>0.08812</td>\n",
       "      <td>0.05155</td>\n",
       "      <td>11.66694</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>0.03925</td>\n",
       "      <td>0.12320</td>\n",
       "      <td>0.04456</td>\n",
       "      <td>0.04057</td>\n",
       "      <td>1.41700</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.34554</td>\n",
       "      <td>0.09800</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.16040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.34371</td>\n",
       "      <td>0.16806</td>\n",
       "      <td>9.83182</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.22405</td>\n",
       "      <td>1.19074</td>\n",
       "      <td>0.24004</td>\n",
       "      <td>0.38612</td>\n",
       "      <td>19.44191</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>0.02677</td>\n",
       "      <td>0.17584</td>\n",
       "      <td>0.00185</td>\n",
       "      <td>0.00995</td>\n",
       "      <td>1.10170</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.31427</td>\n",
       "      <td>0.02299</td>\n",
       "      <td>0.02245</td>\n",
       "      <td>0.09402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.61360</td>\n",
       "      <td>0.24445</td>\n",
       "      <td>3.58854</td>\n",
       "      <td>0.24322</td>\n",
       "      <td>1.04123</td>\n",
       "      <td>2.94950</td>\n",
       "      <td>0.47468</td>\n",
       "      <td>0.29521</td>\n",
       "      <td>13.68700</td>\n",
       "      <td>0.10742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.74899</td>\n",
       "      <td>0.01454</td>\n",
       "      <td>0.01301</td>\n",
       "      <td>7.02022</td>\n",
       "      <td>0.03341</td>\n",
       "      <td>0.25387</td>\n",
       "      <td>0.01831</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.18154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.29459</td>\n",
       "      <td>0.12818</td>\n",
       "      <td>2.68373</td>\n",
       "      <td>1.58893</td>\n",
       "      <td>2.52668</td>\n",
       "      <td>1.39579</td>\n",
       "      <td>2.93867</td>\n",
       "      <td>1.56245</td>\n",
       "      <td>0.01821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>0.08325</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.09326</td>\n",
       "      <td>1.61794</td>\n",
       "      <td>0.02219</td>\n",
       "      <td>1.35493</td>\n",
       "      <td>0.02594</td>\n",
       "      <td>0.13850</td>\n",
       "      <td>0.06231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01847</td>\n",
       "      <td>0.12864</td>\n",
       "      <td>1.05454</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.21822</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00173</td>\n",
       "      <td>3.03937</td>\n",
       "      <td>0.02287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>0.38714</td>\n",
       "      <td>0.31513</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.91256</td>\n",
       "      <td>3.36346</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.14312</td>\n",
       "      <td>0.02889</td>\n",
       "      <td>0.00447</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00966</td>\n",
       "      <td>1.25082</td>\n",
       "      <td>7.67077</td>\n",
       "      <td>1.59254</td>\n",
       "      <td>0.21616</td>\n",
       "      <td>3.99135</td>\n",
       "      <td>0.13552</td>\n",
       "      <td>0.21298</td>\n",
       "      <td>3.02060</td>\n",
       "      <td>0.01936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.15802</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45629</td>\n",
       "      <td>0.18813</td>\n",
       "      <td>0.05725</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.59429</td>\n",
       "      <td>15.91178</td>\n",
       "      <td>1.05093</td>\n",
       "      <td>1.29294</td>\n",
       "      <td>2.84454</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>0.00092</td>\n",
       "      <td>0.17161</td>\n",
       "      <td>0.04507</td>\n",
       "      <td>0.04004</td>\n",
       "      <td>1.93385</td>\n",
       "      <td>0.33382</td>\n",
       "      <td>1.41513</td>\n",
       "      <td>0.02264</td>\n",
       "      <td>0.10252</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02247</td>\n",
       "      <td>0.32285</td>\n",
       "      <td>15.48562</td>\n",
       "      <td>2.73094</td>\n",
       "      <td>0.39106</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02408</td>\n",
       "      <td>0.11431</td>\n",
       "      <td>11.65351</td>\n",
       "      <td>0.00510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04132</td>\n",
       "      <td>0.23878</td>\n",
       "      <td>0.36092</td>\n",
       "      <td>0.00490</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.21457</td>\n",
       "      <td>0.01038</td>\n",
       "      <td>0.00760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>15.06748</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.27323</td>\n",
       "      <td>13.47859</td>\n",
       "      <td>0.10374</td>\n",
       "      <td>0.05318</td>\n",
       "      <td>1.28525</td>\n",
       "      <td>0.00228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>0.03936</td>\n",
       "      <td>0.46514</td>\n",
       "      <td>0.04757</td>\n",
       "      <td>3.56184</td>\n",
       "      <td>4.46240</td>\n",
       "      <td>0.01132</td>\n",
       "      <td>0.58186</td>\n",
       "      <td>0.07696</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.53356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.47669</td>\n",
       "      <td>0.27833</td>\n",
       "      <td>8.10831</td>\n",
       "      <td>0.25307</td>\n",
       "      <td>0.58186</td>\n",
       "      <td>2.36701</td>\n",
       "      <td>0.00762</td>\n",
       "      <td>0.01089</td>\n",
       "      <td>11.69976</td>\n",
       "      <td>0.01633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>0.15101</td>\n",
       "      <td>0.03238</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.16287</td>\n",
       "      <td>0.11999</td>\n",
       "      <td>0.81041</td>\n",
       "      <td>0.07492</td>\n",
       "      <td>0.31291</td>\n",
       "      <td>...</td>\n",
       "      <td>1.65237</td>\n",
       "      <td>2.94848</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.16052</td>\n",
       "      <td>0.93960</td>\n",
       "      <td>2.67069</td>\n",
       "      <td>0.24669</td>\n",
       "      <td>0.14594</td>\n",
       "      <td>15.74064</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>0.11893</td>\n",
       "      <td>0.21598</td>\n",
       "      <td>0.00979</td>\n",
       "      <td>0.23204</td>\n",
       "      <td>0.94470</td>\n",
       "      <td>0.00041</td>\n",
       "      <td>0.08021</td>\n",
       "      <td>0.09491</td>\n",
       "      <td>0.03694</td>\n",
       "      <td>0.09817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03133</td>\n",
       "      <td>3.16531</td>\n",
       "      <td>6.05944</td>\n",
       "      <td>4.30551</td>\n",
       "      <td>1.82837</td>\n",
       "      <td>3.29844</td>\n",
       "      <td>0.11575</td>\n",
       "      <td>0.20157</td>\n",
       "      <td>22.97987</td>\n",
       "      <td>0.02964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>0.07215</td>\n",
       "      <td>0.10029</td>\n",
       "      <td>0.08307</td>\n",
       "      <td>0.18538</td>\n",
       "      <td>1.64980</td>\n",
       "      <td>0.02368</td>\n",
       "      <td>0.71693</td>\n",
       "      <td>0.05739</td>\n",
       "      <td>0.05869</td>\n",
       "      <td>0.86721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00662</td>\n",
       "      <td>15.13028</td>\n",
       "      <td>0.28657</td>\n",
       "      <td>3.38619</td>\n",
       "      <td>1.52202</td>\n",
       "      <td>0.29834</td>\n",
       "      <td>3.50237</td>\n",
       "      <td>13.62461</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.07089</td>\n",
       "      <td>0.00639</td>\n",
       "      <td>0.11392</td>\n",
       "      <td>3.05638</td>\n",
       "      <td>0.06994</td>\n",
       "      <td>0.18336</td>\n",
       "      <td>0.75960</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.89669</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.86071</td>\n",
       "      <td>0.44691</td>\n",
       "      <td>1.19796</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.20189</td>\n",
       "      <td>0.09815</td>\n",
       "      <td>8.58969</td>\n",
       "      <td>0.02892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>0.00703</td>\n",
       "      <td>0.19454</td>\n",
       "      <td>0.06780</td>\n",
       "      <td>0.02968</td>\n",
       "      <td>1.22271</td>\n",
       "      <td>0.34368</td>\n",
       "      <td>0.15662</td>\n",
       "      <td>0.66788</td>\n",
       "      <td>0.03938</td>\n",
       "      <td>0.03713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.31612</td>\n",
       "      <td>3.10802</td>\n",
       "      <td>1.27069</td>\n",
       "      <td>2.08935</td>\n",
       "      <td>1.79695</td>\n",
       "      <td>0.02809</td>\n",
       "      <td>0.15730</td>\n",
       "      <td>17.67187</td>\n",
       "      <td>0.06342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>0.16223</td>\n",
       "      <td>0.01664</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10088</td>\n",
       "      <td>0.44595</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.17010</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.01790</td>\n",
       "      <td>1.28874</td>\n",
       "      <td>7.93466</td>\n",
       "      <td>0.02735</td>\n",
       "      <td>1.09487</td>\n",
       "      <td>2.88139</td>\n",
       "      <td>0.43301</td>\n",
       "      <td>1.97004</td>\n",
       "      <td>4.91888</td>\n",
       "      <td>0.03141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>0.19884</td>\n",
       "      <td>0.05867</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.01760</td>\n",
       "      <td>2.28953</td>\n",
       "      <td>0.00133</td>\n",
       "      <td>0.27898</td>\n",
       "      <td>0.04729</td>\n",
       "      <td>0.01899</td>\n",
       "      <td>0.18190</td>\n",
       "      <td>...</td>\n",
       "      <td>1.44875</td>\n",
       "      <td>1.11747</td>\n",
       "      <td>7.10904</td>\n",
       "      <td>0.77241</td>\n",
       "      <td>0.93631</td>\n",
       "      <td>1.75188</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8.10803</td>\n",
       "      <td>0.00140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>0.03281</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01095</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03420</td>\n",
       "      <td>0.02132</td>\n",
       "      <td>0.04179</td>\n",
       "      <td>0.01564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15863</td>\n",
       "      <td>0.48677</td>\n",
       "      <td>10.82479</td>\n",
       "      <td>0.00214</td>\n",
       "      <td>0.60281</td>\n",
       "      <td>8.56986</td>\n",
       "      <td>0.09585</td>\n",
       "      <td>1.16608</td>\n",
       "      <td>1.05830</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>0.01445</td>\n",
       "      <td>0.02444</td>\n",
       "      <td>0.21956</td>\n",
       "      <td>0.64741</td>\n",
       "      <td>3.02082</td>\n",
       "      <td>0.08567</td>\n",
       "      <td>2.04000</td>\n",
       "      <td>0.09282</td>\n",
       "      <td>0.00193</td>\n",
       "      <td>0.20372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26387</td>\n",
       "      <td>0.61143</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.80980</td>\n",
       "      <td>4.02034</td>\n",
       "      <td>0.05768</td>\n",
       "      <td>0.04275</td>\n",
       "      <td>7.76304</td>\n",
       "      <td>0.01289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.10041</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02829</td>\n",
       "      <td>0.02124</td>\n",
       "      <td>1.36358</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.11230</td>\n",
       "      <td>0.00971</td>\n",
       "      <td>0.13150</td>\n",
       "      <td>0.09578</td>\n",
       "      <td>...</td>\n",
       "      <td>1.19045</td>\n",
       "      <td>0.04916</td>\n",
       "      <td>5.67935</td>\n",
       "      <td>0.42865</td>\n",
       "      <td>1.19442</td>\n",
       "      <td>2.34436</td>\n",
       "      <td>0.04061</td>\n",
       "      <td>0.12346</td>\n",
       "      <td>3.39289</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>0.31539</td>\n",
       "      <td>0.24011</td>\n",
       "      <td>0.20641</td>\n",
       "      <td>11.84443</td>\n",
       "      <td>1.20010</td>\n",
       "      <td>0.05259</td>\n",
       "      <td>0.84423</td>\n",
       "      <td>0.01607</td>\n",
       "      <td>0.02909</td>\n",
       "      <td>0.08182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13834</td>\n",
       "      <td>0.92708</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.36068</td>\n",
       "      <td>8.99791</td>\n",
       "      <td>0.00957</td>\n",
       "      <td>0.00346</td>\n",
       "      <td>4.43547</td>\n",
       "      <td>0.00625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>0.02400</td>\n",
       "      <td>0.07328</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.18027</td>\n",
       "      <td>0.00088</td>\n",
       "      <td>1.26931</td>\n",
       "      <td>0.27152</td>\n",
       "      <td>0.07979</td>\n",
       "      <td>0.36282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.17504</td>\n",
       "      <td>0.00607</td>\n",
       "      <td>2.59344</td>\n",
       "      <td>0.00274</td>\n",
       "      <td>0.41229</td>\n",
       "      <td>0.89011</td>\n",
       "      <td>1.65572</td>\n",
       "      <td>4.76506</td>\n",
       "      <td>41.94086</td>\n",
       "      <td>0.00320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>0.18375</td>\n",
       "      <td>0.41022</td>\n",
       "      <td>0.23257</td>\n",
       "      <td>2.33253</td>\n",
       "      <td>2.74029</td>\n",
       "      <td>0.16498</td>\n",
       "      <td>2.49418</td>\n",
       "      <td>0.01408</td>\n",
       "      <td>0.02382</td>\n",
       "      <td>0.07328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08249</td>\n",
       "      <td>0.00331</td>\n",
       "      <td>0.08742</td>\n",
       "      <td>0.02905</td>\n",
       "      <td>1.63606</td>\n",
       "      <td>7.71727</td>\n",
       "      <td>0.06754</td>\n",
       "      <td>0.04890</td>\n",
       "      <td>2.76216</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>0.41226</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.16198</td>\n",
       "      <td>2.57775</td>\n",
       "      <td>2.49586</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.19276</td>\n",
       "      <td>0.21512</td>\n",
       "      <td>0.34521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.39470</td>\n",
       "      <td>0.08788</td>\n",
       "      <td>25.12031</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.73046</td>\n",
       "      <td>3.50272</td>\n",
       "      <td>0.05414</td>\n",
       "      <td>0.05736</td>\n",
       "      <td>17.13915</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>0.06772</td>\n",
       "      <td>0.03927</td>\n",
       "      <td>0.02860</td>\n",
       "      <td>0.01518</td>\n",
       "      <td>2.09982</td>\n",
       "      <td>0.04961</td>\n",
       "      <td>0.16558</td>\n",
       "      <td>0.04411</td>\n",
       "      <td>0.02699</td>\n",
       "      <td>0.06013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.16921</td>\n",
       "      <td>0.02282</td>\n",
       "      <td>0.01627</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.52314</td>\n",
       "      <td>2.94293</td>\n",
       "      <td>0.00601</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>34.90021</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>0.47196</td>\n",
       "      <td>0.00595</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.46991</td>\n",
       "      <td>2.19166</td>\n",
       "      <td>0.00071</td>\n",
       "      <td>0.32279</td>\n",
       "      <td>0.05733</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.13343</td>\n",
       "      <td>...</td>\n",
       "      <td>3.21554</td>\n",
       "      <td>0.38801</td>\n",
       "      <td>0.01076</td>\n",
       "      <td>0.14470</td>\n",
       "      <td>1.33992</td>\n",
       "      <td>1.78839</td>\n",
       "      <td>0.00066</td>\n",
       "      <td>0.02777</td>\n",
       "      <td>10.35255</td>\n",
       "      <td>0.02103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>0.03488</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01212</td>\n",
       "      <td>0.19092</td>\n",
       "      <td>1.32727</td>\n",
       "      <td>0.00141</td>\n",
       "      <td>0.43158</td>\n",
       "      <td>0.41976</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.15057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01016</td>\n",
       "      <td>0.13042</td>\n",
       "      <td>9.12609</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.19889</td>\n",
       "      <td>8.14107</td>\n",
       "      <td>0.10427</td>\n",
       "      <td>0.33440</td>\n",
       "      <td>17.03688</td>\n",
       "      <td>0.40858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04595</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.02557</td>\n",
       "      <td>4.17462</td>\n",
       "      <td>0.12246</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.51764</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.14733</td>\n",
       "      <td>0.21369</td>\n",
       "      <td>0.23934</td>\n",
       "      <td>0.02878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>0.80258</td>\n",
       "      <td>0.02299</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.17244</td>\n",
       "      <td>0.99433</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.15240</td>\n",
       "      <td>0.01688</td>\n",
       "      <td>0.00283</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.07277</td>\n",
       "      <td>0.40821</td>\n",
       "      <td>11.77548</td>\n",
       "      <td>1.55913</td>\n",
       "      <td>2.48063</td>\n",
       "      <td>9.59993</td>\n",
       "      <td>0.16984</td>\n",
       "      <td>1.16010</td>\n",
       "      <td>5.36634</td>\n",
       "      <td>0.05464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.46958</td>\n",
       "      <td>0.65207</td>\n",
       "      <td>0.21086</td>\n",
       "      <td>0.86716</td>\n",
       "      <td>3.08996</td>\n",
       "      <td>0.33079</td>\n",
       "      <td>0.86090</td>\n",
       "      <td>0.26517</td>\n",
       "      <td>0.01375</td>\n",
       "      <td>0.02154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00914</td>\n",
       "      <td>0.50068</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.05919</td>\n",
       "      <td>6.12105</td>\n",
       "      <td>0.02726</td>\n",
       "      <td>0.03208</td>\n",
       "      <td>4.03417</td>\n",
       "      <td>0.00342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.03653</td>\n",
       "      <td>0.00244</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.39785</td>\n",
       "      <td>1.30939</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03210</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02293</td>\n",
       "      <td>0.01404</td>\n",
       "      <td>9.31312</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.57852</td>\n",
       "      <td>0.86095</td>\n",
       "      <td>0.07704</td>\n",
       "      <td>0.10298</td>\n",
       "      <td>8.54457</td>\n",
       "      <td>0.00419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>0.02664</td>\n",
       "      <td>0.39996</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.58753</td>\n",
       "      <td>0.59089</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01665</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.45965</td>\n",
       "      <td>0.67608</td>\n",
       "      <td>9.27991</td>\n",
       "      <td>0.04502</td>\n",
       "      <td>0.56325</td>\n",
       "      <td>2.61889</td>\n",
       "      <td>0.02901</td>\n",
       "      <td>0.38185</td>\n",
       "      <td>0.56248</td>\n",
       "      <td>0.00963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.15636</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.38368</td>\n",
       "      <td>2.69387</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01724</td>\n",
       "      <td>0.15919</td>\n",
       "      <td>0.36902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01783</td>\n",
       "      <td>0.07011</td>\n",
       "      <td>7.23383</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.63671</td>\n",
       "      <td>1.91881</td>\n",
       "      <td>2.03660</td>\n",
       "      <td>2.94334</td>\n",
       "      <td>15.12912</td>\n",
       "      <td>0.07795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>0.30213</td>\n",
       "      <td>0.00345</td>\n",
       "      <td>0.26163</td>\n",
       "      <td>0.43681</td>\n",
       "      <td>0.09306</td>\n",
       "      <td>0.02819</td>\n",
       "      <td>0.81927</td>\n",
       "      <td>2.36517</td>\n",
       "      <td>0.21785</td>\n",
       "      <td>0.27346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01652</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>22.17197</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.68206</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01335</td>\n",
       "      <td>0.06099</td>\n",
       "      <td>13.66552</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0.44670</td>\n",
       "      <td>0.01529</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.14309</td>\n",
       "      <td>0.04073</td>\n",
       "      <td>0.00063</td>\n",
       "      <td>0.01549</td>\n",
       "      <td>0.03583</td>\n",
       "      <td>0.06983</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00226</td>\n",
       "      <td>0.02863</td>\n",
       "      <td>5.96757</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.58703</td>\n",
       "      <td>2.20485</td>\n",
       "      <td>9.19442</td>\n",
       "      <td>7.26994</td>\n",
       "      <td>3.08273</td>\n",
       "      <td>0.00428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.11001</td>\n",
       "      <td>0.06098</td>\n",
       "      <td>0.11831</td>\n",
       "      <td>0.38983</td>\n",
       "      <td>0.06181</td>\n",
       "      <td>0.47201</td>\n",
       "      <td>0.43311</td>\n",
       "      <td>0.07540</td>\n",
       "      <td>0.01925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04030</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>18.36034</td>\n",
       "      <td>0.15540</td>\n",
       "      <td>0.49918</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>27.88353</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>0.07047</td>\n",
       "      <td>0.05192</td>\n",
       "      <td>0.07896</td>\n",
       "      <td>0.17521</td>\n",
       "      <td>1.31529</td>\n",
       "      <td>0.02014</td>\n",
       "      <td>1.15965</td>\n",
       "      <td>0.46545</td>\n",
       "      <td>0.13387</td>\n",
       "      <td>0.27208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07419</td>\n",
       "      <td>0.02115</td>\n",
       "      <td>3.01032</td>\n",
       "      <td>4.34735</td>\n",
       "      <td>1.22595</td>\n",
       "      <td>1.35483</td>\n",
       "      <td>0.08121</td>\n",
       "      <td>0.18296</td>\n",
       "      <td>28.73081</td>\n",
       "      <td>0.00115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00265</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.16361</td>\n",
       "      <td>0.82034</td>\n",
       "      <td>0.01585</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.41936</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08429</td>\n",
       "      <td>0.66994</td>\n",
       "      <td>11.64583</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.77354</td>\n",
       "      <td>11.05715</td>\n",
       "      <td>0.45202</td>\n",
       "      <td>0.37436</td>\n",
       "      <td>6.67062</td>\n",
       "      <td>0.02738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>0.08075</td>\n",
       "      <td>0.00357</td>\n",
       "      <td>0.00473</td>\n",
       "      <td>0.06956</td>\n",
       "      <td>0.65149</td>\n",
       "      <td>0.00271</td>\n",
       "      <td>0.29342</td>\n",
       "      <td>0.09004</td>\n",
       "      <td>0.00225</td>\n",
       "      <td>0.11459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00283</td>\n",
       "      <td>0.26985</td>\n",
       "      <td>0.00998</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.06783</td>\n",
       "      <td>2.85321</td>\n",
       "      <td>2.13367</td>\n",
       "      <td>10.29113</td>\n",
       "      <td>6.10536</td>\n",
       "      <td>0.18934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>0.04612</td>\n",
       "      <td>0.06370</td>\n",
       "      <td>0.15231</td>\n",
       "      <td>0.32347</td>\n",
       "      <td>1.88545</td>\n",
       "      <td>0.06768</td>\n",
       "      <td>0.22761</td>\n",
       "      <td>0.00164</td>\n",
       "      <td>0.10544</td>\n",
       "      <td>0.09729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02299</td>\n",
       "      <td>0.31096</td>\n",
       "      <td>16.53661</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.22112</td>\n",
       "      <td>0.99210</td>\n",
       "      <td>0.00988</td>\n",
       "      <td>0.05988</td>\n",
       "      <td>6.43034</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>0.00300</td>\n",
       "      <td>0.40141</td>\n",
       "      <td>0.01326</td>\n",
       "      <td>0.05201</td>\n",
       "      <td>1.23417</td>\n",
       "      <td>0.16464</td>\n",
       "      <td>0.31522</td>\n",
       "      <td>0.08965</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.36392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14835</td>\n",
       "      <td>0.41270</td>\n",
       "      <td>12.00993</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.66801</td>\n",
       "      <td>0.72923</td>\n",
       "      <td>0.24871</td>\n",
       "      <td>0.36403</td>\n",
       "      <td>21.78377</td>\n",
       "      <td>0.05601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         61       72       73        74       75       76       77       95   \\\n",
       "850  0.08763  0.00227  0.00337   0.00912  0.92729  0.07266  0.30569  0.02557   \n",
       "814  0.11185  0.00464  0.00000   0.17030  1.00166  0.00000  0.00000  0.33259   \n",
       "720  0.01344  0.00286  0.02261   0.10007  3.92172  0.08675  0.00000  0.01937   \n",
       "737  0.00655  0.00300  0.04929   0.39836  0.80423  0.01613  0.28416  0.02062   \n",
       "843  0.48082  0.09798  0.29394   0.55871  2.58449  0.11017  0.26504  1.04471   \n",
       "864  0.03204  0.04425  0.00000   0.10824  0.86262  0.00000  0.03526  0.05876   \n",
       "877  0.62166  0.00000  0.01723   0.00632  0.93107  0.00000  0.09023  0.00137   \n",
       "743  0.00000  0.10927  0.00122   0.01962  0.07067  0.02285  0.44075  0.04095   \n",
       "785  0.08739  0.00000  0.06191   0.00000  0.36653  0.00192  0.20238  0.19706   \n",
       "828  0.16399  0.79895  0.02749   2.04479  0.00000  0.03040  0.00000  0.03699   \n",
       "756  0.00000  0.00000  0.00000   0.00000  0.00000  0.00000  0.00000  0.06799   \n",
       "783  0.00000  0.00000  0.00000   0.00000  2.57254  0.00000  0.00000  0.10735   \n",
       "793  0.21623  0.07427  0.00000   0.48242  1.78546  0.00000  0.11366  0.06596   \n",
       "750  0.10642  0.03069  0.00520   0.12162  1.07610  0.02478  2.11103  0.13262   \n",
       "856  0.03959  0.00000  0.67538   0.34223  0.00000  0.00000  0.24728  0.10485   \n",
       "789  0.00000  0.00000  0.00000   0.00000  0.03388  0.02393  0.00055  0.00000   \n",
       "792  0.00000  0.00317  0.00000   0.08281  1.23321  0.01195  0.10511  0.20621   \n",
       "725  0.00000  0.00556  0.00451   0.03041  1.42271  0.07010  0.41342  0.12524   \n",
       "729  0.00000  0.00744  0.04946   0.88820  2.53431  0.07960  0.30497  0.00000   \n",
       "878  0.52340  0.00060  0.01474   0.71126  1.69977  0.04472  0.86150  0.10590   \n",
       "822  0.03925  0.12320  0.04456   0.04057  1.41700  0.00000  0.34554  0.09800   \n",
       "851  0.02677  0.17584  0.00185   0.00995  1.10170  0.00000  0.31427  0.02299   \n",
       "710  0.00000  0.74899  0.01454   0.01301  7.02022  0.03341  0.25387  0.01831   \n",
       "810  0.08325  0.00000  0.00000   0.09326  1.61794  0.02219  1.35493  0.02594   \n",
       "744  0.38714  0.31513  0.00000   0.91256  3.36346  0.00000  0.14312  0.02889   \n",
       "811  0.00000  0.00000  0.00000   0.00000  0.15802  0.00000  0.00000  0.00000   \n",
       "853  0.00092  0.17161  0.04507   0.04004  1.93385  0.33382  1.41513  0.02264   \n",
       "798  0.00000  0.00000  0.04132   0.23878  0.36092  0.00490  0.00000  0.21457   \n",
       "837  0.03936  0.46514  0.04757   3.56184  4.46240  0.01132  0.58186  0.07696   \n",
       "751  0.15101  0.03238  0.00000   0.00000  0.00000  0.16287  0.11999  0.81041   \n",
       "..       ...      ...      ...       ...      ...      ...      ...      ...   \n",
       "863  0.11893  0.21598  0.00979   0.23204  0.94470  0.00041  0.08021  0.09491   \n",
       "741  0.07215  0.10029  0.08307   0.18538  1.64980  0.02368  0.71693  0.05739   \n",
       "875  0.00000  0.07089  0.00639   0.11392  3.05638  0.06994  0.18336  0.75960   \n",
       "775  0.00703  0.19454  0.06780   0.02968  1.22271  0.34368  0.15662  0.66788   \n",
       "758  0.16223  0.01664  0.00000   0.10088  0.44595  0.00000  0.00000  0.17010   \n",
       "845  0.19884  0.05867  0.00000   1.01760  2.28953  0.00133  0.27898  0.04729   \n",
       "739  0.03281  0.00000  0.00000   0.01095  0.00000  0.00000  0.03420  0.02132   \n",
       "826  0.01445  0.02444  0.21956   0.64741  3.02082  0.08567  2.04000  0.09282   \n",
       "765  0.10041  0.00000  0.02829   0.02124  1.36358  0.00000  0.11230  0.00971   \n",
       "858  0.31539  0.24011  0.20641  11.84443  1.20010  0.05259  0.84423  0.01607   \n",
       "730  0.02400  0.07328  0.00000   0.00000  5.18027  0.00088  1.26931  0.27152   \n",
       "709  0.18375  0.41022  0.23257   2.33253  2.74029  0.16498  2.49418  0.01408   \n",
       "840  0.41226  0.00000  0.16198   2.57775  2.49586  0.00000  0.00000  0.19276   \n",
       "761  0.06772  0.03927  0.02860   0.01518  2.09982  0.04961  0.16558  0.04411   \n",
       "835  0.47196  0.00595  0.00000   0.46991  2.19166  0.00071  0.32279  0.05733   \n",
       "747  0.03488  0.00000  0.01212   0.19092  1.32727  0.00141  0.43158  0.41976   \n",
       "753  0.00000  0.00000  0.00000   0.00000  0.00000  0.00000  0.00000  0.04595   \n",
       "876  0.80258  0.02299  0.00000   1.17244  0.99433  0.00000  0.15240  0.01688   \n",
       "766  0.46958  0.65207  0.21086   0.86716  3.08996  0.33079  0.86090  0.26517   \n",
       "764  0.03653  0.00244  0.00000   0.39785  1.30939  0.00000  0.00000  0.03210   \n",
       "803  0.02664  0.39996  0.00000   0.58753  0.59089  0.00000  0.00000  0.01665   \n",
       "818  0.00000  0.15636  0.00000   2.38368  2.69387  0.00000  0.00000  0.01724   \n",
       "812  0.30213  0.00345  0.26163   0.43681  0.09306  0.02819  0.81927  2.36517   \n",
       "767  0.44670  0.01529  0.00000   0.14309  0.04073  0.00063  0.01549  0.03583   \n",
       "846  0.00000  0.11001  0.06098   0.11831  0.38983  0.06181  0.47201  0.43311   \n",
       "759  0.07047  0.05192  0.07896   0.17521  1.31529  0.02014  1.15965  0.46545   \n",
       "796  0.00000  0.00265  0.00000   0.16361  0.82034  0.01585  0.00000  0.41936   \n",
       "813  0.08075  0.00357  0.00473   0.06956  0.65149  0.00271  0.29342  0.09004   \n",
       "838  0.04612  0.06370  0.15231   0.32347  1.88545  0.06768  0.22761  0.00164   \n",
       "831  0.00300  0.40141  0.01326   0.05201  1.23417  0.16464  0.31522  0.08965   \n",
       "\n",
       "         98       111   ...         610      611       619      624      625  \\\n",
       "850  0.03173  0.26319   ...     0.83851  0.16932   0.27568  0.65651  0.42383   \n",
       "814  0.00000  0.35661   ...     0.00000  0.48900   8.13686  0.00000  1.28955   \n",
       "720  0.00000  1.36165   ...     2.02262  1.06370   5.25088  0.55415  0.67356   \n",
       "737  0.00000  0.28807   ...     4.94547  1.50510  14.74455  0.00000  1.39289   \n",
       "843  0.17303  0.14684   ...     0.16139  0.24277   2.22713  1.98828  1.50117   \n",
       "864  0.00000  0.18182   ...     0.12252  0.00038   3.94376  0.02213  0.48827   \n",
       "877  0.00000  0.08427   ...     6.69592  1.02801  10.82138  0.40760  2.40580   \n",
       "743  0.01397  0.00000   ...     0.28673  0.26287  10.17593  0.60657  0.98747   \n",
       "785  0.00000  0.17877   ...     0.01116  4.06874   9.49282  1.23557  0.39896   \n",
       "828  0.01799  0.70545   ...     0.60606  0.46186   6.79872  0.00000  1.62241   \n",
       "756  0.03393  0.00000   ...     0.28796  0.52041  17.19084  0.00000  0.99326   \n",
       "783  0.04081  0.00000   ...     0.00103  0.06429   8.20110  0.66091  0.46247   \n",
       "793  0.04611  0.00000   ...     0.06108  0.47826   7.84393  0.00000  3.73410   \n",
       "750  0.07617  0.03921   ...     0.00000  0.14875   0.00000  0.00000  3.36625   \n",
       "856  0.00000  0.92156   ...     0.00000  0.08811   1.27991  1.04462  0.77945   \n",
       "789  0.00000  0.01908   ...     0.07239  0.18708   0.03324  0.00000  0.57961   \n",
       "792  0.03924  0.04807   ...     0.49849  1.60531  18.60599  0.21709  0.76444   \n",
       "725  0.02375  0.45324   ...     0.55632  1.49514   9.96385  0.00287  2.09317   \n",
       "729  0.00000  0.19612   ...     0.02549  0.04222  19.33150  0.00909  0.31635   \n",
       "878  0.13163  0.57698   ...     1.75720  0.16714   0.14412  0.14576  2.96856   \n",
       "822  0.00000  0.16040   ...     0.34371  0.16806   9.83182  0.00000  1.22405   \n",
       "851  0.02245  0.09402   ...     0.61360  0.24445   3.58854  0.24322  1.04123   \n",
       "710  0.00000  0.18154   ...     0.00000  4.29459   0.12818  2.68373  1.58893   \n",
       "810  0.13850  0.06231   ...     0.01847  0.12864   1.05454  0.00000  0.21822   \n",
       "744  0.00447  0.00000   ...     1.00966  1.25082   7.67077  1.59254  0.21616   \n",
       "811  0.00000  0.00000   ...     0.45629  0.18813   0.05725  0.00000  0.59429   \n",
       "853  0.10252  0.00000   ...     0.02247  0.32285  15.48562  2.73094  0.39106   \n",
       "798  0.01038  0.00760   ...     0.00000  0.00000  15.06748  0.00000  2.27323   \n",
       "837  0.00000  0.53356   ...     0.47669  0.27833   8.10831  0.25307  0.58186   \n",
       "751  0.07492  0.31291   ...     1.65237  2.94848   0.00000  5.16052  0.93960   \n",
       "..       ...      ...   ...         ...      ...       ...      ...      ...   \n",
       "863  0.03694  0.09817   ...     0.03133  3.16531   6.05944  4.30551  1.82837   \n",
       "741  0.05869  0.86721   ...     0.00000  0.00662  15.13028  0.28657  3.38619   \n",
       "875  0.00000  0.00000   ...     1.89669  0.00000   4.86071  0.44691  1.19796   \n",
       "775  0.03938  0.03713   ...     0.00000  0.31612   3.10802  1.27069  2.08935   \n",
       "758  0.00000  0.00000   ...     8.01790  1.28874   7.93466  0.02735  1.09487   \n",
       "845  0.01899  0.18190   ...     1.44875  1.11747   7.10904  0.77241  0.93631   \n",
       "739  0.04179  0.01564   ...     0.15863  0.48677  10.82479  0.00214  0.60281   \n",
       "826  0.00193  0.20372   ...     0.26387  0.61143   0.00000  0.00000  1.80980   \n",
       "765  0.13150  0.09578   ...     1.19045  0.04916   5.67935  0.42865  1.19442   \n",
       "858  0.02909  0.08182   ...     0.13834  0.92708   0.00000  0.00000  1.36068   \n",
       "730  0.07979  0.36282   ...     0.17504  0.00607   2.59344  0.00274  0.41229   \n",
       "709  0.02382  0.07328   ...     0.08249  0.00331   0.08742  0.02905  1.63606   \n",
       "840  0.21512  0.34521   ...     0.39470  0.08788  25.12031  0.00000  1.73046   \n",
       "761  0.02699  0.06013   ...     0.16921  0.02282   0.01627  0.00000  1.52314   \n",
       "835  0.00000  1.13343   ...     3.21554  0.38801   0.01076  0.14470  1.33992   \n",
       "747  0.00000  0.15057   ...     0.01016  0.13042   9.12609  0.00000  1.19889   \n",
       "753  0.00000  0.00000   ...     7.02557  4.17462   0.12246  0.00000  0.51764   \n",
       "876  0.00283  0.00000   ...     2.07277  0.40821  11.77548  1.55913  2.48063   \n",
       "766  0.01375  0.02154   ...     0.00914  0.50068   0.00000  0.00000  1.05919   \n",
       "764  0.00000  0.00000   ...     0.02293  0.01404   9.31312  0.00000  2.57852   \n",
       "803  0.00000  0.00000   ...     5.45965  0.67608   9.27991  0.04502  0.56325   \n",
       "818  0.15919  0.36902   ...     0.01783  0.07011   7.23383  0.00000  0.63671   \n",
       "812  0.21785  0.27346   ...     0.01652  0.00000  22.17197  0.00000  0.68206   \n",
       "767  0.06983  0.00000   ...     0.00226  0.02863   5.96757  0.00000  0.58703   \n",
       "846  0.07540  0.01925   ...     0.04030  0.00000  18.36034  0.15540  0.49918   \n",
       "759  0.13387  0.27208   ...     0.07419  0.02115   3.01032  4.34735  1.22595   \n",
       "796  0.00000  0.00000   ...     0.08429  0.66994  11.64583  0.00000  0.77354   \n",
       "813  0.00225  0.11459   ...     0.00283  0.26985   0.00998  0.00000  2.06783   \n",
       "838  0.10544  0.09729   ...     0.02299  0.31096  16.53661  0.00000  2.22112   \n",
       "831  0.00000  0.36392   ...     0.14835  0.41270  12.00993  0.00000  1.66801   \n",
       "\n",
       "          627      685       690       702      726  \n",
       "850   2.21426  0.49426   0.75520  13.60813  0.02305  \n",
       "814   5.02893  0.06792   0.16304   9.56091  0.00000  \n",
       "720   1.45788  0.38973   0.30366  16.02550  0.06081  \n",
       "737   6.21633  0.00328   0.12309   8.45865  0.03540  \n",
       "843   7.18742  0.10728   0.16467   8.81397  0.02787  \n",
       "864   0.00000  0.00654   0.15705  18.24170  0.02871  \n",
       "877   1.55032  0.10949   0.17668   3.61774  0.34012  \n",
       "743   2.51809  0.00495   0.08544  18.55156  0.00000  \n",
       "785   0.60494  0.22398   0.59859   4.55491  0.39903  \n",
       "828   1.74928  0.00100   0.02290   8.94006  0.00000  \n",
       "756   3.54834  0.97580   1.59456   1.86928  0.20449  \n",
       "783   3.41952  0.01496   0.11130   2.60901  0.00958  \n",
       "793   3.10690  0.16846   1.76587   1.55576  0.00000  \n",
       "750   4.50622  0.00000   0.06290  20.08717  0.00000  \n",
       "856   0.00000  0.12353   0.07120  27.61270  0.05802  \n",
       "789   1.90755  1.48208   0.28730   1.61595  0.09017  \n",
       "792   0.63998  0.01888   0.03818   7.99493  0.36140  \n",
       "725   0.00000  0.07726   0.63980  12.13454  0.08526  \n",
       "729   0.00000  0.06417   0.21777  12.71725  0.00000  \n",
       "878   9.51526  0.08812   0.05155  11.66694  0.00000  \n",
       "822   1.19074  0.24004   0.38612  19.44191  0.00000  \n",
       "851   2.94950  0.47468   0.29521  13.68700  0.10742  \n",
       "710   2.52668  1.39579   2.93867   1.56245  0.01821  \n",
       "810   0.00000  0.00000   0.00173   3.03937  0.02287  \n",
       "744   3.99135  0.13552   0.21298   3.02060  0.01936  \n",
       "811  15.91178  1.05093   1.29294   2.84454  0.00000  \n",
       "853   0.00000  0.02408   0.11431  11.65351  0.00510  \n",
       "798  13.47859  0.10374   0.05318   1.28525  0.00228  \n",
       "837   2.36701  0.00762   0.01089  11.69976  0.01633  \n",
       "751   2.67069  0.24669   0.14594  15.74064  0.00000  \n",
       "..        ...      ...       ...       ...      ...  \n",
       "863   3.29844  0.11575   0.20157  22.97987  0.02964  \n",
       "741   1.52202  0.29834   3.50237  13.62461  0.00000  \n",
       "875   0.00000  0.20189   0.09815   8.58969  0.02892  \n",
       "775   1.79695  0.02809   0.15730  17.67187  0.06342  \n",
       "758   2.88139  0.43301   1.97004   4.91888  0.03141  \n",
       "845   1.75188  0.00000   0.00000   8.10803  0.00140  \n",
       "739   8.56986  0.09585   1.16608   1.05830  0.00000  \n",
       "826   4.02034  0.05768   0.04275   7.76304  0.01289  \n",
       "765   2.34436  0.04061   0.12346   3.39289  0.00000  \n",
       "858   8.99791  0.00957   0.00346   4.43547  0.00625  \n",
       "730   0.89011  1.65572   4.76506  41.94086  0.00320  \n",
       "709   7.71727  0.06754   0.04890   2.76216  0.00000  \n",
       "840   3.50272  0.05414   0.05736  17.13915  0.00000  \n",
       "761   2.94293  0.00601   0.00000  34.90021  0.00000  \n",
       "835   1.78839  0.00066   0.02777  10.35255  0.02103  \n",
       "747   8.14107  0.10427   0.33440  17.03688  0.40858  \n",
       "753   0.00000  0.14733   0.21369   0.23934  0.02878  \n",
       "876   9.59993  0.16984   1.16010   5.36634  0.05464  \n",
       "766   6.12105  0.02726   0.03208   4.03417  0.00342  \n",
       "764   0.86095  0.07704   0.10298   8.54457  0.00419  \n",
       "803   2.61889  0.02901   0.38185   0.56248  0.00963  \n",
       "818   1.91881  2.03660   2.94334  15.12912  0.07795  \n",
       "812   0.00000  0.01335   0.06099  13.66552  0.00000  \n",
       "767   2.20485  9.19442   7.26994   3.08273  0.00428  \n",
       "846   0.00000  0.00000   0.00000  27.88353  0.00000  \n",
       "759   1.35483  0.08121   0.18296  28.73081  0.00115  \n",
       "796  11.05715  0.45202   0.37436   6.67062  0.02738  \n",
       "813   2.85321  2.13367  10.29113   6.10536  0.18934  \n",
       "838   0.99210  0.00988   0.05988   6.43034  0.00000  \n",
       "831   0.72923  0.24871   0.36403  21.78377  0.05601  \n",
       "\n",
       "[136 rows x 51 columns]"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyplot.bar(range(len(XGB_model.feature_importances_)), XGB_model.feature_importances_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

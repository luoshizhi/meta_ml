{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meta machine_learning baseline:\n",
    "\n",
    "1.针对回归\n",
    "\n",
    "2.包含逻辑回归，决策树，随机森林，xgboost模型\n",
    "\n",
    "3.可根据需求自动调整超参数\n",
    "\n",
    "待改进：\n",
    "1.加入特征工程\n",
    "3.加入回归分析\n",
    "4.加入神经网络方法(tensorflow框架)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以bmi预测为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shizhiluo/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.utils import column_or_1d\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StratifiedKFold in module sklearn.model_selection._split:\n",
      "\n",
      "class StratifiedKFold(_BaseKFold)\n",
      " |  Stratified K-Folds cross-validator\n",
      " |  \n",
      " |  Provides train/test indices to split data in train/test sets.\n",
      " |  \n",
      " |  This cross-validation object is a variation of KFold that returns\n",
      " |  stratified folds. The folds are made by preserving the percentage of\n",
      " |  samples for each class.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=3\n",
      " |      Number of folds. Must be at least 2.\n",
      " |  \n",
      " |  shuffle : boolean, optional\n",
      " |      Whether to shuffle each stratification of the data before splitting\n",
      " |      into batches.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default=None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`. Used when ``shuffle`` == True.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.model_selection import StratifiedKFold\n",
      " |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      " |  >>> y = np.array([0, 0, 1, 1])\n",
      " |  >>> skf = StratifiedKFold(n_splits=2)\n",
      " |  >>> skf.get_n_splits(X, y)\n",
      " |  2\n",
      " |  >>> print(skf)  # doctest: +NORMALIZE_WHITESPACE\n",
      " |  StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
      " |  >>> for train_index, test_index in skf.split(X, y):\n",
      " |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      " |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      " |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      " |  TRAIN: [1 3] TEST: [0 2]\n",
      " |  TRAIN: [0 2] TEST: [1 3]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  All the folds have size ``trunc(n_samples / n_splits)``, the last one has\n",
      " |  the complementary.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StratifiedKFold\n",
      " |      _BaseKFold\n",
      " |      abc.NewBase\n",
      " |      BaseCrossValidator\n",
      " |      abc.NewBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_splits=3, shuffle=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  split(self, X, y, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |          Note that providing ``y`` is sufficient to generate the splits and\n",
      " |          hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
      " |          ``X`` instead of actual training data.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target variable for supervised learning problems.\n",
      " |          Stratification is done based on the y labels.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |      \n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Randomized CV splitters may return different results for each call of\n",
      " |      split. You can make the results identical by setting ``random_state``\n",
      " |      to an integer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseKFold:\n",
      " |  \n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from abc.NewBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(StratifiedKFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _BaseKFold.split at 0x1137c2678>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "skf=StratifiedKFold(n_splits=2,shuffle=True, random_state=1)\n",
    "skf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalu(y_test, y_pred):\n",
    "    print(\"explained_variance_score:\", explained_variance_score(y_test,y_pred),\"越接近1越好\")\n",
    "    print(\"mean_absolute_error:\", mean_absolute_error(y_test,y_pred))\n",
    "    print(\"mean_squared_error:\", mean_squared_error(y_test,y_pred))\n",
    "    print(\"median_absolute_error:\", median_absolute_error(y_test,y_pred))\n",
    "    print(\"r2_score:\", r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_table(\"../data/merge.metaphlan_tables.tree.merge.metadata.new.noLD16_2\",index_col=0,header='infer')\n",
    "data = pd.read_table(\"../data/merge.metaphlan_tables.tree.merge.metadata.new.noLD16_2\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>812</th>\n",
       "      <th>813</th>\n",
       "      <th>814</th>\n",
       "      <th>815</th>\n",
       "      <th>816</th>\n",
       "      <th>817</th>\n",
       "      <th>818</th>\n",
       "      <th>819</th>\n",
       "      <th>820</th>\n",
       "      <th>821</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>project</td>\n",
       "      <td>age</td>\n",
       "      <td>bmi</td>\n",
       "      <td>gender</td>\n",
       "      <td>whr</td>\n",
       "      <td>dis_CRC</td>\n",
       "      <td>dis_HBV</td>\n",
       "      <td>dis_T2D</td>\n",
       "      <td>Cholesterol</td>\n",
       "      <td>TG</td>\n",
       "      <td>...</td>\n",
       "      <td>k__Viruses|p__Viruses_noname|c__Viruses_noname...</td>\n",
       "      <td>k__Viruses|p__Viruses_noname|c__Viruses_noname...</td>\n",
       "      <td>k__Viruses|p__Viruses_noname|c__Viruses_noname...</td>\n",
       "      <td>k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactob...</td>\n",
       "      <td>k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactob...</td>\n",
       "      <td>k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactob...</td>\n",
       "      <td>k__Bacteria|p__Proteobacteria|c__Gammaproteoba...</td>\n",
       "      <td>k__Bacteria|p__Proteobacteria|c__Gammaproteoba...</td>\n",
       "      <td>k__Bacteria|p__Candidatus_Saccharibacteria|c__...</td>\n",
       "      <td>k__Bacteria|p__Candidatus_Saccharibacteria|c__...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>austria.crc.advanced_adenoma</td>\n",
       "      <td>78</td>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "      <td>0.87</td>\n",
       "      <td>advanced_adenoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>austria.crc.advanced_adenoma</td>\n",
       "      <td>48</td>\n",
       "      <td>25.61</td>\n",
       "      <td>male</td>\n",
       "      <td>0.87</td>\n",
       "      <td>advanced_adenoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>austria.crc.advanced_adenoma</td>\n",
       "      <td>67</td>\n",
       "      <td>27.14</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>advanced_adenoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>137</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>austria.crc.advanced_adenoma</td>\n",
       "      <td>61</td>\n",
       "      <td>22.8</td>\n",
       "      <td>female</td>\n",
       "      <td>0.88</td>\n",
       "      <td>advanced_adenoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.08264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 822 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0    1      2       3     4                 5    \\\n",
       "0                       project  age    bmi  gender   whr           dis_CRC   \n",
       "1  austria.crc.advanced_adenoma   78     24  female  0.87  advanced_adenoma   \n",
       "2  austria.crc.advanced_adenoma   48  25.61    male  0.87  advanced_adenoma   \n",
       "3  austria.crc.advanced_adenoma   67  27.14    male   NaN  advanced_adenoma   \n",
       "4  austria.crc.advanced_adenoma   61   22.8  female  0.88  advanced_adenoma   \n",
       "\n",
       "       6        7            8    9    \\\n",
       "0  dis_HBV  dis_T2D  Cholesterol   TG   \n",
       "1      NaN      NaN          NaN  144   \n",
       "2      NaN      NaN          NaN   75   \n",
       "3      NaN      NaN          NaN  137   \n",
       "4      NaN      NaN          NaN   82   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "2                        ...                           \n",
       "3                        ...                           \n",
       "4                        ...                           \n",
       "\n",
       "                                                 812  \\\n",
       "0  k__Viruses|p__Viruses_noname|c__Viruses_noname...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "                                                 813  \\\n",
       "0  k__Viruses|p__Viruses_noname|c__Viruses_noname...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "                                                 814  \\\n",
       "0  k__Viruses|p__Viruses_noname|c__Viruses_noname...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "                                                 815  \\\n",
       "0  k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactob...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "                                                 816  \\\n",
       "0  k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactob...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "                                                 817  \\\n",
       "0  k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactob...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                            0.08264   \n",
       "\n",
       "                                                 818  \\\n",
       "0  k__Bacteria|p__Proteobacteria|c__Gammaproteoba...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "                                                 819  \\\n",
       "0  k__Bacteria|p__Proteobacteria|c__Gammaproteoba...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "                                                 820  \\\n",
       "0  k__Bacteria|p__Candidatus_Saccharibacteria|c__...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "                                                 821  \n",
       "0  k__Bacteria|p__Candidatus_Saccharibacteria|c__...  \n",
       "1                                                  0  \n",
       "2                                                  0  \n",
       "3                                                  0  \n",
       "4                                            0.00051  \n",
       "\n",
       "[5 rows x 822 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_title_index=8\n",
    "y_title = data.loc[0,y_title_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df=data.loc[1:,31:]\n",
    "y_df=data.loc[1:,y_title_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_x_y(x_df, y_df):\n",
    "    y_data=y_df[y_df.isnull().values==False]\n",
    "    x_tmp=x_df.loc[y_df.isnull().values==False,:]\n",
    "    x_tmp=pd.DataFrame(x_tmp,dtype=np.float)\n",
    "    x_data=x_tmp.loc[:,(x_tmp==0).sum(axis=0)/x_tmp.shape[0]<0.9]\n",
    "    if len(y_data.unique()) * 20 > x_data.shape[0]:\n",
    "        lable_type = \"regress\"\n",
    "    else:\n",
    "        lable_type = \"classify\"\n",
    "    if lable_type == \"regress\":\n",
    "        y_data = pd.DataFrame(y_data,dtype=np.float)\n",
    "    return x_data, y_data, lable_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data, lable_type= filter_x_y(x_df, y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119d9c2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#pyplot.figure(figsize=(15.0, 10.0))\n",
    "if lable_type == \"classify\":   \n",
    "    sns.countplot(y_data)\n",
    "    pyplot.xlabel(y_title+\" sample\");\n",
    "    pyplot.savefig(\"./temp.png\")\n",
    "else:\n",
    "    sns.distplot(y_data, hist=True, kde=True)\n",
    "    pyplot.xlabel(y_title+\" sample\");\n",
    "    pyplot.savefig(\"./temp.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>7.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>7.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>4.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>6.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>4.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>5.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>6.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>5.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>4.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>4.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>5.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>6.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>4.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>5.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>5.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>6.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>5.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>4.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>5.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>3.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>6.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>4.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>6.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>3.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>5.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>5.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>5.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>5.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>5.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>7.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>5.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>5.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>4.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>6.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>6.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>6.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>5.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>5.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>6.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>4.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>4.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>6.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>4.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>4.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>8.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>4.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>6.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>6.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        8\n",
       "807  7.01\n",
       "805  7.05\n",
       "858  4.40\n",
       "853  6.67\n",
       "724  4.13\n",
       "786  5.43\n",
       "850  6.35\n",
       "720  5.84\n",
       "730  4.72\n",
       "750  4.70\n",
       "810  5.42\n",
       "842  6.10\n",
       "868  7.00\n",
       "809  4.91\n",
       "843  5.91\n",
       "813  3.50\n",
       "857  5.41\n",
       "847  6.23\n",
       "711  5.94\n",
       "804  4.71\n",
       "817  5.83\n",
       "849  3.53\n",
       "859  6.51\n",
       "778  6.48\n",
       "791  6.30\n",
       "855  5.25\n",
       "827  4.38\n",
       "772  6.14\n",
       "784  3.56\n",
       "819  5.49\n",
       "828  5.26\n",
       "746  5.29\n",
       "821  4.39\n",
       "873  5.37\n",
       "837  5.49\n",
       "759  5.39\n",
       "830  5.88\n",
       "721  7.94\n",
       "728  5.48\n",
       "818  5.05\n",
       "755  4.38\n",
       "776  6.65\n",
       "874  6.04\n",
       "797  6.43\n",
       "740  5.41\n",
       "872  5.56\n",
       "840  6.01\n",
       "822  4.80\n",
       "749  4.41\n",
       "863  6.24\n",
       "719  3.71\n",
       "756  4.12\n",
       "795  4.51\n",
       "794  8.71\n",
       "799  4.32\n",
       "861  6.10\n",
       "771  6.65"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_data,y_data,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=column_or_1d(y_train, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.91, 5.76, 6.51, 4.6 , 7.46, 4.32, 6.88, 4.38, 4.92, 4.25, 5.07,\n",
       "       4.61, 4.4 , 5.23, 5.26, 5.73, 5.42, 3.49, 4.38, 5.18, 5.82, 6.74,\n",
       "       4.52, 7.  , 5.35, 5.39, 5.66, 4.12, 6.42, 6.43, 5.29, 5.31, 6.24,\n",
       "       5.4 , 3.87, 5.25, 6.48, 5.65, 6.6 , 4.28, 5.05, 3.78, 6.55, 5.51,\n",
       "       6.04, 5.37, 4.97, 5.71, 5.55, 3.34, 5.39, 3.42, 4.81, 5.74, 7.44,\n",
       "       6.01, 5.41, 8.71, 5.72, 3.53, 5.49, 5.61, 4.8 , 6.58, 6.35, 4.17,\n",
       "       4.72, 5.2 , 3.72, 2.96, 5.59, 3.56, 5.91, 5.24, 6.67, 5.51, 6.23,\n",
       "       7.05, 6.1 , 4.04, 5.83, 6.65, 4.48, 6.72, 5.1 , 4.7 , 4.71, 7.62,\n",
       "       5.15, 4.41, 6.39, 4.79, 4.19, 5.73, 7.1 , 4.5 , 5.85, 4.94, 3.24,\n",
       "       6.1 , 5.43, 6.83, 6.99, 5.48, 4.73, 5.49, 4.51, 5.38, 7.01, 5.94,\n",
       "       5.84, 3.5 , 5.56, 4.51, 3.71, 6.45, 5.25, 4.7 , 5.44, 5.39, 5.41,\n",
       "       6.3 , 5.34, 6.65, 7.01, 5.88, 4.15, 5.14, 5.73, 4.47, 5.88, 6.23,\n",
       "       4.13, 6.09, 4.4 , 4.39])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelencoder=LabelEncoder()\n",
    "# labelencoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: -0.7058894502677024 越接近1越好\n",
      "mean_absolute_error: 1.1489473684210525\n",
      "mean_squared_error: 2.0995771929824563\n",
      "median_absolute_error: 0.7999999999999994\n",
      "r2_score: -0.7204184624665735\n"
     ]
    }
   ],
   "source": [
    "y_pred = tree_model.predict(X_test)\n",
    "tree_model.score(X_test, y_pred)\n",
    "evalu(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.51, 6.99, 4.5 , 5.88, 4.81, 5.41, 8.83, 6.45, 6.4 , 6.03, 4.4 ,\n",
       "       3.82, 6.09, 6.83, 5.49, 5.2 , 5.35, 5.85, 5.31, 6.74, 5.39, 5.71,\n",
       "       3.78, 4.19, 7.62, 6.39, 5.14, 5.59, 2.96, 6.58, 6.72, 5.07, 7.44,\n",
       "       5.66, 6.03, 4.52, 3.95, 3.42, 5.74, 5.65, 5.76, 4.11, 5.44, 5.23,\n",
       "       4.73, 4.28, 4.83, 6.55, 5.73, 6.43, 4.92, 3.75, 6.6 , 5.18, 5.38,\n",
       "       5.75, 7.1 , 6.15, 4.94, 3.71, 5.72, 6.28, 5.73, 7.46, 4.15, 4.25,\n",
       "       4.47, 5.55, 5.82, 6.1 , 3.72, 5.48, 4.6 , 3.34, 3.92, 5.73, 4.48,\n",
       "       4.9 , 5.1 , 5.34, 6.23, 6.08, 4.13, 7.18, 4.04, 5.51, 5.61, 4.7 ,\n",
       "       3.87, 5.8 , 3.49, 6.42, 5.44, 4.51, 4.17, 4.81, 6.47, 5.02, 5.31,\n",
       "       5.25, 4.79, 5.4 , 5.15, 5.24, 5.39, 5.84, 5.34, 3.87, 4.97, 6.88,\n",
       "       7.01, 4.61, 3.24])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: 1.0 越接近1越好\n",
      "mean_absolute_error: 0.0\n",
      "mean_squared_error: 0.0\n",
      "median_absolute_error: 0.0\n",
      "r2_score: 1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = tree_model.predict(X_train)\n",
    "evalu(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model2 = DecisionTreeRegressor()\n",
    "tuned_parameters= { 'max_features': [\"auto\",\"sqrt\",\"log2\"],\n",
    "                  'min_samples_leaf': range(1,10,1) , 'max_depth': range(1,10,1)\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "TM = GridSearchCV(tree_model2, tuned_parameters,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_features': ['auto', 'sqrt', 'log2'], 'min_samples_leaf': range(1, 10), 'max_depth': range(1, 10)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 1, 'max_features': 'sqrt', 'min_samples_leaf': 6}\n"
     ]
    }
   ],
   "source": [
    "print(TM.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: -0.5397139891315716 越接近1越好\n",
      "mean_absolute_error: 1.122280701754386\n",
      "mean_squared_error: 1.8491631578947367\n",
      "median_absolute_error: 1.1100000000000003\n",
      "r2_score: -0.6174256405070913\n"
     ]
    }
   ],
   "source": [
    "y_pred = tree_model.predict(X_test)\n",
    "tree_model.score(X_test, y_pred)\n",
    "evalu(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: 1.0 越接近1越好\n",
      "mean_absolute_error: 0.0\n",
      "mean_squared_error: 0.0\n",
      "median_absolute_error: 0.0\n",
      "r2_score: 1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = tree_model.predict(X_train)\n",
    "evalu(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_model=RandomForestRegressor(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fit in module sklearn.ensemble.forest:\n",
      "\n",
      "fit(self, X, y, sample_weight=None)\n",
      "    Build a forest of trees from the training set (X, y).\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "        The training input samples. Internally, its dtype will be converted to\n",
      "        ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "        converted into a sparse ``csc_matrix``.\n",
      "    \n",
      "    y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      "        The target values (class labels in classification, real numbers in\n",
      "        regression).\n",
      "    \n",
      "    sample_weight : array-like, shape = [n_samples] or None\n",
      "        Sample weights. If None, then samples are equally weighted. Splits\n",
      "        that would create child nodes with net zero or negative weight are\n",
      "        ignored while searching for a split in each node. In the case of\n",
      "        classification, splits are also ignored if they would result in any\n",
      "        single class carrying a negative weight in either child node.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    self : object\n",
      "        Returns self.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestRegressor.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=True,\n",
       "           warm_start=False)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: -1.6769072097985185 越接近1越好\n",
      "mean_absolute_error: 1.3514705882352942\n",
      "mean_squared_error: 2.8244205882352946\n",
      "median_absolute_error: 1.3000000000000003\n",
      "r2_score: -1.7758895857407824\n"
     ]
    }
   ],
   "source": [
    "y_pred = tree_model.predict(X_test)\n",
    "tree_model.score(X_test, y_pred)\n",
    "evalu(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: 1.0 越接近1越好\n",
      "mean_absolute_error: 0.0\n",
      "mean_squared_error: 0.0\n",
      "median_absolute_error: 0.0\n",
      "r2_score: 1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = tree_model.predict(X_train)\n",
    "evalu(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = {'min_samples_leaf': range(10,100,10), 'n_estimators' : range(10,500,10),\n",
    "                    'max_features':['auto','sqrt','log2']\n",
    "                    }\n",
    "RR = GridSearchCV(rr_model, tuned_parameters,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GridSearchCV in module sklearn.model_selection._search:\n",
      "\n",
      "class GridSearchCV(BaseSearchCV)\n",
      " |  Exhaustive search over specified parameter values for an estimator.\n",
      " |  \n",
      " |  Important members are fit, predict.\n",
      " |  \n",
      " |  GridSearchCV implements a \"fit\" and a \"score\" method.\n",
      " |  It also implements \"predict\", \"predict_proba\", \"decision_function\",\n",
      " |  \"transform\" and \"inverse_transform\" if they are implemented in the\n",
      " |  estimator used.\n",
      " |  \n",
      " |  The parameters of the estimator used to apply these methods are optimized\n",
      " |  by cross-validated grid-search over a parameter grid.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <grid_search>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimator : estimator object.\n",
      " |      This is assumed to implement the scikit-learn estimator interface.\n",
      " |      Either estimator needs to provide a ``score`` function,\n",
      " |      or ``scoring`` must be passed.\n",
      " |  \n",
      " |  param_grid : dict or list of dictionaries\n",
      " |      Dictionary with parameters names (string) as keys and lists of\n",
      " |      parameter settings to try as values, or a list of such\n",
      " |      dictionaries, in which case the grids spanned by each dictionary\n",
      " |      in the list are explored. This enables searching over any sequence\n",
      " |      of parameter settings.\n",
      " |  \n",
      " |  scoring : string, callable, list/tuple, dict or None, default: None\n",
      " |      A single string (see :ref:`scoring_parameter`) or a callable\n",
      " |      (see :ref:`scoring`) to evaluate the predictions on the test set.\n",
      " |  \n",
      " |      For evaluating multiple metrics, either give a list of (unique) strings\n",
      " |      or a dict with names as keys and callables as values.\n",
      " |  \n",
      " |      NOTE that when using custom scorers, each scorer should return a single\n",
      " |      value. Metric functions returning a list/array of values can be wrapped\n",
      " |      into multiple scorers that return one value each.\n",
      " |  \n",
      " |      See :ref:`multimetric_grid_search` for an example.\n",
      " |  \n",
      " |      If None, the estimator's default scorer (if available) is used.\n",
      " |  \n",
      " |  fit_params : dict, optional\n",
      " |      Parameters to pass to the fit method.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``fit_params`` as a constructor argument was deprecated in version\n",
      " |         0.19 and will be removed in version 0.21. Pass fit parameters to\n",
      " |         the ``fit`` method instead.\n",
      " |  \n",
      " |  n_jobs : int, default=1\n",
      " |      Number of jobs to run in parallel.\n",
      " |  \n",
      " |  pre_dispatch : int, or string, optional\n",
      " |      Controls the number of jobs that get dispatched during parallel\n",
      " |      execution. Reducing this number can be useful to avoid an\n",
      " |      explosion of memory consumption when more jobs get dispatched\n",
      " |      than CPUs can process. This parameter can be:\n",
      " |  \n",
      " |          - None, in which case all the jobs are immediately\n",
      " |            created and spawned. Use this for lightweight and\n",
      " |            fast-running jobs, to avoid delays due to on-demand\n",
      " |            spawning of the jobs\n",
      " |  \n",
      " |          - An int, giving the exact number of total jobs that are\n",
      " |            spawned\n",
      " |  \n",
      " |          - A string, giving an expression as a function of n_jobs,\n",
      " |            as in '2*n_jobs'\n",
      " |  \n",
      " |  iid : boolean, default=True\n",
      " |      If True, the data is assumed to be identically distributed across\n",
      " |      the folds, and the loss minimized is the total loss per sample,\n",
      " |      and not the mean loss across the folds.\n",
      " |  \n",
      " |  cv : int, cross-validation generator or an iterable, optional\n",
      " |      Determines the cross-validation splitting strategy.\n",
      " |      Possible inputs for cv are:\n",
      " |        - None, to use the default 3-fold cross validation,\n",
      " |        - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      " |        - An object to be used as a cross-validation generator.\n",
      " |        - An iterable yielding train, test splits.\n",
      " |  \n",
      " |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      " |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      " |      other cases, :class:`KFold` is used.\n",
      " |  \n",
      " |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      " |      cross-validation strategies that can be used here.\n",
      " |  \n",
      " |  refit : boolean, or string, default=True\n",
      " |      Refit an estimator using the best found parameters on the whole\n",
      " |      dataset.\n",
      " |  \n",
      " |      For multiple metric evaluation, this needs to be a string denoting the\n",
      " |      scorer is used to find the best parameters for refitting the estimator\n",
      " |      at the end.\n",
      " |  \n",
      " |      The refitted estimator is made available at the ``best_estimator_``\n",
      " |      attribute and permits using ``predict`` directly on this\n",
      " |      ``GridSearchCV`` instance.\n",
      " |  \n",
      " |      Also for multiple metric evaluation, the attributes ``best_index_``,\n",
      " |      ``best_score_`` and ``best_parameters_`` will only be available if\n",
      " |      ``refit`` is set and all of them will be determined w.r.t this specific\n",
      " |      scorer.\n",
      " |  \n",
      " |      See ``scoring`` parameter to know more about multiple metric\n",
      " |      evaluation.\n",
      " |  \n",
      " |  verbose : integer\n",
      " |      Controls the verbosity: the higher, the more messages.\n",
      " |  \n",
      " |  error_score : 'raise' (default) or numeric\n",
      " |      Value to assign to the score if an error occurs in estimator fitting.\n",
      " |      If set to 'raise', the error is raised. If a numeric value is given,\n",
      " |      FitFailedWarning is raised. This parameter does not affect the refit\n",
      " |      step, which will always raise the error.\n",
      " |  \n",
      " |  return_train_score : boolean, default=True\n",
      " |      If ``'False'``, the ``cv_results_`` attribute will not include training\n",
      " |      scores.\n",
      " |  \n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn import svm, datasets\n",
      " |  >>> from sklearn.model_selection import GridSearchCV\n",
      " |  >>> iris = datasets.load_iris()\n",
      " |  >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      " |  >>> svc = svm.SVC()\n",
      " |  >>> clf = GridSearchCV(svc, parameters)\n",
      " |  >>> clf.fit(iris.data, iris.target)\n",
      " |  ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n",
      " |  GridSearchCV(cv=None, error_score=...,\n",
      " |         estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,\n",
      " |                       decision_function_shape='ovr', degree=..., gamma=...,\n",
      " |                       kernel='rbf', max_iter=-1, probability=False,\n",
      " |                       random_state=None, shrinking=True, tol=...,\n",
      " |                       verbose=False),\n",
      " |         fit_params=None, iid=..., n_jobs=1,\n",
      " |         param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,\n",
      " |         scoring=..., verbose=...)\n",
      " |  >>> sorted(clf.cv_results_.keys())\n",
      " |  ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n",
      " |  ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n",
      " |   'mean_train_score', 'param_C', 'param_kernel', 'params',...\n",
      " |   'rank_test_score', 'split0_test_score',...\n",
      " |   'split0_train_score', 'split1_test_score', 'split1_train_score',...\n",
      " |   'split2_test_score', 'split2_train_score',...\n",
      " |   'std_fit_time', 'std_score_time', 'std_test_score', 'std_train_score'...]\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cv_results_ : dict of numpy (masked) ndarrays\n",
      " |      A dict with keys as column headers and values as columns, that can be\n",
      " |      imported into a pandas ``DataFrame``.\n",
      " |  \n",
      " |      For instance the below given table\n",
      " |  \n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n",
      " |      +============+===========+============+=================+===+=========+\n",
      " |      |  'poly'    |     --    |      2     |        0.8      |...|    2    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'poly'    |     --    |      3     |        0.7      |...|    4    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'rbf'     |     0.1   |     --     |        0.8      |...|    3    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'rbf'     |     0.2   |     --     |        0.9      |...|    1    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |  \n",
      " |      will be represented by a ``cv_results_`` dict of::\n",
      " |  \n",
      " |          {\n",
      " |          'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n",
      " |                                       mask = [False False False False]...)\n",
      " |          'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n",
      " |                                      mask = [ True  True False False]...),\n",
      " |          'param_degree': masked_array(data = [2.0 3.0 -- --],\n",
      " |                                       mask = [False False  True  True]...),\n",
      " |          'split0_test_score'  : [0.8, 0.7, 0.8, 0.9],\n",
      " |          'split1_test_score'  : [0.82, 0.5, 0.7, 0.78],\n",
      " |          'mean_test_score'    : [0.81, 0.60, 0.75, 0.82],\n",
      " |          'std_test_score'     : [0.02, 0.01, 0.03, 0.03],\n",
      " |          'rank_test_score'    : [2, 4, 3, 1],\n",
      " |          'split0_train_score' : [0.8, 0.9, 0.7],\n",
      " |          'split1_train_score' : [0.82, 0.5, 0.7],\n",
      " |          'mean_train_score'   : [0.81, 0.7, 0.7],\n",
      " |          'std_train_score'    : [0.03, 0.03, 0.04],\n",
      " |          'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n",
      " |          'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n",
      " |          'mean_score_time'    : [0.007, 0.06, 0.04, 0.04],\n",
      " |          'std_score_time'     : [0.001, 0.002, 0.003, 0.005],\n",
      " |          'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
      " |          }\n",
      " |  \n",
      " |      NOTE\n",
      " |  \n",
      " |      The key ``'params'`` is used to store a list of parameter\n",
      " |      settings dicts for all the parameter candidates.\n",
      " |  \n",
      " |      The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      " |      ``std_score_time`` are all in seconds.\n",
      " |  \n",
      " |      For multi-metric evaluation, the scores for all the scorers are\n",
      " |      available in the ``cv_results_`` dict at the keys ending with that\n",
      " |      scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
      " |      above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      " |  \n",
      " |  best_estimator_ : estimator or dict\n",
      " |      Estimator that was chosen by the search, i.e. estimator\n",
      " |      which gave highest score (or smallest loss if specified)\n",
      " |      on the left out data. Not available if ``refit=False``.\n",
      " |  \n",
      " |      See ``refit`` parameter for more information on allowed values.\n",
      " |  \n",
      " |  best_score_ : float\n",
      " |      Mean cross-validated score of the best_estimator\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |  best_params_ : dict\n",
      " |      Parameter setting that gave the best results on the hold out data.\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |  best_index_ : int\n",
      " |      The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      " |      candidate parameter setting.\n",
      " |  \n",
      " |      The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      " |      the parameter setting for the best model, that gives the highest\n",
      " |      mean score (``search.best_score_``).\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |  scorer_ : function or a dict\n",
      " |      Scorer function used on the held out data to choose the best\n",
      " |      parameters for the model.\n",
      " |  \n",
      " |      For multi-metric evaluation, this attribute holds the validated\n",
      " |      ``scoring`` dict which maps the scorer key to the scorer callable.\n",
      " |  \n",
      " |  n_splits_ : int\n",
      " |      The number of cross-validation splits (folds/iterations).\n",
      " |  \n",
      " |  Notes\n",
      " |  ------\n",
      " |  The parameters selected are those that maximize the score of the left out\n",
      " |  data, unless an explicit score is passed in which case it is used instead.\n",
      " |  \n",
      " |  If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      " |  point in the grid (and not `n_jobs` times). This is done for efficiency\n",
      " |  reasons if individual jobs take very little time, but may raise errors if\n",
      " |  the dataset is large and not enough memory is available.  A workaround in\n",
      " |  this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      " |  `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      " |  n_jobs`.\n",
      " |  \n",
      " |  See Also\n",
      " |  ---------\n",
      " |  :class:`ParameterGrid`:\n",
      " |      generates all the combinations of a hyperparameter grid.\n",
      " |  \n",
      " |  :func:`sklearn.model_selection.train_test_split`:\n",
      " |      utility function to split the data into a development set usable\n",
      " |      for fitting a GridSearchCV instance and an evaluation set for\n",
      " |      its final evaluation.\n",
      " |  \n",
      " |  :func:`sklearn.metrics.make_scorer`:\n",
      " |      Make a scorer from a performance metric or loss function.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GridSearchCV\n",
      " |      BaseSearchCV\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score='raise', return_train_score=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSearchCV:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Call decision_function on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``decision_function``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  fit(self, X, y=None, groups=None, **fit_params)\n",
      " |      Run fit with all sets of parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      groups : array-like, with shape (n_samples,), optional\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set.\n",
      " |      \n",
      " |      **fit_params : dict of string -> object\n",
      " |          Parameters passed to the ``fit`` method of the estimator\n",
      " |  \n",
      " |  inverse_transform(self, Xt)\n",
      " |      Call inverse_transform on the estimator with the best found params.\n",
      " |      \n",
      " |      Only available if the underlying estimator implements\n",
      " |      ``inverse_transform`` and ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      Xt : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Call predict on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Call predict_log_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_log_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Call predict_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Returns the score on the given data, if the estimator has been refit.\n",
      " |      \n",
      " |      This uses the score defined by ``scoring`` where provided, and the\n",
      " |      ``best_estimator_.score`` method otherwise.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |          Input data, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Call transform on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if the underlying estimator supports ``transform`` and\n",
      " |      ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseSearchCV:\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  grid_scores_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'min_samples_leaf': range(10, 100, 10), 'n_estimators': range(10, 100, 10), 'max_features': ['auto', 'sqrt', 'log2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RR.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: -0.5598673067283451 越接近1越好\n",
      "mean_absolute_error: 1.0570588235294118\n",
      "mean_squared_error: 1.8553235294117647\n",
      "median_absolute_error: 0.8399999999999999\n",
      "r2_score: -0.6106079522093957\n"
     ]
    }
   ],
   "source": [
    "y_pred = tree_model.predict(X_test)\n",
    "tree_model.score(X_test, y_pred)\n",
    "evalu(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import preprocessing\n",
    "#lbl = preprocessing.LabelEncoder()\n",
    "#train_x['acc_id1'] = lbl.fit_transform(train_x['acc_id1'].astype(str))#将提示的包含错误数据类型这一列进行转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "XGB_model=XGBRegressor(max_depth=5,learning_rate=0.1,verbose=True,n_estimators=200)  \n",
    "# XGB_model=XGBClassifier(objective= 'multi:softprob') # 多分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:4.51616\n",
      "[1]\tvalidation_0-rmse:4.09213\n",
      "[2]\tvalidation_0-rmse:3.71194\n",
      "[3]\tvalidation_0-rmse:3.37021\n",
      "[4]\tvalidation_0-rmse:3.06114\n",
      "[5]\tvalidation_0-rmse:2.7841\n",
      "[6]\tvalidation_0-rmse:2.53683\n",
      "[7]\tvalidation_0-rmse:2.3131\n",
      "[8]\tvalidation_0-rmse:2.11024\n",
      "[9]\tvalidation_0-rmse:1.92395\n",
      "[10]\tvalidation_0-rmse:1.75688\n",
      "[11]\tvalidation_0-rmse:1.60425\n",
      "[12]\tvalidation_0-rmse:1.46672\n",
      "[13]\tvalidation_0-rmse:1.34166\n",
      "[14]\tvalidation_0-rmse:1.22924\n",
      "[15]\tvalidation_0-rmse:1.12744\n",
      "[16]\tvalidation_0-rmse:1.03471\n",
      "[17]\tvalidation_0-rmse:0.951077\n",
      "[18]\tvalidation_0-rmse:0.875088\n",
      "[19]\tvalidation_0-rmse:0.805853\n",
      "[20]\tvalidation_0-rmse:0.743365\n",
      "[21]\tvalidation_0-rmse:0.684455\n",
      "[22]\tvalidation_0-rmse:0.631442\n",
      "[23]\tvalidation_0-rmse:0.583811\n",
      "[24]\tvalidation_0-rmse:0.539535\n",
      "[25]\tvalidation_0-rmse:0.499832\n",
      "[26]\tvalidation_0-rmse:0.461854\n",
      "[27]\tvalidation_0-rmse:0.429025\n",
      "[28]\tvalidation_0-rmse:0.398513\n",
      "[29]\tvalidation_0-rmse:0.370768\n",
      "[30]\tvalidation_0-rmse:0.344006\n",
      "[31]\tvalidation_0-rmse:0.321394\n",
      "[32]\tvalidation_0-rmse:0.301452\n",
      "[33]\tvalidation_0-rmse:0.280571\n",
      "[34]\tvalidation_0-rmse:0.264121\n",
      "[35]\tvalidation_0-rmse:0.246286\n",
      "[36]\tvalidation_0-rmse:0.231731\n",
      "[37]\tvalidation_0-rmse:0.218704\n",
      "[38]\tvalidation_0-rmse:0.205578\n",
      "[39]\tvalidation_0-rmse:0.193915\n",
      "[40]\tvalidation_0-rmse:0.182305\n",
      "[41]\tvalidation_0-rmse:0.17171\n",
      "[42]\tvalidation_0-rmse:0.16275\n",
      "[43]\tvalidation_0-rmse:0.154683\n",
      "[44]\tvalidation_0-rmse:0.147523\n",
      "[45]\tvalidation_0-rmse:0.139155\n",
      "[46]\tvalidation_0-rmse:0.131822\n",
      "[47]\tvalidation_0-rmse:0.126136\n",
      "[48]\tvalidation_0-rmse:0.119727\n",
      "[49]\tvalidation_0-rmse:0.112823\n",
      "[50]\tvalidation_0-rmse:0.10802\n",
      "[51]\tvalidation_0-rmse:0.102149\n",
      "[52]\tvalidation_0-rmse:0.096972\n",
      "[53]\tvalidation_0-rmse:0.092615\n",
      "[54]\tvalidation_0-rmse:0.087915\n",
      "[55]\tvalidation_0-rmse:0.084128\n",
      "[56]\tvalidation_0-rmse:0.080151\n",
      "[57]\tvalidation_0-rmse:0.076575\n",
      "[58]\tvalidation_0-rmse:0.073591\n",
      "[59]\tvalidation_0-rmse:0.069537\n",
      "[60]\tvalidation_0-rmse:0.065694\n",
      "[61]\tvalidation_0-rmse:0.062416\n",
      "[62]\tvalidation_0-rmse:0.059234\n",
      "[63]\tvalidation_0-rmse:0.056795\n",
      "[64]\tvalidation_0-rmse:0.054734\n",
      "[65]\tvalidation_0-rmse:0.052389\n",
      "[66]\tvalidation_0-rmse:0.050241\n",
      "[67]\tvalidation_0-rmse:0.048222\n",
      "[68]\tvalidation_0-rmse:0.045524\n",
      "[69]\tvalidation_0-rmse:0.043148\n",
      "[70]\tvalidation_0-rmse:0.040691\n",
      "[71]\tvalidation_0-rmse:0.0392\n",
      "[72]\tvalidation_0-rmse:0.037255\n",
      "[73]\tvalidation_0-rmse:0.035835\n",
      "[74]\tvalidation_0-rmse:0.034484\n",
      "[75]\tvalidation_0-rmse:0.033229\n",
      "[76]\tvalidation_0-rmse:0.031668\n",
      "[77]\tvalidation_0-rmse:0.03055\n",
      "[78]\tvalidation_0-rmse:0.028982\n",
      "[79]\tvalidation_0-rmse:0.027839\n",
      "[80]\tvalidation_0-rmse:0.026865\n",
      "[81]\tvalidation_0-rmse:0.025732\n",
      "[82]\tvalidation_0-rmse:0.024392\n",
      "[83]\tvalidation_0-rmse:0.023323\n",
      "[84]\tvalidation_0-rmse:0.022286\n",
      "[85]\tvalidation_0-rmse:0.021527\n",
      "[86]\tvalidation_0-rmse:0.020317\n",
      "[87]\tvalidation_0-rmse:0.019637\n",
      "[88]\tvalidation_0-rmse:0.019019\n",
      "[89]\tvalidation_0-rmse:0.018106\n",
      "[90]\tvalidation_0-rmse:0.017553\n",
      "[91]\tvalidation_0-rmse:0.01685\n",
      "[92]\tvalidation_0-rmse:0.015975\n",
      "[93]\tvalidation_0-rmse:0.014989\n",
      "[94]\tvalidation_0-rmse:0.014497\n",
      "[95]\tvalidation_0-rmse:0.013739\n",
      "[96]\tvalidation_0-rmse:0.013299\n",
      "[97]\tvalidation_0-rmse:0.012664\n",
      "[98]\tvalidation_0-rmse:0.01192\n",
      "[99]\tvalidation_0-rmse:0.011354\n",
      "[100]\tvalidation_0-rmse:0.010676\n",
      "[101]\tvalidation_0-rmse:0.010285\n",
      "[102]\tvalidation_0-rmse:0.009643\n",
      "[103]\tvalidation_0-rmse:0.009162\n",
      "[104]\tvalidation_0-rmse:0.008689\n",
      "[105]\tvalidation_0-rmse:0.008289\n",
      "[106]\tvalidation_0-rmse:0.007993\n",
      "[107]\tvalidation_0-rmse:0.007692\n",
      "[108]\tvalidation_0-rmse:0.007355\n",
      "[109]\tvalidation_0-rmse:0.007121\n",
      "[110]\tvalidation_0-rmse:0.006855\n",
      "[111]\tvalidation_0-rmse:0.006559\n",
      "[112]\tvalidation_0-rmse:0.006238\n",
      "[113]\tvalidation_0-rmse:0.006009\n",
      "[114]\tvalidation_0-rmse:0.005726\n",
      "[115]\tvalidation_0-rmse:0.005404\n",
      "[116]\tvalidation_0-rmse:0.005229\n",
      "[117]\tvalidation_0-rmse:0.005049\n",
      "[118]\tvalidation_0-rmse:0.004865\n",
      "[119]\tvalidation_0-rmse:0.004616\n",
      "[120]\tvalidation_0-rmse:0.004393\n",
      "[121]\tvalidation_0-rmse:0.004252\n",
      "[122]\tvalidation_0-rmse:0.004104\n",
      "[123]\tvalidation_0-rmse:0.003905\n",
      "[124]\tvalidation_0-rmse:0.003692\n",
      "[125]\tvalidation_0-rmse:0.003596\n",
      "[126]\tvalidation_0-rmse:0.003392\n",
      "[127]\tvalidation_0-rmse:0.003286\n",
      "[128]\tvalidation_0-rmse:0.003091\n",
      "[129]\tvalidation_0-rmse:0.002998\n",
      "[130]\tvalidation_0-rmse:0.002858\n",
      "[131]\tvalidation_0-rmse:0.002765\n",
      "[132]\tvalidation_0-rmse:0.002612\n",
      "[133]\tvalidation_0-rmse:0.002455\n",
      "[134]\tvalidation_0-rmse:0.002326\n",
      "[135]\tvalidation_0-rmse:0.00222\n",
      "[136]\tvalidation_0-rmse:0.002143\n",
      "[137]\tvalidation_0-rmse:0.002034\n",
      "[138]\tvalidation_0-rmse:0.001954\n",
      "[139]\tvalidation_0-rmse:0.001845\n",
      "[140]\tvalidation_0-rmse:0.001748\n",
      "[141]\tvalidation_0-rmse:0.001671\n",
      "[142]\tvalidation_0-rmse:0.001626\n",
      "[143]\tvalidation_0-rmse:0.001579\n",
      "[144]\tvalidation_0-rmse:0.001505\n",
      "[145]\tvalidation_0-rmse:0.001442\n",
      "[146]\tvalidation_0-rmse:0.001381\n",
      "[147]\tvalidation_0-rmse:0.001327\n",
      "[148]\tvalidation_0-rmse:0.001282\n",
      "[149]\tvalidation_0-rmse:0.001222\n",
      "[150]\tvalidation_0-rmse:0.001162\n",
      "[151]\tvalidation_0-rmse:0.001109\n",
      "[152]\tvalidation_0-rmse:0.001081\n",
      "[153]\tvalidation_0-rmse:0.001027\n",
      "[154]\tvalidation_0-rmse:0.000986\n",
      "[155]\tvalidation_0-rmse:0.000948\n",
      "[156]\tvalidation_0-rmse:0.000895\n",
      "[157]\tvalidation_0-rmse:0.000866\n",
      "[158]\tvalidation_0-rmse:0.000833\n",
      "[159]\tvalidation_0-rmse:0.000807\n",
      "[160]\tvalidation_0-rmse:0.000779\n",
      "[161]\tvalidation_0-rmse:0.000737\n",
      "[162]\tvalidation_0-rmse:0.000706\n",
      "[163]\tvalidation_0-rmse:0.000668\n",
      "[164]\tvalidation_0-rmse:0.000637\n",
      "[165]\tvalidation_0-rmse:0.000617\n",
      "[166]\tvalidation_0-rmse:0.000598\n",
      "[167]\tvalidation_0-rmse:0.000573\n",
      "[168]\tvalidation_0-rmse:0.000552\n",
      "[169]\tvalidation_0-rmse:0.000536\n",
      "[170]\tvalidation_0-rmse:0.00051\n",
      "[171]\tvalidation_0-rmse:0.000495\n",
      "[172]\tvalidation_0-rmse:0.000478\n",
      "[173]\tvalidation_0-rmse:0.000462\n",
      "[174]\tvalidation_0-rmse:0.00045\n",
      "[175]\tvalidation_0-rmse:0.000446\n",
      "[176]\tvalidation_0-rmse:0.000439\n",
      "[177]\tvalidation_0-rmse:0.000439\n",
      "[178]\tvalidation_0-rmse:0.000439\n",
      "[179]\tvalidation_0-rmse:0.00044\n",
      "[180]\tvalidation_0-rmse:0.00044\n",
      "[181]\tvalidation_0-rmse:0.00044\n",
      "[182]\tvalidation_0-rmse:0.00044\n",
      "[183]\tvalidation_0-rmse:0.00044\n",
      "[184]\tvalidation_0-rmse:0.00044\n",
      "[185]\tvalidation_0-rmse:0.00044\n",
      "[186]\tvalidation_0-rmse:0.000436\n",
      "[187]\tvalidation_0-rmse:0.000436\n",
      "[188]\tvalidation_0-rmse:0.000436\n",
      "[189]\tvalidation_0-rmse:0.000436\n",
      "[190]\tvalidation_0-rmse:0.000436\n",
      "[191]\tvalidation_0-rmse:0.000436\n",
      "[192]\tvalidation_0-rmse:0.000436\n",
      "[193]\tvalidation_0-rmse:0.000437\n",
      "[194]\tvalidation_0-rmse:0.000437\n",
      "[195]\tvalidation_0-rmse:0.000437\n",
      "[196]\tvalidation_0-rmse:0.000437\n",
      "[197]\tvalidation_0-rmse:0.000437\n",
      "[198]\tvalidation_0-rmse:0.000437\n",
      "[199]\tvalidation_0-rmse:0.000437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, importance_type='gain',\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=None, n_estimators=200, n_jobs=1,\n",
       "       nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1, verbose=True)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set = [(X_train, y_train)]\n",
    "XGB_model.fit(X_train,y_train,eval_set=eval_set,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:4.66372\n",
      "[1]\tvalidation_0-rmse:4.26316\n",
      "[2]\tvalidation_0-rmse:3.89255\n",
      "[3]\tvalidation_0-rmse:3.57508\n",
      "[4]\tvalidation_0-rmse:3.28461\n",
      "[5]\tvalidation_0-rmse:3.03818\n",
      "[6]\tvalidation_0-rmse:2.82011\n",
      "[7]\tvalidation_0-rmse:2.64038\n",
      "[8]\tvalidation_0-rmse:2.47037\n",
      "[9]\tvalidation_0-rmse:2.29194\n",
      "[10]\tvalidation_0-rmse:2.16315\n",
      "[11]\tvalidation_0-rmse:2.0376\n",
      "[12]\tvalidation_0-rmse:1.92927\n",
      "[13]\tvalidation_0-rmse:1.831\n",
      "[14]\tvalidation_0-rmse:1.74207\n",
      "[15]\tvalidation_0-rmse:1.67695\n",
      "[16]\tvalidation_0-rmse:1.60883\n",
      "[17]\tvalidation_0-rmse:1.56682\n",
      "[18]\tvalidation_0-rmse:1.52025\n",
      "[19]\tvalidation_0-rmse:1.47468\n",
      "[20]\tvalidation_0-rmse:1.44741\n",
      "[21]\tvalidation_0-rmse:1.42675\n",
      "[22]\tvalidation_0-rmse:1.40188\n",
      "[23]\tvalidation_0-rmse:1.37183\n",
      "[24]\tvalidation_0-rmse:1.35053\n",
      "[25]\tvalidation_0-rmse:1.33471\n",
      "[26]\tvalidation_0-rmse:1.32308\n",
      "[27]\tvalidation_0-rmse:1.31639\n",
      "[28]\tvalidation_0-rmse:1.31197\n",
      "[29]\tvalidation_0-rmse:1.30185\n",
      "[30]\tvalidation_0-rmse:1.2916\n",
      "[31]\tvalidation_0-rmse:1.28466\n",
      "[32]\tvalidation_0-rmse:1.28026\n",
      "[33]\tvalidation_0-rmse:1.2787\n",
      "[34]\tvalidation_0-rmse:1.27574\n",
      "[35]\tvalidation_0-rmse:1.27018\n",
      "[36]\tvalidation_0-rmse:1.267\n",
      "[37]\tvalidation_0-rmse:1.26705\n",
      "[38]\tvalidation_0-rmse:1.2651\n",
      "[39]\tvalidation_0-rmse:1.26403\n",
      "[40]\tvalidation_0-rmse:1.26357\n",
      "[41]\tvalidation_0-rmse:1.26413\n",
      "[42]\tvalidation_0-rmse:1.2619\n",
      "[43]\tvalidation_0-rmse:1.26062\n",
      "[44]\tvalidation_0-rmse:1.25985\n",
      "[45]\tvalidation_0-rmse:1.25927\n",
      "[46]\tvalidation_0-rmse:1.25852\n",
      "[47]\tvalidation_0-rmse:1.25739\n",
      "[48]\tvalidation_0-rmse:1.25907\n",
      "[49]\tvalidation_0-rmse:1.26033\n",
      "[50]\tvalidation_0-rmse:1.25975\n",
      "[51]\tvalidation_0-rmse:1.25898\n",
      "[52]\tvalidation_0-rmse:1.25902\n",
      "[53]\tvalidation_0-rmse:1.25901\n",
      "[54]\tvalidation_0-rmse:1.25922\n",
      "[55]\tvalidation_0-rmse:1.25985\n",
      "[56]\tvalidation_0-rmse:1.26021\n",
      "[57]\tvalidation_0-rmse:1.25972\n",
      "[58]\tvalidation_0-rmse:1.25966\n",
      "[59]\tvalidation_0-rmse:1.25808\n",
      "[60]\tvalidation_0-rmse:1.25816\n",
      "[61]\tvalidation_0-rmse:1.258\n",
      "[62]\tvalidation_0-rmse:1.25895\n",
      "[63]\tvalidation_0-rmse:1.25941\n",
      "[64]\tvalidation_0-rmse:1.25983\n",
      "[65]\tvalidation_0-rmse:1.25972\n",
      "[66]\tvalidation_0-rmse:1.2591\n",
      "[67]\tvalidation_0-rmse:1.25971\n",
      "[68]\tvalidation_0-rmse:1.25941\n",
      "[69]\tvalidation_0-rmse:1.25902\n",
      "[70]\tvalidation_0-rmse:1.25845\n",
      "[71]\tvalidation_0-rmse:1.25877\n",
      "[72]\tvalidation_0-rmse:1.25855\n",
      "[73]\tvalidation_0-rmse:1.25793\n",
      "[74]\tvalidation_0-rmse:1.25823\n",
      "[75]\tvalidation_0-rmse:1.25825\n",
      "[76]\tvalidation_0-rmse:1.25849\n",
      "[77]\tvalidation_0-rmse:1.25816\n",
      "[78]\tvalidation_0-rmse:1.25804\n",
      "[79]\tvalidation_0-rmse:1.25807\n",
      "[80]\tvalidation_0-rmse:1.2582\n",
      "[81]\tvalidation_0-rmse:1.25866\n",
      "[82]\tvalidation_0-rmse:1.25894\n",
      "[83]\tvalidation_0-rmse:1.25899\n",
      "[84]\tvalidation_0-rmse:1.25953\n",
      "[85]\tvalidation_0-rmse:1.25931\n",
      "[86]\tvalidation_0-rmse:1.25927\n",
      "[87]\tvalidation_0-rmse:1.25935\n",
      "[88]\tvalidation_0-rmse:1.25925\n",
      "[89]\tvalidation_0-rmse:1.25951\n",
      "[90]\tvalidation_0-rmse:1.25968\n",
      "[91]\tvalidation_0-rmse:1.2597\n",
      "[92]\tvalidation_0-rmse:1.25941\n",
      "[93]\tvalidation_0-rmse:1.25939\n",
      "[94]\tvalidation_0-rmse:1.25914\n",
      "[95]\tvalidation_0-rmse:1.25916\n",
      "[96]\tvalidation_0-rmse:1.25907\n",
      "[97]\tvalidation_0-rmse:1.2588\n",
      "[98]\tvalidation_0-rmse:1.25876\n",
      "[99]\tvalidation_0-rmse:1.25873\n",
      "[100]\tvalidation_0-rmse:1.25857\n",
      "[101]\tvalidation_0-rmse:1.25834\n",
      "[102]\tvalidation_0-rmse:1.25823\n",
      "[103]\tvalidation_0-rmse:1.25807\n",
      "[104]\tvalidation_0-rmse:1.25818\n",
      "[105]\tvalidation_0-rmse:1.25833\n",
      "[106]\tvalidation_0-rmse:1.25831\n",
      "[107]\tvalidation_0-rmse:1.25817\n",
      "[108]\tvalidation_0-rmse:1.25828\n",
      "[109]\tvalidation_0-rmse:1.25827\n",
      "[110]\tvalidation_0-rmse:1.25842\n",
      "[111]\tvalidation_0-rmse:1.25843\n",
      "[112]\tvalidation_0-rmse:1.25834\n",
      "[113]\tvalidation_0-rmse:1.25836\n",
      "[114]\tvalidation_0-rmse:1.25837\n",
      "[115]\tvalidation_0-rmse:1.25819\n",
      "[116]\tvalidation_0-rmse:1.25819\n",
      "[117]\tvalidation_0-rmse:1.25824\n",
      "[118]\tvalidation_0-rmse:1.25832\n",
      "[119]\tvalidation_0-rmse:1.25839\n",
      "[120]\tvalidation_0-rmse:1.25848\n",
      "[121]\tvalidation_0-rmse:1.25853\n",
      "[122]\tvalidation_0-rmse:1.25857\n",
      "[123]\tvalidation_0-rmse:1.25868\n",
      "[124]\tvalidation_0-rmse:1.25859\n",
      "[125]\tvalidation_0-rmse:1.25859\n",
      "[126]\tvalidation_0-rmse:1.25867\n",
      "[127]\tvalidation_0-rmse:1.25869\n",
      "[128]\tvalidation_0-rmse:1.25873\n",
      "[129]\tvalidation_0-rmse:1.25873\n",
      "[130]\tvalidation_0-rmse:1.2587\n",
      "[131]\tvalidation_0-rmse:1.2587\n",
      "[132]\tvalidation_0-rmse:1.25867\n",
      "[133]\tvalidation_0-rmse:1.25864\n",
      "[134]\tvalidation_0-rmse:1.25868\n",
      "[135]\tvalidation_0-rmse:1.25866\n",
      "[136]\tvalidation_0-rmse:1.25869\n",
      "[137]\tvalidation_0-rmse:1.25868\n",
      "[138]\tvalidation_0-rmse:1.25866\n",
      "[139]\tvalidation_0-rmse:1.25867\n",
      "[140]\tvalidation_0-rmse:1.25864\n",
      "[141]\tvalidation_0-rmse:1.25862\n",
      "[142]\tvalidation_0-rmse:1.25861\n",
      "[143]\tvalidation_0-rmse:1.25863\n",
      "[144]\tvalidation_0-rmse:1.25865\n",
      "[145]\tvalidation_0-rmse:1.25864\n",
      "[146]\tvalidation_0-rmse:1.25865\n",
      "[147]\tvalidation_0-rmse:1.25869\n",
      "[148]\tvalidation_0-rmse:1.25868\n",
      "[149]\tvalidation_0-rmse:1.25868\n",
      "[150]\tvalidation_0-rmse:1.25865\n",
      "[151]\tvalidation_0-rmse:1.25867\n",
      "[152]\tvalidation_0-rmse:1.25867\n",
      "[153]\tvalidation_0-rmse:1.25867\n",
      "[154]\tvalidation_0-rmse:1.25868\n",
      "[155]\tvalidation_0-rmse:1.25868\n",
      "[156]\tvalidation_0-rmse:1.25868\n",
      "[157]\tvalidation_0-rmse:1.25867\n",
      "[158]\tvalidation_0-rmse:1.25868\n",
      "[159]\tvalidation_0-rmse:1.25868\n",
      "[160]\tvalidation_0-rmse:1.25869\n",
      "[161]\tvalidation_0-rmse:1.2587\n",
      "[162]\tvalidation_0-rmse:1.25869\n",
      "[163]\tvalidation_0-rmse:1.25869\n",
      "[164]\tvalidation_0-rmse:1.25869\n",
      "[165]\tvalidation_0-rmse:1.25871\n",
      "[166]\tvalidation_0-rmse:1.2587\n",
      "[167]\tvalidation_0-rmse:1.2587\n",
      "[168]\tvalidation_0-rmse:1.25871\n",
      "[169]\tvalidation_0-rmse:1.25871\n",
      "[170]\tvalidation_0-rmse:1.25872\n",
      "[171]\tvalidation_0-rmse:1.25872\n",
      "[172]\tvalidation_0-rmse:1.25873\n",
      "[173]\tvalidation_0-rmse:1.25873\n",
      "[174]\tvalidation_0-rmse:1.25873\n",
      "[175]\tvalidation_0-rmse:1.25873\n",
      "[176]\tvalidation_0-rmse:1.25874\n",
      "[177]\tvalidation_0-rmse:1.25874\n",
      "[178]\tvalidation_0-rmse:1.25874\n",
      "[179]\tvalidation_0-rmse:1.25874\n",
      "[180]\tvalidation_0-rmse:1.25874\n",
      "[181]\tvalidation_0-rmse:1.25874\n",
      "[182]\tvalidation_0-rmse:1.25874\n",
      "[183]\tvalidation_0-rmse:1.25874\n",
      "[184]\tvalidation_0-rmse:1.25874\n",
      "[185]\tvalidation_0-rmse:1.25874\n",
      "[186]\tvalidation_0-rmse:1.25874\n",
      "[187]\tvalidation_0-rmse:1.25874\n",
      "[188]\tvalidation_0-rmse:1.25874\n",
      "[189]\tvalidation_0-rmse:1.25874\n",
      "[190]\tvalidation_0-rmse:1.25874\n",
      "[191]\tvalidation_0-rmse:1.25874\n",
      "[192]\tvalidation_0-rmse:1.25874\n",
      "[193]\tvalidation_0-rmse:1.25874\n",
      "[194]\tvalidation_0-rmse:1.25874\n",
      "[195]\tvalidation_0-rmse:1.25874\n",
      "[196]\tvalidation_0-rmse:1.25874\n",
      "[197]\tvalidation_0-rmse:1.25874\n",
      "[198]\tvalidation_0-rmse:1.25874\n",
      "[199]\tvalidation_0-rmse:1.25874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, importance_type='gain',\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=None, n_estimators=200, n_jobs=1,\n",
       "       nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1, verbose=True)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set = [(X_test, y_test)]\n",
    "XGB_model.fit(X_train,y_train,eval_set=eval_set,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: -0.11030833643675142 越接近1越好\n",
      "mean_absolute_error: 0.967518866482903\n",
      "mean_squared_error: 1.5844191801563159\n",
      "median_absolute_error: 0.8034601688385008\n",
      "r2_score: -0.13438254190769516\n"
     ]
    }
   ],
   "source": [
    "y_pred = XGB_model.predict(X_test)\n",
    "XGB_model.score(X_test, y_pred)\n",
    "evalu(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: 0.9999998224653982 越接近1越好\n",
      "mean_absolute_error: 0.00032646487740907367\n",
      "mean_squared_error: 1.9114968446074953e-07\n",
      "median_absolute_error: 0.0002623462677000532\n",
      "r2_score: 0.9999998224635729\n"
     ]
    }
   ],
   "source": [
    "y_pred = XGB_model.predict(X_train)\n",
    "evalu(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00153561 0.00527742 0.01120412 0.02320008 0.00365687 0.00863486\n",
      " 0.0315629  0.02448498 0.00824199 0.01315442 0.02825086 0.00389032\n",
      " 0.00782227 0.02494201 0.00560086 0.03832502 0.00326854 0.00961272\n",
      " 0.02290437 0.02951537 0.00982624 0.02673565 0.00667023 0.01993603\n",
      " 0.02356114 0.00302439 0.01299371 0.01602833 0.00965167 0.01373242\n",
      " 0.00233139 0.0248308  0.02015337 0.00485    0.0009356  0.01589436\n",
      " 0.02464838 0.02012677 0.02653887 0.03397303 0.01272395 0.01324642\n",
      " 0.00488051 0.00677778 0.00993506 0.02151003 0.02870334 0.06596873\n",
      " 0.00670753 0.01223959 0.00942217 0.01191725 0.01335743 0.00804259\n",
      " 0.02240744 0.01190659 0.0068285  0.02663179 0.03610566 0.01109584\n",
      " 0.00178001 0.00620313 0.01693356 0.01314718]\n"
     ]
    }
   ],
   "source": [
    "print(XGB_model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f9f4dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "from matplotlib import pyplot\n",
    "pyplot.bar(range(len(XGB_model.feature_importances_)), XGB_model.feature_importances_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2gkhMp0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119952d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot feature importance using built-in function\n",
    "from xgboost import plot_importance\n",
    "plot_importance(XGB_model)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  9,  1,  1,  2,  7,  1, 11,  1,  1,  1,  1,  1,  1,  1, 13,\n",
       "        6,  1,  1,  8,  1,  1, 12,  1,  1,  1,  1,  5,  1, 14,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1, 10,  1,  1,  4,  1,  1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor()\n",
    "gbdt_RFE = RFE(model, int(0.8*X_train.shape[1]))\n",
    "gbdt_RFE.fit(X_train, y_train)\n",
    "gbdt_RFE.ranking_\n",
    "#特征选择输出结果\n",
    "#gbdt_RFE.support_\n",
    "#输出结果为：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119ceb588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.bar(range(len(XGB_model.feature_importances_)), XGB_model.feature_importances_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 TRAIN: [1 3 4 5] TEST: [0 2]\n",
      "0\n",
      "2\n",
      "i: 1 TRAIN: [0 2 4 5] TEST: [1 3]\n",
      "1\n",
      "3\n",
      "i: 2 TRAIN: [0 1 2 3] TEST: [4 5]\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4],[1,1],[2,2]])\n",
    "y = np.array([0, 0, 1, 1,1,0])\n",
    "skf = StratifiedKFold(3,random_state=1)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(\"i:\",i,\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    for s in test_index:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StratifiedKFold in module sklearn.model_selection._split:\n",
      "\n",
      "class StratifiedKFold(_BaseKFold)\n",
      " |  Stratified K-Folds cross-validator\n",
      " |  \n",
      " |  Provides train/test indices to split data in train/test sets.\n",
      " |  \n",
      " |  This cross-validation object is a variation of KFold that returns\n",
      " |  stratified folds. The folds are made by preserving the percentage of\n",
      " |  samples for each class.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=3\n",
      " |      Number of folds. Must be at least 2.\n",
      " |  \n",
      " |  shuffle : boolean, optional\n",
      " |      Whether to shuffle each stratification of the data before splitting\n",
      " |      into batches.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default=None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`. Used when ``shuffle`` == True.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.model_selection import StratifiedKFold\n",
      " |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      " |  >>> y = np.array([0, 0, 1, 1])\n",
      " |  >>> skf = StratifiedKFold(n_splits=2)\n",
      " |  >>> skf.get_n_splits(X, y)\n",
      " |  2\n",
      " |  >>> print(skf)  # doctest: +NORMALIZE_WHITESPACE\n",
      " |  StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
      " |  >>> for train_index, test_index in skf.split(X, y):\n",
      " |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      " |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      " |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      " |  TRAIN: [1 3] TEST: [0 2]\n",
      " |  TRAIN: [0 2] TEST: [1 3]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  All the folds have size ``trunc(n_samples / n_splits)``, the last one has\n",
      " |  the complementary.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StratifiedKFold\n",
      " |      _BaseKFold\n",
      " |      abc.NewBase\n",
      " |      BaseCrossValidator\n",
      " |      abc.NewBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_splits=3, shuffle=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  split(self, X, y, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |          Note that providing ``y`` is sufficient to generate the splits and\n",
      " |          hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
      " |          ``X`` instead of actual training data.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target variable for supervised learning problems.\n",
      " |          Stratification is done based on the y labels.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |      \n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Randomized CV splitters may return different results for each call of\n",
      " |      split. You can make the results identical by setting ``random_state``\n",
      " |      to an integer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseKFold:\n",
      " |  \n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from abc.NewBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(StratifiedKFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, X_train, y_train, useTrainCV=True, early_stopping_rounds=40):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        \n",
    "        train_data = xgb.DMatrix(X_train, label=y_train, missing=float('nan'))\n",
    "        cvresult = xgb.cv(xgb_param, train_data, num_boost_round=alg.get_params()['n_estimators'], nfold =5,\n",
    "             metrics='rmse', early_stopping_rounds=early_stopping_rounds)\n",
    "        \n",
    "        n_estimators = cvresult.shape[0]\n",
    "        alg.set_params(n_estimators = n_estimators)\n",
    "        alg.set_params(learning_rate = 0.02)\n",
    "        print(cvresult)\n",
    "        #result = pd.DataFrame(cvresult)   #cv缺省返回结果为DataFrame\n",
    "        #result.to_csv('my_preds.csv', index_label = 'n_estimators')\n",
    "        cvresult.to_csv('my_preds_4_1.csv', index_label = 'n_estimators')\n",
    "        \n",
    "        # plot\n",
    "        #test_means = cvresult['test-mlogloss-mean']\n",
    "        #test_stds = cvresult['test-mlogloss-std'] \n",
    "        \n",
    "        #train_means = cvresult['train-mlogloss-mean']\n",
    "        #train_stds = cvresult['train-mlogloss-std'] \n",
    "\n",
    "        #x_axis = range(0, n_estimators)\n",
    "        #pyplot.errorbar(x_axis, test_means, yerr=test_stds ,label='Test')\n",
    "        #pyplot.errorbar(x_axis, train_means, yerr=train_stds ,label='Train')\n",
    "        #pyplot.title(\"XGBoost n_estimators vs Log Loss\")\n",
    "        #pyplot.xlabel( 'n_estimators' )\n",
    "        #pyplot.ylabel( 'Log Loss' )\n",
    "        #pyplot.savefig( 'n_estimators.png' )\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(X_train, y_train)\n",
    "        \n",
    "    #Predict training set:\n",
    "    y_pred = alg.predict(X_train)\n",
    "    evalu(y_train, y_pred)\n",
    "    return alg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'gamma': 0,\n",
       " 'importance_type': 'gain',\n",
       " 'learning_rate': 0.02,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': None,\n",
       " 'n_estimators': 33,\n",
       " 'nthread': 1,\n",
       " 'objective': 'reg:linear',\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'seed': 0,\n",
       " 'silent': True,\n",
       " 'subsample': 1,\n",
       " 'verbosity': 0}"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB_model.get_xgb_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1=X_train.replace(0,np.nan)\n",
    "X_test1=y_test.replace(0,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB_model=XGBRegressor(max_depth=6)  \n",
    "model2=modelfit(XGB_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEXCAYAAACu1P9TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119aae0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cvresult = pd.DataFrame.from_csv('my_preds_4_1.csv')\n",
    "        \n",
    "# plot\n",
    "test_means = cvresult['test-rmse-mean']\n",
    "test_stds = cvresult['test-rmse-std'] \n",
    "        \n",
    "train_means = cvresult['train-rmse-mean']\n",
    "train_stds = cvresult['train-rmse-std'] \n",
    "\n",
    "x_axis = range(0, cvresult.shape[0])\n",
    "        \n",
    "pyplot.errorbar(x_axis, test_means, yerr=test_stds ,label='Test')\n",
    "pyplot.errorbar(x_axis, train_means, yerr=train_stds ,label='Train')\n",
    "pyplot.title(\"XGBoost n_estimators vs rmse\")\n",
    "pyplot.xlabel( 'n_estimators')\n",
    "pyplot.ylabel( 'rmse-std' )\n",
    "pyplot.legend()\n",
    "pyplot.savefig( 'n_estimators4_1.png' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GridSearchCV in module sklearn.model_selection._search:\n",
      "\n",
      "class GridSearchCV(BaseSearchCV)\n",
      " |  Exhaustive search over specified parameter values for an estimator.\n",
      " |  \n",
      " |  Important members are fit, predict.\n",
      " |  \n",
      " |  GridSearchCV implements a \"fit\" and a \"score\" method.\n",
      " |  It also implements \"predict\", \"predict_proba\", \"decision_function\",\n",
      " |  \"transform\" and \"inverse_transform\" if they are implemented in the\n",
      " |  estimator used.\n",
      " |  \n",
      " |  The parameters of the estimator used to apply these methods are optimized\n",
      " |  by cross-validated grid-search over a parameter grid.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <grid_search>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimator : estimator object.\n",
      " |      This is assumed to implement the scikit-learn estimator interface.\n",
      " |      Either estimator needs to provide a ``score`` function,\n",
      " |      or ``scoring`` must be passed.\n",
      " |  \n",
      " |  param_grid : dict or list of dictionaries\n",
      " |      Dictionary with parameters names (string) as keys and lists of\n",
      " |      parameter settings to try as values, or a list of such\n",
      " |      dictionaries, in which case the grids spanned by each dictionary\n",
      " |      in the list are explored. This enables searching over any sequence\n",
      " |      of parameter settings.\n",
      " |  \n",
      " |  scoring : string, callable, list/tuple, dict or None, default: None\n",
      " |      A single string (see :ref:`scoring_parameter`) or a callable\n",
      " |      (see :ref:`scoring`) to evaluate the predictions on the test set.\n",
      " |  \n",
      " |      For evaluating multiple metrics, either give a list of (unique) strings\n",
      " |      or a dict with names as keys and callables as values.\n",
      " |  \n",
      " |      NOTE that when using custom scorers, each scorer should return a single\n",
      " |      value. Metric functions returning a list/array of values can be wrapped\n",
      " |      into multiple scorers that return one value each.\n",
      " |  \n",
      " |      See :ref:`multimetric_grid_search` for an example.\n",
      " |  \n",
      " |      If None, the estimator's default scorer (if available) is used.\n",
      " |  \n",
      " |  fit_params : dict, optional\n",
      " |      Parameters to pass to the fit method.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``fit_params`` as a constructor argument was deprecated in version\n",
      " |         0.19 and will be removed in version 0.21. Pass fit parameters to\n",
      " |         the ``fit`` method instead.\n",
      " |  \n",
      " |  n_jobs : int, default=1\n",
      " |      Number of jobs to run in parallel.\n",
      " |  \n",
      " |  pre_dispatch : int, or string, optional\n",
      " |      Controls the number of jobs that get dispatched during parallel\n",
      " |      execution. Reducing this number can be useful to avoid an\n",
      " |      explosion of memory consumption when more jobs get dispatched\n",
      " |      than CPUs can process. This parameter can be:\n",
      " |  \n",
      " |          - None, in which case all the jobs are immediately\n",
      " |            created and spawned. Use this for lightweight and\n",
      " |            fast-running jobs, to avoid delays due to on-demand\n",
      " |            spawning of the jobs\n",
      " |  \n",
      " |          - An int, giving the exact number of total jobs that are\n",
      " |            spawned\n",
      " |  \n",
      " |          - A string, giving an expression as a function of n_jobs,\n",
      " |            as in '2*n_jobs'\n",
      " |  \n",
      " |  iid : boolean, default=True\n",
      " |      If True, the data is assumed to be identically distributed across\n",
      " |      the folds, and the loss minimized is the total loss per sample,\n",
      " |      and not the mean loss across the folds.\n",
      " |  \n",
      " |  cv : int, cross-validation generator or an iterable, optional\n",
      " |      Determines the cross-validation splitting strategy.\n",
      " |      Possible inputs for cv are:\n",
      " |        - None, to use the default 3-fold cross validation,\n",
      " |        - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      " |        - An object to be used as a cross-validation generator.\n",
      " |        - An iterable yielding train, test splits.\n",
      " |  \n",
      " |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      " |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      " |      other cases, :class:`KFold` is used.\n",
      " |  \n",
      " |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      " |      cross-validation strategies that can be used here.\n",
      " |  \n",
      " |  refit : boolean, or string, default=True\n",
      " |      Refit an estimator using the best found parameters on the whole\n",
      " |      dataset.\n",
      " |  \n",
      " |      For multiple metric evaluation, this needs to be a string denoting the\n",
      " |      scorer is used to find the best parameters for refitting the estimator\n",
      " |      at the end.\n",
      " |  \n",
      " |      The refitted estimator is made available at the ``best_estimator_``\n",
      " |      attribute and permits using ``predict`` directly on this\n",
      " |      ``GridSearchCV`` instance.\n",
      " |  \n",
      " |      Also for multiple metric evaluation, the attributes ``best_index_``,\n",
      " |      ``best_score_`` and ``best_parameters_`` will only be available if\n",
      " |      ``refit`` is set and all of them will be determined w.r.t this specific\n",
      " |      scorer.\n",
      " |  \n",
      " |      See ``scoring`` parameter to know more about multiple metric\n",
      " |      evaluation.\n",
      " |  \n",
      " |  verbose : integer\n",
      " |      Controls the verbosity: the higher, the more messages.\n",
      " |  \n",
      " |  error_score : 'raise' (default) or numeric\n",
      " |      Value to assign to the score if an error occurs in estimator fitting.\n",
      " |      If set to 'raise', the error is raised. If a numeric value is given,\n",
      " |      FitFailedWarning is raised. This parameter does not affect the refit\n",
      " |      step, which will always raise the error.\n",
      " |  \n",
      " |  return_train_score : boolean, default=True\n",
      " |      If ``'False'``, the ``cv_results_`` attribute will not include training\n",
      " |      scores.\n",
      " |  \n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn import svm, datasets\n",
      " |  >>> from sklearn.model_selection import GridSearchCV\n",
      " |  >>> iris = datasets.load_iris()\n",
      " |  >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      " |  >>> svc = svm.SVC()\n",
      " |  >>> clf = GridSearchCV(svc, parameters)\n",
      " |  >>> clf.fit(iris.data, iris.target)\n",
      " |  ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n",
      " |  GridSearchCV(cv=None, error_score=...,\n",
      " |         estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,\n",
      " |                       decision_function_shape='ovr', degree=..., gamma=...,\n",
      " |                       kernel='rbf', max_iter=-1, probability=False,\n",
      " |                       random_state=None, shrinking=True, tol=...,\n",
      " |                       verbose=False),\n",
      " |         fit_params=None, iid=..., n_jobs=1,\n",
      " |         param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,\n",
      " |         scoring=..., verbose=...)\n",
      " |  >>> sorted(clf.cv_results_.keys())\n",
      " |  ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n",
      " |  ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n",
      " |   'mean_train_score', 'param_C', 'param_kernel', 'params',...\n",
      " |   'rank_test_score', 'split0_test_score',...\n",
      " |   'split0_train_score', 'split1_test_score', 'split1_train_score',...\n",
      " |   'split2_test_score', 'split2_train_score',...\n",
      " |   'std_fit_time', 'std_score_time', 'std_test_score', 'std_train_score'...]\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cv_results_ : dict of numpy (masked) ndarrays\n",
      " |      A dict with keys as column headers and values as columns, that can be\n",
      " |      imported into a pandas ``DataFrame``.\n",
      " |  \n",
      " |      For instance the below given table\n",
      " |  \n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n",
      " |      +============+===========+============+=================+===+=========+\n",
      " |      |  'poly'    |     --    |      2     |        0.8      |...|    2    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'poly'    |     --    |      3     |        0.7      |...|    4    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'rbf'     |     0.1   |     --     |        0.8      |...|    3    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'rbf'     |     0.2   |     --     |        0.9      |...|    1    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |  \n",
      " |      will be represented by a ``cv_results_`` dict of::\n",
      " |  \n",
      " |          {\n",
      " |          'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n",
      " |                                       mask = [False False False False]...)\n",
      " |          'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n",
      " |                                      mask = [ True  True False False]...),\n",
      " |          'param_degree': masked_array(data = [2.0 3.0 -- --],\n",
      " |                                       mask = [False False  True  True]...),\n",
      " |          'split0_test_score'  : [0.8, 0.7, 0.8, 0.9],\n",
      " |          'split1_test_score'  : [0.82, 0.5, 0.7, 0.78],\n",
      " |          'mean_test_score'    : [0.81, 0.60, 0.75, 0.82],\n",
      " |          'std_test_score'     : [0.02, 0.01, 0.03, 0.03],\n",
      " |          'rank_test_score'    : [2, 4, 3, 1],\n",
      " |          'split0_train_score' : [0.8, 0.9, 0.7],\n",
      " |          'split1_train_score' : [0.82, 0.5, 0.7],\n",
      " |          'mean_train_score'   : [0.81, 0.7, 0.7],\n",
      " |          'std_train_score'    : [0.03, 0.03, 0.04],\n",
      " |          'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n",
      " |          'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n",
      " |          'mean_score_time'    : [0.007, 0.06, 0.04, 0.04],\n",
      " |          'std_score_time'     : [0.001, 0.002, 0.003, 0.005],\n",
      " |          'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
      " |          }\n",
      " |  \n",
      " |      NOTE\n",
      " |  \n",
      " |      The key ``'params'`` is used to store a list of parameter\n",
      " |      settings dicts for all the parameter candidates.\n",
      " |  \n",
      " |      The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      " |      ``std_score_time`` are all in seconds.\n",
      " |  \n",
      " |      For multi-metric evaluation, the scores for all the scorers are\n",
      " |      available in the ``cv_results_`` dict at the keys ending with that\n",
      " |      scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
      " |      above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      " |  \n",
      " |  best_estimator_ : estimator or dict\n",
      " |      Estimator that was chosen by the search, i.e. estimator\n",
      " |      which gave highest score (or smallest loss if specified)\n",
      " |      on the left out data. Not available if ``refit=False``.\n",
      " |  \n",
      " |      See ``refit`` parameter for more information on allowed values.\n",
      " |  \n",
      " |  best_score_ : float\n",
      " |      Mean cross-validated score of the best_estimator\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |  best_params_ : dict\n",
      " |      Parameter setting that gave the best results on the hold out data.\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |  best_index_ : int\n",
      " |      The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      " |      candidate parameter setting.\n",
      " |  \n",
      " |      The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      " |      the parameter setting for the best model, that gives the highest\n",
      " |      mean score (``search.best_score_``).\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |  scorer_ : function or a dict\n",
      " |      Scorer function used on the held out data to choose the best\n",
      " |      parameters for the model.\n",
      " |  \n",
      " |      For multi-metric evaluation, this attribute holds the validated\n",
      " |      ``scoring`` dict which maps the scorer key to the scorer callable.\n",
      " |  \n",
      " |  n_splits_ : int\n",
      " |      The number of cross-validation splits (folds/iterations).\n",
      " |  \n",
      " |  Notes\n",
      " |  ------\n",
      " |  The parameters selected are those that maximize the score of the left out\n",
      " |  data, unless an explicit score is passed in which case it is used instead.\n",
      " |  \n",
      " |  If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      " |  point in the grid (and not `n_jobs` times). This is done for efficiency\n",
      " |  reasons if individual jobs take very little time, but may raise errors if\n",
      " |  the dataset is large and not enough memory is available.  A workaround in\n",
      " |  this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      " |  `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      " |  n_jobs`.\n",
      " |  \n",
      " |  See Also\n",
      " |  ---------\n",
      " |  :class:`ParameterGrid`:\n",
      " |      generates all the combinations of a hyperparameter grid.\n",
      " |  \n",
      " |  :func:`sklearn.model_selection.train_test_split`:\n",
      " |      utility function to split the data into a development set usable\n",
      " |      for fitting a GridSearchCV instance and an evaluation set for\n",
      " |      its final evaluation.\n",
      " |  \n",
      " |  :func:`sklearn.metrics.make_scorer`:\n",
      " |      Make a scorer from a performance metric or loss function.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GridSearchCV\n",
      " |      BaseSearchCV\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score='raise', return_train_score=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSearchCV:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Call decision_function on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``decision_function``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  fit(self, X, y=None, groups=None, **fit_params)\n",
      " |      Run fit with all sets of parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      groups : array-like, with shape (n_samples,), optional\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set.\n",
      " |      \n",
      " |      **fit_params : dict of string -> object\n",
      " |          Parameters passed to the ``fit`` method of the estimator\n",
      " |  \n",
      " |  inverse_transform(self, Xt)\n",
      " |      Call inverse_transform on the estimator with the best found params.\n",
      " |      \n",
      " |      Only available if the underlying estimator implements\n",
      " |      ``inverse_transform`` and ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      Xt : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Call predict on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Call predict_log_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_log_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Call predict_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Returns the score on the given data, if the estimator has been refit.\n",
      " |      \n",
      " |      This uses the score defined by ``scoring`` where provided, and the\n",
      " |      ``best_estimator_.score`` method otherwise.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |          Input data, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Call transform on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if the underlying estimator supports ``transform`` and\n",
      " |      ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseSearchCV:\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  grid_scores_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.0093184, 5.445152 , 4.867327 , 4.598325 , 5.014253 , 3.958854 ,\n",
       "       5.32154  , 5.253059 , 5.5825653, 5.388024 , 4.9734144, 5.6429524,\n",
       "       5.080106 , 4.872859 , 4.5703444, 4.8155637, 5.513289 , 5.127613 ,\n",
       "       4.5042696, 5.0479097, 5.245301 , 5.462021 , 5.6713257, 5.49961  ,\n",
       "       4.858288 , 5.516447 , 4.9598913, 5.0579996, 4.899659 , 4.216854 ,\n",
       "       5.490884 , 5.099413 , 4.588899 , 4.784512 ], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: -0.15338225646038484 越接近1越好\n",
      "mean_absolute_error: 0.9345443240334006\n",
      "mean_squared_error: 1.432011156317457\n",
      "median_absolute_error: 0.9742831420898437\n",
      "r2_score: -0.2431301169067377\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(X_test)\n",
    "evalu(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>95</th>\n",
       "      <th>151</th>\n",
       "      <th>227</th>\n",
       "      <th>255</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>328</th>\n",
       "      <th>329</th>\n",
       "      <th>333</th>\n",
       "      <th>341</th>\n",
       "      <th>408</th>\n",
       "      <th>410</th>\n",
       "      <th>512</th>\n",
       "      <th>609</th>\n",
       "      <th>611</th>\n",
       "      <th>625</th>\n",
       "      <th>629</th>\n",
       "      <th>690</th>\n",
       "      <th>702</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>0.21665</td>\n",
       "      <td>0.88219</td>\n",
       "      <td>0.04984</td>\n",
       "      <td>1.42515</td>\n",
       "      <td>0.50316</td>\n",
       "      <td>1.36946</td>\n",
       "      <td>0.81417</td>\n",
       "      <td>0.77809</td>\n",
       "      <td>3.68838</td>\n",
       "      <td>3.34931</td>\n",
       "      <td>0.04667</td>\n",
       "      <td>0.01206</td>\n",
       "      <td>0.01894</td>\n",
       "      <td>0.40095</td>\n",
       "      <td>1.93523</td>\n",
       "      <td>0.99138</td>\n",
       "      <td>1.46274</td>\n",
       "      <td>0.31146</td>\n",
       "      <td>3.03461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>0.13166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53613</td>\n",
       "      <td>0.87813</td>\n",
       "      <td>0.35152</td>\n",
       "      <td>0.97828</td>\n",
       "      <td>0.12497</td>\n",
       "      <td>2.05840</td>\n",
       "      <td>24.05715</td>\n",
       "      <td>8.29215</td>\n",
       "      <td>0.02381</td>\n",
       "      <td>0.02490</td>\n",
       "      <td>0.33430</td>\n",
       "      <td>0.17264</td>\n",
       "      <td>0.80085</td>\n",
       "      <td>0.57797</td>\n",
       "      <td>0.53797</td>\n",
       "      <td>0.01671</td>\n",
       "      <td>14.48115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>0.02171</td>\n",
       "      <td>1.61407</td>\n",
       "      <td>0.62370</td>\n",
       "      <td>1.02824</td>\n",
       "      <td>0.25232</td>\n",
       "      <td>1.80161</td>\n",
       "      <td>0.42362</td>\n",
       "      <td>2.34287</td>\n",
       "      <td>22.86243</td>\n",
       "      <td>5.64347</td>\n",
       "      <td>0.46603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.06362</td>\n",
       "      <td>0.02620</td>\n",
       "      <td>0.90634</td>\n",
       "      <td>0.92651</td>\n",
       "      <td>1.53389</td>\n",
       "      <td>0.15329</td>\n",
       "      <td>6.77172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>0.05876</td>\n",
       "      <td>6.25162</td>\n",
       "      <td>0.02937</td>\n",
       "      <td>0.08661</td>\n",
       "      <td>0.15397</td>\n",
       "      <td>0.02480</td>\n",
       "      <td>0.42012</td>\n",
       "      <td>1.69665</td>\n",
       "      <td>0.04387</td>\n",
       "      <td>0.62999</td>\n",
       "      <td>0.30132</td>\n",
       "      <td>0.09270</td>\n",
       "      <td>0.01189</td>\n",
       "      <td>0.04555</td>\n",
       "      <td>0.00038</td>\n",
       "      <td>0.48827</td>\n",
       "      <td>1.23912</td>\n",
       "      <td>0.15705</td>\n",
       "      <td>18.24170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>0.06799</td>\n",
       "      <td>0.97867</td>\n",
       "      <td>0.16832</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.01265</td>\n",
       "      <td>4.24549</td>\n",
       "      <td>0.03225</td>\n",
       "      <td>2.50411</td>\n",
       "      <td>27.12543</td>\n",
       "      <td>7.57322</td>\n",
       "      <td>0.32966</td>\n",
       "      <td>0.00982</td>\n",
       "      <td>0.21648</td>\n",
       "      <td>0.05139</td>\n",
       "      <td>0.52041</td>\n",
       "      <td>0.99326</td>\n",
       "      <td>1.66178</td>\n",
       "      <td>1.59456</td>\n",
       "      <td>1.86928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>0.02062</td>\n",
       "      <td>1.69398</td>\n",
       "      <td>0.01813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.08985</td>\n",
       "      <td>1.06756</td>\n",
       "      <td>1.13181</td>\n",
       "      <td>0.29836</td>\n",
       "      <td>21.69010</td>\n",
       "      <td>7.57267</td>\n",
       "      <td>0.04062</td>\n",
       "      <td>0.04818</td>\n",
       "      <td>0.16694</td>\n",
       "      <td>0.36983</td>\n",
       "      <td>1.50510</td>\n",
       "      <td>1.39289</td>\n",
       "      <td>1.17217</td>\n",
       "      <td>0.12309</td>\n",
       "      <td>8.45865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>0.03385</td>\n",
       "      <td>0.85393</td>\n",
       "      <td>0.00860</td>\n",
       "      <td>0.23050</td>\n",
       "      <td>0.36963</td>\n",
       "      <td>0.42680</td>\n",
       "      <td>4.11528</td>\n",
       "      <td>0.67918</td>\n",
       "      <td>0.63434</td>\n",
       "      <td>7.06480</td>\n",
       "      <td>0.02577</td>\n",
       "      <td>0.02745</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>0.70556</td>\n",
       "      <td>0.33778</td>\n",
       "      <td>0.90136</td>\n",
       "      <td>0.75658</td>\n",
       "      <td>0.03949</td>\n",
       "      <td>10.02541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>0.01147</td>\n",
       "      <td>2.49746</td>\n",
       "      <td>0.02023</td>\n",
       "      <td>1.13841</td>\n",
       "      <td>0.73311</td>\n",
       "      <td>1.56417</td>\n",
       "      <td>0.26545</td>\n",
       "      <td>0.17630</td>\n",
       "      <td>7.21582</td>\n",
       "      <td>8.95995</td>\n",
       "      <td>0.48772</td>\n",
       "      <td>0.04295</td>\n",
       "      <td>0.12646</td>\n",
       "      <td>0.36928</td>\n",
       "      <td>0.17894</td>\n",
       "      <td>1.94130</td>\n",
       "      <td>0.83185</td>\n",
       "      <td>0.20342</td>\n",
       "      <td>7.85651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>0.08797</td>\n",
       "      <td>5.51194</td>\n",
       "      <td>0.05585</td>\n",
       "      <td>0.59987</td>\n",
       "      <td>1.32968</td>\n",
       "      <td>0.90234</td>\n",
       "      <td>5.09951</td>\n",
       "      <td>4.89462</td>\n",
       "      <td>5.14207</td>\n",
       "      <td>2.74218</td>\n",
       "      <td>0.18407</td>\n",
       "      <td>0.01817</td>\n",
       "      <td>0.16725</td>\n",
       "      <td>0.30772</td>\n",
       "      <td>0.76981</td>\n",
       "      <td>1.08495</td>\n",
       "      <td>1.21542</td>\n",
       "      <td>0.09419</td>\n",
       "      <td>8.04926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>0.09491</td>\n",
       "      <td>1.31921</td>\n",
       "      <td>0.01431</td>\n",
       "      <td>2.01048</td>\n",
       "      <td>0.50664</td>\n",
       "      <td>0.87318</td>\n",
       "      <td>8.17920</td>\n",
       "      <td>2.23231</td>\n",
       "      <td>0.00046</td>\n",
       "      <td>13.19897</td>\n",
       "      <td>0.73339</td>\n",
       "      <td>0.01593</td>\n",
       "      <td>0.04562</td>\n",
       "      <td>0.41027</td>\n",
       "      <td>3.16531</td>\n",
       "      <td>1.82837</td>\n",
       "      <td>1.02263</td>\n",
       "      <td>0.20157</td>\n",
       "      <td>22.97987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>0.41992</td>\n",
       "      <td>0.88538</td>\n",
       "      <td>0.01799</td>\n",
       "      <td>1.74525</td>\n",
       "      <td>0.94244</td>\n",
       "      <td>5.34755</td>\n",
       "      <td>1.61558</td>\n",
       "      <td>1.78076</td>\n",
       "      <td>0.35707</td>\n",
       "      <td>4.41223</td>\n",
       "      <td>0.05842</td>\n",
       "      <td>0.00992</td>\n",
       "      <td>0.02183</td>\n",
       "      <td>2.16896</td>\n",
       "      <td>0.41462</td>\n",
       "      <td>0.70009</td>\n",
       "      <td>0.34637</td>\n",
       "      <td>0.22988</td>\n",
       "      <td>9.98995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>0.00113</td>\n",
       "      <td>1.35206</td>\n",
       "      <td>0.04591</td>\n",
       "      <td>0.34025</td>\n",
       "      <td>0.33539</td>\n",
       "      <td>0.45868</td>\n",
       "      <td>0.62443</td>\n",
       "      <td>1.26417</td>\n",
       "      <td>12.05048</td>\n",
       "      <td>3.28502</td>\n",
       "      <td>0.05837</td>\n",
       "      <td>0.01788</td>\n",
       "      <td>0.06615</td>\n",
       "      <td>1.33412</td>\n",
       "      <td>0.10335</td>\n",
       "      <td>0.45685</td>\n",
       "      <td>0.48942</td>\n",
       "      <td>0.00417</td>\n",
       "      <td>7.89301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>0.40796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23866</td>\n",
       "      <td>1.17559</td>\n",
       "      <td>1.76807</td>\n",
       "      <td>0.17887</td>\n",
       "      <td>0.01133</td>\n",
       "      <td>5.32448</td>\n",
       "      <td>33.60701</td>\n",
       "      <td>7.12865</td>\n",
       "      <td>0.32424</td>\n",
       "      <td>0.02416</td>\n",
       "      <td>0.09210</td>\n",
       "      <td>1.18006</td>\n",
       "      <td>0.42058</td>\n",
       "      <td>0.55031</td>\n",
       "      <td>2.05115</td>\n",
       "      <td>0.20938</td>\n",
       "      <td>1.94367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>0.67405</td>\n",
       "      <td>0.37177</td>\n",
       "      <td>0.10587</td>\n",
       "      <td>0.16205</td>\n",
       "      <td>3.59076</td>\n",
       "      <td>12.25760</td>\n",
       "      <td>0.01248</td>\n",
       "      <td>0.72428</td>\n",
       "      <td>1.10377</td>\n",
       "      <td>2.09027</td>\n",
       "      <td>1.15650</td>\n",
       "      <td>0.03697</td>\n",
       "      <td>0.11980</td>\n",
       "      <td>0.10955</td>\n",
       "      <td>0.20712</td>\n",
       "      <td>6.08305</td>\n",
       "      <td>13.13537</td>\n",
       "      <td>0.17801</td>\n",
       "      <td>16.98043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>0.07001</td>\n",
       "      <td>0.32965</td>\n",
       "      <td>0.01033</td>\n",
       "      <td>2.81866</td>\n",
       "      <td>0.18569</td>\n",
       "      <td>4.94361</td>\n",
       "      <td>2.78965</td>\n",
       "      <td>1.52370</td>\n",
       "      <td>25.50913</td>\n",
       "      <td>5.39667</td>\n",
       "      <td>0.30388</td>\n",
       "      <td>0.00290</td>\n",
       "      <td>0.00581</td>\n",
       "      <td>0.35655</td>\n",
       "      <td>2.29981</td>\n",
       "      <td>1.93435</td>\n",
       "      <td>0.99718</td>\n",
       "      <td>0.14217</td>\n",
       "      <td>8.22111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>0.02889</td>\n",
       "      <td>3.50863</td>\n",
       "      <td>0.02749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.22258</td>\n",
       "      <td>0.00099</td>\n",
       "      <td>2.73331</td>\n",
       "      <td>2.23906</td>\n",
       "      <td>20.84720</td>\n",
       "      <td>13.50719</td>\n",
       "      <td>0.19307</td>\n",
       "      <td>0.11313</td>\n",
       "      <td>0.55639</td>\n",
       "      <td>0.04670</td>\n",
       "      <td>1.25082</td>\n",
       "      <td>0.21616</td>\n",
       "      <td>1.59506</td>\n",
       "      <td>0.21298</td>\n",
       "      <td>3.02060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>0.00164</td>\n",
       "      <td>0.75942</td>\n",
       "      <td>0.13510</td>\n",
       "      <td>0.97619</td>\n",
       "      <td>0.65942</td>\n",
       "      <td>2.71784</td>\n",
       "      <td>0.20504</td>\n",
       "      <td>1.91330</td>\n",
       "      <td>27.26684</td>\n",
       "      <td>3.48436</td>\n",
       "      <td>0.35362</td>\n",
       "      <td>0.00452</td>\n",
       "      <td>0.54344</td>\n",
       "      <td>0.00676</td>\n",
       "      <td>0.31096</td>\n",
       "      <td>2.22112</td>\n",
       "      <td>1.69986</td>\n",
       "      <td>0.05988</td>\n",
       "      <td>6.43034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>0.01724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25375</td>\n",
       "      <td>0.09839</td>\n",
       "      <td>1.20841</td>\n",
       "      <td>1.66670</td>\n",
       "      <td>6.67624</td>\n",
       "      <td>4.35269</td>\n",
       "      <td>0.03119</td>\n",
       "      <td>0.01703</td>\n",
       "      <td>0.33269</td>\n",
       "      <td>0.03250</td>\n",
       "      <td>0.07011</td>\n",
       "      <td>0.63671</td>\n",
       "      <td>1.86806</td>\n",
       "      <td>2.94334</td>\n",
       "      <td>15.12912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>0.05733</td>\n",
       "      <td>1.21480</td>\n",
       "      <td>0.08082</td>\n",
       "      <td>0.83988</td>\n",
       "      <td>1.01109</td>\n",
       "      <td>3.81467</td>\n",
       "      <td>0.55319</td>\n",
       "      <td>1.64202</td>\n",
       "      <td>14.69790</td>\n",
       "      <td>18.69431</td>\n",
       "      <td>0.06907</td>\n",
       "      <td>0.00858</td>\n",
       "      <td>0.38908</td>\n",
       "      <td>0.02894</td>\n",
       "      <td>0.38801</td>\n",
       "      <td>1.33992</td>\n",
       "      <td>0.89026</td>\n",
       "      <td>0.02777</td>\n",
       "      <td>10.35255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>0.04595</td>\n",
       "      <td>0.94857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.01870</td>\n",
       "      <td>0.67295</td>\n",
       "      <td>2.22099</td>\n",
       "      <td>1.03221</td>\n",
       "      <td>5.08137</td>\n",
       "      <td>1.40578</td>\n",
       "      <td>4.80469</td>\n",
       "      <td>0.07142</td>\n",
       "      <td>0.12582</td>\n",
       "      <td>0.34505</td>\n",
       "      <td>0.02583</td>\n",
       "      <td>4.17462</td>\n",
       "      <td>0.51764</td>\n",
       "      <td>4.86192</td>\n",
       "      <td>0.21369</td>\n",
       "      <td>0.23934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>0.01831</td>\n",
       "      <td>1.93053</td>\n",
       "      <td>0.36806</td>\n",
       "      <td>0.80604</td>\n",
       "      <td>0.44843</td>\n",
       "      <td>1.03499</td>\n",
       "      <td>4.69355</td>\n",
       "      <td>1.68496</td>\n",
       "      <td>16.68297</td>\n",
       "      <td>13.97481</td>\n",
       "      <td>0.07460</td>\n",
       "      <td>0.03235</td>\n",
       "      <td>0.37690</td>\n",
       "      <td>0.70773</td>\n",
       "      <td>4.29459</td>\n",
       "      <td>1.58893</td>\n",
       "      <td>2.74500</td>\n",
       "      <td>2.93867</td>\n",
       "      <td>1.56245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>0.13614</td>\n",
       "      <td>0.20685</td>\n",
       "      <td>0.01564</td>\n",
       "      <td>0.06968</td>\n",
       "      <td>0.79499</td>\n",
       "      <td>0.47522</td>\n",
       "      <td>1.30937</td>\n",
       "      <td>2.60831</td>\n",
       "      <td>19.56838</td>\n",
       "      <td>4.72219</td>\n",
       "      <td>0.03778</td>\n",
       "      <td>0.01410</td>\n",
       "      <td>0.18734</td>\n",
       "      <td>0.02082</td>\n",
       "      <td>0.72723</td>\n",
       "      <td>1.58478</td>\n",
       "      <td>0.40223</td>\n",
       "      <td>0.43712</td>\n",
       "      <td>4.03261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>0.35549</td>\n",
       "      <td>0.08493</td>\n",
       "      <td>0.03541</td>\n",
       "      <td>0.49645</td>\n",
       "      <td>1.37095</td>\n",
       "      <td>0.50479</td>\n",
       "      <td>2.20892</td>\n",
       "      <td>4.27759</td>\n",
       "      <td>11.23747</td>\n",
       "      <td>4.50273</td>\n",
       "      <td>0.16811</td>\n",
       "      <td>0.01327</td>\n",
       "      <td>0.11545</td>\n",
       "      <td>0.00453</td>\n",
       "      <td>2.72574</td>\n",
       "      <td>0.62829</td>\n",
       "      <td>1.64001</td>\n",
       "      <td>0.05581</td>\n",
       "      <td>14.53408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.08698</td>\n",
       "      <td>1.44135</td>\n",
       "      <td>0.08199</td>\n",
       "      <td>2.28560</td>\n",
       "      <td>0.81060</td>\n",
       "      <td>2.57431</td>\n",
       "      <td>0.86784</td>\n",
       "      <td>2.96214</td>\n",
       "      <td>15.02368</td>\n",
       "      <td>2.36794</td>\n",
       "      <td>0.18042</td>\n",
       "      <td>0.00607</td>\n",
       "      <td>0.30132</td>\n",
       "      <td>0.57997</td>\n",
       "      <td>0.35730</td>\n",
       "      <td>1.49679</td>\n",
       "      <td>1.11217</td>\n",
       "      <td>0.23667</td>\n",
       "      <td>16.59995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.07703</td>\n",
       "      <td>4.37042</td>\n",
       "      <td>0.02719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.66130</td>\n",
       "      <td>2.65407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.36600</td>\n",
       "      <td>16.24611</td>\n",
       "      <td>0.41096</td>\n",
       "      <td>0.01318</td>\n",
       "      <td>0.17176</td>\n",
       "      <td>0.87238</td>\n",
       "      <td>0.02532</td>\n",
       "      <td>0.10130</td>\n",
       "      <td>2.46159</td>\n",
       "      <td>9.67803</td>\n",
       "      <td>0.04572</td>\n",
       "      <td>3.43578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>0.00172</td>\n",
       "      <td>0.15917</td>\n",
       "      <td>0.04739</td>\n",
       "      <td>1.04552</td>\n",
       "      <td>0.72218</td>\n",
       "      <td>2.65623</td>\n",
       "      <td>0.08633</td>\n",
       "      <td>0.72477</td>\n",
       "      <td>0.52252</td>\n",
       "      <td>4.42403</td>\n",
       "      <td>0.02257</td>\n",
       "      <td>0.01219</td>\n",
       "      <td>0.14477</td>\n",
       "      <td>0.05535</td>\n",
       "      <td>0.25596</td>\n",
       "      <td>1.82677</td>\n",
       "      <td>4.80256</td>\n",
       "      <td>0.04505</td>\n",
       "      <td>21.51340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>0.04609</td>\n",
       "      <td>8.42434</td>\n",
       "      <td>0.05426</td>\n",
       "      <td>0.15820</td>\n",
       "      <td>0.14677</td>\n",
       "      <td>0.07892</td>\n",
       "      <td>0.11753</td>\n",
       "      <td>0.49263</td>\n",
       "      <td>1.39586</td>\n",
       "      <td>4.98916</td>\n",
       "      <td>0.20482</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.54838</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67235</td>\n",
       "      <td>0.21902</td>\n",
       "      <td>0.07023</td>\n",
       "      <td>4.33134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0.06129</td>\n",
       "      <td>3.65217</td>\n",
       "      <td>0.01798</td>\n",
       "      <td>1.28508</td>\n",
       "      <td>0.76930</td>\n",
       "      <td>2.14840</td>\n",
       "      <td>0.99602</td>\n",
       "      <td>1.11058</td>\n",
       "      <td>18.15777</td>\n",
       "      <td>5.92058</td>\n",
       "      <td>0.03307</td>\n",
       "      <td>0.02176</td>\n",
       "      <td>0.11454</td>\n",
       "      <td>0.08893</td>\n",
       "      <td>0.59581</td>\n",
       "      <td>0.97051</td>\n",
       "      <td>0.72669</td>\n",
       "      <td>0.01245</td>\n",
       "      <td>10.45828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>0.08962</td>\n",
       "      <td>0.43735</td>\n",
       "      <td>0.14898</td>\n",
       "      <td>2.35033</td>\n",
       "      <td>0.97159</td>\n",
       "      <td>2.25592</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.37425</td>\n",
       "      <td>36.33365</td>\n",
       "      <td>4.50121</td>\n",
       "      <td>0.03147</td>\n",
       "      <td>0.31067</td>\n",
       "      <td>1.20580</td>\n",
       "      <td>0.13381</td>\n",
       "      <td>0.28491</td>\n",
       "      <td>2.11995</td>\n",
       "      <td>1.09618</td>\n",
       "      <td>0.07481</td>\n",
       "      <td>11.45324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.47233</td>\n",
       "      <td>0.87256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.63958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02674</td>\n",
       "      <td>5.67772</td>\n",
       "      <td>10.70089</td>\n",
       "      <td>0.45223</td>\n",
       "      <td>2.17698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.18813</td>\n",
       "      <td>0.59429</td>\n",
       "      <td>4.38237</td>\n",
       "      <td>1.29294</td>\n",
       "      <td>2.84454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>0.05739</td>\n",
       "      <td>0.43778</td>\n",
       "      <td>0.05538</td>\n",
       "      <td>1.17485</td>\n",
       "      <td>0.65782</td>\n",
       "      <td>0.62809</td>\n",
       "      <td>0.97295</td>\n",
       "      <td>0.33408</td>\n",
       "      <td>0.01420</td>\n",
       "      <td>4.19182</td>\n",
       "      <td>0.24957</td>\n",
       "      <td>0.03277</td>\n",
       "      <td>0.12110</td>\n",
       "      <td>0.29622</td>\n",
       "      <td>0.00662</td>\n",
       "      <td>3.38619</td>\n",
       "      <td>0.39497</td>\n",
       "      <td>3.50237</td>\n",
       "      <td>13.62461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02567</td>\n",
       "      <td>0.03532</td>\n",
       "      <td>1.65594</td>\n",
       "      <td>0.50407</td>\n",
       "      <td>1.26006</td>\n",
       "      <td>0.26099</td>\n",
       "      <td>1.00464</td>\n",
       "      <td>5.16494</td>\n",
       "      <td>15.13262</td>\n",
       "      <td>0.00460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01749</td>\n",
       "      <td>0.12936</td>\n",
       "      <td>0.18708</td>\n",
       "      <td>0.57961</td>\n",
       "      <td>0.50270</td>\n",
       "      <td>0.28730</td>\n",
       "      <td>1.61595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>0.15475</td>\n",
       "      <td>8.44389</td>\n",
       "      <td>0.01503</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>0.53668</td>\n",
       "      <td>1.19112</td>\n",
       "      <td>3.38522</td>\n",
       "      <td>5.08439</td>\n",
       "      <td>6.16411</td>\n",
       "      <td>4.32149</td>\n",
       "      <td>0.83219</td>\n",
       "      <td>0.00962</td>\n",
       "      <td>0.30794</td>\n",
       "      <td>3.69777</td>\n",
       "      <td>0.09750</td>\n",
       "      <td>2.23211</td>\n",
       "      <td>3.89286</td>\n",
       "      <td>0.14833</td>\n",
       "      <td>25.01708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>0.01459</td>\n",
       "      <td>0.00401</td>\n",
       "      <td>0.02346</td>\n",
       "      <td>0.23093</td>\n",
       "      <td>0.47328</td>\n",
       "      <td>0.32246</td>\n",
       "      <td>2.69078</td>\n",
       "      <td>1.55914</td>\n",
       "      <td>4.19636</td>\n",
       "      <td>3.50924</td>\n",
       "      <td>0.03163</td>\n",
       "      <td>0.01075</td>\n",
       "      <td>0.15572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.40091</td>\n",
       "      <td>1.57246</td>\n",
       "      <td>2.16862</td>\n",
       "      <td>0.65240</td>\n",
       "      <td>4.87138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.01937</td>\n",
       "      <td>5.80125</td>\n",
       "      <td>0.07579</td>\n",
       "      <td>0.61823</td>\n",
       "      <td>0.24043</td>\n",
       "      <td>0.94849</td>\n",
       "      <td>4.01693</td>\n",
       "      <td>0.25719</td>\n",
       "      <td>6.68537</td>\n",
       "      <td>7.69273</td>\n",
       "      <td>0.23592</td>\n",
       "      <td>0.03020</td>\n",
       "      <td>0.14550</td>\n",
       "      <td>0.83280</td>\n",
       "      <td>1.06370</td>\n",
       "      <td>0.67356</td>\n",
       "      <td>0.50923</td>\n",
       "      <td>0.30366</td>\n",
       "      <td>16.02550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>0.09004</td>\n",
       "      <td>0.59947</td>\n",
       "      <td>0.02680</td>\n",
       "      <td>1.76369</td>\n",
       "      <td>0.29094</td>\n",
       "      <td>1.25737</td>\n",
       "      <td>2.71761</td>\n",
       "      <td>1.93078</td>\n",
       "      <td>15.56965</td>\n",
       "      <td>4.57963</td>\n",
       "      <td>0.06636</td>\n",
       "      <td>0.01258</td>\n",
       "      <td>0.01085</td>\n",
       "      <td>2.44015</td>\n",
       "      <td>0.26985</td>\n",
       "      <td>2.06783</td>\n",
       "      <td>1.31885</td>\n",
       "      <td>10.29113</td>\n",
       "      <td>6.10536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>0.35699</td>\n",
       "      <td>4.92366</td>\n",
       "      <td>0.40253</td>\n",
       "      <td>1.20949</td>\n",
       "      <td>0.43120</td>\n",
       "      <td>1.11650</td>\n",
       "      <td>2.07081</td>\n",
       "      <td>0.77863</td>\n",
       "      <td>10.44293</td>\n",
       "      <td>22.68659</td>\n",
       "      <td>0.04099</td>\n",
       "      <td>0.00742</td>\n",
       "      <td>0.46007</td>\n",
       "      <td>0.37742</td>\n",
       "      <td>0.47852</td>\n",
       "      <td>1.37475</td>\n",
       "      <td>1.27801</td>\n",
       "      <td>0.31825</td>\n",
       "      <td>3.65319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>0.04890</td>\n",
       "      <td>12.86899</td>\n",
       "      <td>0.14357</td>\n",
       "      <td>0.37339</td>\n",
       "      <td>1.07726</td>\n",
       "      <td>1.51404</td>\n",
       "      <td>2.72988</td>\n",
       "      <td>1.42560</td>\n",
       "      <td>8.47075</td>\n",
       "      <td>4.84051</td>\n",
       "      <td>0.05898</td>\n",
       "      <td>0.11941</td>\n",
       "      <td>0.24535</td>\n",
       "      <td>0.16572</td>\n",
       "      <td>4.37312</td>\n",
       "      <td>0.65899</td>\n",
       "      <td>1.55107</td>\n",
       "      <td>0.03805</td>\n",
       "      <td>4.97474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>0.03919</td>\n",
       "      <td>1.19160</td>\n",
       "      <td>0.06284</td>\n",
       "      <td>0.40511</td>\n",
       "      <td>0.16577</td>\n",
       "      <td>0.43625</td>\n",
       "      <td>0.49479</td>\n",
       "      <td>1.14986</td>\n",
       "      <td>26.99858</td>\n",
       "      <td>10.99878</td>\n",
       "      <td>0.01134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.08375</td>\n",
       "      <td>0.09414</td>\n",
       "      <td>4.49382</td>\n",
       "      <td>0.47883</td>\n",
       "      <td>0.78991</td>\n",
       "      <td>0.73725</td>\n",
       "      <td>5.19320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>0.63747</td>\n",
       "      <td>2.22407</td>\n",
       "      <td>0.00896</td>\n",
       "      <td>0.43741</td>\n",
       "      <td>0.75627</td>\n",
       "      <td>3.93194</td>\n",
       "      <td>2.94400</td>\n",
       "      <td>2.65654</td>\n",
       "      <td>1.34007</td>\n",
       "      <td>11.82353</td>\n",
       "      <td>0.19012</td>\n",
       "      <td>0.02958</td>\n",
       "      <td>0.09458</td>\n",
       "      <td>2.42953</td>\n",
       "      <td>1.67899</td>\n",
       "      <td>1.03900</td>\n",
       "      <td>1.55533</td>\n",
       "      <td>0.53963</td>\n",
       "      <td>33.52076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0.03583</td>\n",
       "      <td>0.37735</td>\n",
       "      <td>2.83233</td>\n",
       "      <td>0.53483</td>\n",
       "      <td>1.90670</td>\n",
       "      <td>2.52871</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>1.62854</td>\n",
       "      <td>0.01647</td>\n",
       "      <td>0.33365</td>\n",
       "      <td>0.06150</td>\n",
       "      <td>0.01449</td>\n",
       "      <td>0.11803</td>\n",
       "      <td>0.00511</td>\n",
       "      <td>0.02863</td>\n",
       "      <td>0.58703</td>\n",
       "      <td>1.86421</td>\n",
       "      <td>7.26994</td>\n",
       "      <td>3.08273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>0.13262</td>\n",
       "      <td>1.17988</td>\n",
       "      <td>0.10405</td>\n",
       "      <td>1.97824</td>\n",
       "      <td>1.13016</td>\n",
       "      <td>0.69249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.54224</td>\n",
       "      <td>8.94439</td>\n",
       "      <td>1.41328</td>\n",
       "      <td>0.01424</td>\n",
       "      <td>0.03562</td>\n",
       "      <td>0.21590</td>\n",
       "      <td>0.38432</td>\n",
       "      <td>0.14875</td>\n",
       "      <td>3.36625</td>\n",
       "      <td>3.37624</td>\n",
       "      <td>0.06290</td>\n",
       "      <td>20.08717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>NaN</td>\n",
       "      <td>12.82835</td>\n",
       "      <td>0.29127</td>\n",
       "      <td>0.15538</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.55252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.48736</td>\n",
       "      <td>5.68074</td>\n",
       "      <td>3.17288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02280</td>\n",
       "      <td>1.41932</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.28081</td>\n",
       "      <td>1.61811</td>\n",
       "      <td>0.09149</td>\n",
       "      <td>7.01696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>0.46545</td>\n",
       "      <td>0.71132</td>\n",
       "      <td>0.04908</td>\n",
       "      <td>0.61554</td>\n",
       "      <td>0.26392</td>\n",
       "      <td>0.81657</td>\n",
       "      <td>2.06771</td>\n",
       "      <td>2.04911</td>\n",
       "      <td>2.56256</td>\n",
       "      <td>2.77016</td>\n",
       "      <td>0.13958</td>\n",
       "      <td>0.01597</td>\n",
       "      <td>0.29486</td>\n",
       "      <td>0.16630</td>\n",
       "      <td>0.02115</td>\n",
       "      <td>1.22595</td>\n",
       "      <td>1.74577</td>\n",
       "      <td>0.18296</td>\n",
       "      <td>28.73081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>0.01562</td>\n",
       "      <td>0.48811</td>\n",
       "      <td>0.03495</td>\n",
       "      <td>1.75698</td>\n",
       "      <td>0.42159</td>\n",
       "      <td>1.21486</td>\n",
       "      <td>1.73595</td>\n",
       "      <td>1.14603</td>\n",
       "      <td>14.42328</td>\n",
       "      <td>9.94085</td>\n",
       "      <td>0.02185</td>\n",
       "      <td>0.02559</td>\n",
       "      <td>0.26322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.34612</td>\n",
       "      <td>1.23505</td>\n",
       "      <td>0.96376</td>\n",
       "      <td>0.34919</td>\n",
       "      <td>16.62813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>0.01144</td>\n",
       "      <td>4.76577</td>\n",
       "      <td>0.03609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.22909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.40252</td>\n",
       "      <td>5.48350</td>\n",
       "      <td>0.00680</td>\n",
       "      <td>10.91227</td>\n",
       "      <td>0.04834</td>\n",
       "      <td>0.10115</td>\n",
       "      <td>1.12672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01152</td>\n",
       "      <td>4.95653</td>\n",
       "      <td>3.36542</td>\n",
       "      <td>0.19886</td>\n",
       "      <td>3.95967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>0.02380</td>\n",
       "      <td>0.26820</td>\n",
       "      <td>0.03583</td>\n",
       "      <td>2.44307</td>\n",
       "      <td>1.44195</td>\n",
       "      <td>4.17322</td>\n",
       "      <td>2.87235</td>\n",
       "      <td>2.49513</td>\n",
       "      <td>11.74536</td>\n",
       "      <td>4.32097</td>\n",
       "      <td>0.02665</td>\n",
       "      <td>0.01190</td>\n",
       "      <td>0.02368</td>\n",
       "      <td>0.10148</td>\n",
       "      <td>0.13649</td>\n",
       "      <td>0.53538</td>\n",
       "      <td>0.47335</td>\n",
       "      <td>3.94181</td>\n",
       "      <td>11.95528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>0.01053</td>\n",
       "      <td>5.10908</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.83309</td>\n",
       "      <td>0.46398</td>\n",
       "      <td>0.50210</td>\n",
       "      <td>1.21030</td>\n",
       "      <td>1.26572</td>\n",
       "      <td>7.05694</td>\n",
       "      <td>3.25797</td>\n",
       "      <td>0.35984</td>\n",
       "      <td>0.00512</td>\n",
       "      <td>0.02447</td>\n",
       "      <td>2.63669</td>\n",
       "      <td>0.04651</td>\n",
       "      <td>0.22349</td>\n",
       "      <td>0.45539</td>\n",
       "      <td>0.02902</td>\n",
       "      <td>4.65335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>0.84675</td>\n",
       "      <td>1.34073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.61071</td>\n",
       "      <td>0.97467</td>\n",
       "      <td>1.09926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.84026</td>\n",
       "      <td>9.39063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.73216</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.48609</td>\n",
       "      <td>2.18579</td>\n",
       "      <td>13.61501</td>\n",
       "      <td>0.54348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>0.00359</td>\n",
       "      <td>6.31974</td>\n",
       "      <td>0.02361</td>\n",
       "      <td>0.45986</td>\n",
       "      <td>0.28121</td>\n",
       "      <td>0.02372</td>\n",
       "      <td>0.58563</td>\n",
       "      <td>0.74524</td>\n",
       "      <td>13.48167</td>\n",
       "      <td>3.30766</td>\n",
       "      <td>0.06387</td>\n",
       "      <td>0.04312</td>\n",
       "      <td>0.18389</td>\n",
       "      <td>0.78298</td>\n",
       "      <td>0.02916</td>\n",
       "      <td>0.30938</td>\n",
       "      <td>1.66913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.82745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>0.01700</td>\n",
       "      <td>3.36213</td>\n",
       "      <td>0.01137</td>\n",
       "      <td>0.58911</td>\n",
       "      <td>0.30296</td>\n",
       "      <td>0.40801</td>\n",
       "      <td>0.00021</td>\n",
       "      <td>0.15581</td>\n",
       "      <td>0.19521</td>\n",
       "      <td>4.65349</td>\n",
       "      <td>0.01702</td>\n",
       "      <td>0.01691</td>\n",
       "      <td>0.16671</td>\n",
       "      <td>0.09885</td>\n",
       "      <td>0.63858</td>\n",
       "      <td>0.20643</td>\n",
       "      <td>0.99758</td>\n",
       "      <td>0.04181</td>\n",
       "      <td>4.30869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>0.06224</td>\n",
       "      <td>3.50819</td>\n",
       "      <td>0.20594</td>\n",
       "      <td>1.14546</td>\n",
       "      <td>0.67621</td>\n",
       "      <td>0.34780</td>\n",
       "      <td>0.00940</td>\n",
       "      <td>3.40473</td>\n",
       "      <td>10.05673</td>\n",
       "      <td>6.67387</td>\n",
       "      <td>0.46322</td>\n",
       "      <td>0.04591</td>\n",
       "      <td>0.88254</td>\n",
       "      <td>0.08021</td>\n",
       "      <td>0.43676</td>\n",
       "      <td>1.83387</td>\n",
       "      <td>5.91174</td>\n",
       "      <td>0.14928</td>\n",
       "      <td>6.57568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>0.01688</td>\n",
       "      <td>1.55130</td>\n",
       "      <td>0.03437</td>\n",
       "      <td>2.23532</td>\n",
       "      <td>0.68314</td>\n",
       "      <td>3.38963</td>\n",
       "      <td>0.20599</td>\n",
       "      <td>4.82273</td>\n",
       "      <td>10.26876</td>\n",
       "      <td>3.28365</td>\n",
       "      <td>0.42494</td>\n",
       "      <td>0.01901</td>\n",
       "      <td>0.14556</td>\n",
       "      <td>0.72336</td>\n",
       "      <td>0.40821</td>\n",
       "      <td>2.48063</td>\n",
       "      <td>1.43761</td>\n",
       "      <td>1.16010</td>\n",
       "      <td>5.36634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>0.01213</td>\n",
       "      <td>0.51919</td>\n",
       "      <td>0.01708</td>\n",
       "      <td>0.55632</td>\n",
       "      <td>0.86241</td>\n",
       "      <td>0.16735</td>\n",
       "      <td>0.63342</td>\n",
       "      <td>3.28359</td>\n",
       "      <td>22.36562</td>\n",
       "      <td>5.21590</td>\n",
       "      <td>0.02148</td>\n",
       "      <td>0.04421</td>\n",
       "      <td>0.22354</td>\n",
       "      <td>0.31141</td>\n",
       "      <td>0.00645</td>\n",
       "      <td>2.31461</td>\n",
       "      <td>4.21667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.01843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>0.04555</td>\n",
       "      <td>1.52632</td>\n",
       "      <td>0.01762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.90976</td>\n",
       "      <td>2.43892</td>\n",
       "      <td>5.14338</td>\n",
       "      <td>2.89245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.20060</td>\n",
       "      <td>0.04156</td>\n",
       "      <td>0.03885</td>\n",
       "      <td>0.23893</td>\n",
       "      <td>0.47573</td>\n",
       "      <td>0.29036</td>\n",
       "      <td>1.22212</td>\n",
       "      <td>1.26240</td>\n",
       "      <td>7.64148</td>\n",
       "      <td>13.89061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>0.09126</td>\n",
       "      <td>2.36342</td>\n",
       "      <td>0.02932</td>\n",
       "      <td>1.28123</td>\n",
       "      <td>0.33447</td>\n",
       "      <td>2.12795</td>\n",
       "      <td>2.23254</td>\n",
       "      <td>1.03966</td>\n",
       "      <td>14.67014</td>\n",
       "      <td>21.97238</td>\n",
       "      <td>0.04344</td>\n",
       "      <td>0.15905</td>\n",
       "      <td>0.16164</td>\n",
       "      <td>0.22130</td>\n",
       "      <td>0.71209</td>\n",
       "      <td>0.65649</td>\n",
       "      <td>0.68682</td>\n",
       "      <td>0.30638</td>\n",
       "      <td>4.93792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>0.01408</td>\n",
       "      <td>4.19662</td>\n",
       "      <td>0.10975</td>\n",
       "      <td>2.11015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.69445</td>\n",
       "      <td>13.62391</td>\n",
       "      <td>0.09351</td>\n",
       "      <td>3.22074</td>\n",
       "      <td>1.43591</td>\n",
       "      <td>0.34745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.47881</td>\n",
       "      <td>0.00331</td>\n",
       "      <td>1.63606</td>\n",
       "      <td>2.93770</td>\n",
       "      <td>0.04890</td>\n",
       "      <td>2.76216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>0.02705</td>\n",
       "      <td>1.24907</td>\n",
       "      <td>2.86111</td>\n",
       "      <td>0.92703</td>\n",
       "      <td>0.66706</td>\n",
       "      <td>2.39532</td>\n",
       "      <td>1.38829</td>\n",
       "      <td>1.08583</td>\n",
       "      <td>0.46673</td>\n",
       "      <td>10.95953</td>\n",
       "      <td>0.03546</td>\n",
       "      <td>0.00595</td>\n",
       "      <td>0.14353</td>\n",
       "      <td>0.31936</td>\n",
       "      <td>2.15708</td>\n",
       "      <td>1.11921</td>\n",
       "      <td>0.78324</td>\n",
       "      <td>0.31525</td>\n",
       "      <td>13.91351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>0.01607</td>\n",
       "      <td>2.77967</td>\n",
       "      <td>0.29588</td>\n",
       "      <td>2.14739</td>\n",
       "      <td>0.77604</td>\n",
       "      <td>0.09161</td>\n",
       "      <td>0.14588</td>\n",
       "      <td>5.64159</td>\n",
       "      <td>17.49839</td>\n",
       "      <td>1.93098</td>\n",
       "      <td>0.79063</td>\n",
       "      <td>0.11130</td>\n",
       "      <td>0.73607</td>\n",
       "      <td>2.81358</td>\n",
       "      <td>0.92708</td>\n",
       "      <td>1.36068</td>\n",
       "      <td>3.39580</td>\n",
       "      <td>0.00346</td>\n",
       "      <td>4.43547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>0.24297</td>\n",
       "      <td>1.71018</td>\n",
       "      <td>0.23087</td>\n",
       "      <td>1.46301</td>\n",
       "      <td>1.09646</td>\n",
       "      <td>9.46870</td>\n",
       "      <td>0.20581</td>\n",
       "      <td>3.39290</td>\n",
       "      <td>12.63603</td>\n",
       "      <td>8.59403</td>\n",
       "      <td>0.20624</td>\n",
       "      <td>0.00114</td>\n",
       "      <td>0.04914</td>\n",
       "      <td>0.52265</td>\n",
       "      <td>0.31644</td>\n",
       "      <td>5.83931</td>\n",
       "      <td>1.97836</td>\n",
       "      <td>0.97412</td>\n",
       "      <td>7.78451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         95        151      227      255      281       282      328  \\\n",
       "868  0.21665   0.88219  0.04984  1.42515  0.50316   1.36946  0.81417   \n",
       "736  0.13166       NaN  0.53613  0.87813  0.35152   0.97828  0.12497   \n",
       "869  0.02171   1.61407  0.62370  1.02824  0.25232   1.80161  0.42362   \n",
       "864  0.05876   6.25162  0.02937  0.08661  0.15397   0.02480  0.42012   \n",
       "756  0.06799   0.97867  0.16832      NaN  1.01265   4.24549  0.03225   \n",
       "737  0.02062   1.69398  0.01813      NaN  1.08985   1.06756  1.13181   \n",
       "797  0.03385   0.85393  0.00860  0.23050  0.36963   0.42680  4.11528   \n",
       "746  0.01147   2.49746  0.02023  1.13841  0.73311   1.56417  0.26545   \n",
       "790  0.08797   5.51194  0.05585  0.59987  1.32968   0.90234  5.09951   \n",
       "863  0.09491   1.31921  0.01431  2.01048  0.50664   0.87318  8.17920   \n",
       "871  0.41992   0.88538  0.01799  1.74525  0.94244   5.34755  1.61558   \n",
       "802  0.00113   1.35206  0.04591  0.34025  0.33539   0.45868  0.62443   \n",
       "855  0.40796       NaN  0.23866  1.17559  1.76807   0.17887  0.01133   \n",
       "778  0.67405   0.37177  0.10587  0.16205  3.59076  12.25760  0.01248   \n",
       "820  0.07001   0.32965  0.01033  2.81866  0.18569   4.94361  2.78965   \n",
       "744  0.02889   3.50863  0.02749      NaN  1.22258   0.00099  2.73331   \n",
       "838  0.00164   0.75942  0.13510  0.97619  0.65942   2.71784  0.20504   \n",
       "818  0.01724       NaN  0.02475      NaN  0.25375   0.09839  1.20841   \n",
       "835  0.05733   1.21480  0.08082  0.83988  1.01109   3.81467  0.55319   \n",
       "753  0.04595   0.94857      NaN  3.01870  0.67295   2.22099  1.03221   \n",
       "710  0.01831   1.93053  0.36806  0.80604  0.44843   1.03499  4.69355   \n",
       "874  0.13614   0.20685  0.01564  0.06968  0.79499   0.47522  1.30937   \n",
       "873  0.35549   0.08493  0.03541  0.49645  1.37095   0.50479  2.20892   \n",
       "800  0.08698   1.44135  0.08199  2.28560  0.81060   2.57431  0.86784   \n",
       "763  0.07703   4.37042  0.02719      NaN  0.66130   2.65407      NaN   \n",
       "865  0.00172   0.15917  0.04739  1.04552  0.72218   2.65623  0.08633   \n",
       "727  0.04609   8.42434  0.05426  0.15820  0.14677   0.07892  0.11753   \n",
       "860  0.06129   3.65217  0.01798  1.28508  0.76930   2.14840  0.99602   \n",
       "788  0.08962   0.43735  0.14898  2.35033  0.97159   2.25592      NaN   \n",
       "811      NaN   6.47233  0.87256      NaN  0.63958       NaN  0.02674   \n",
       "..       ...       ...      ...      ...      ...       ...      ...   \n",
       "741  0.05739   0.43778  0.05538  1.17485  0.65782   0.62809  0.97295   \n",
       "789      NaN   0.02567  0.03532  1.65594  0.50407   1.26006  0.26099   \n",
       "760  0.15475   8.44389  0.01503  0.99512  0.53668   1.19112  3.38522   \n",
       "711  0.01459   0.00401  0.02346  0.23093  0.47328   0.32246  2.69078   \n",
       "720  0.01937   5.80125  0.07579  0.61823  0.24043   0.94849  4.01693   \n",
       "813  0.09004   0.59947  0.02680  1.76369  0.29094   1.25737  2.71761   \n",
       "872  0.35699   4.92366  0.40253  1.20949  0.43120   1.11650  2.07081   \n",
       "795  0.04890  12.86899  0.14357  0.37339  1.07726   1.51404  2.72988   \n",
       "719  0.03919   1.19160  0.06284  0.40511  0.16577   0.43625  0.49479   \n",
       "745  0.63747   2.22407  0.00896  0.43741  0.75627   3.93194  2.94400   \n",
       "767  0.03583   0.37735  2.83233  0.53483  1.90670   2.52871  0.00064   \n",
       "750  0.13262   1.17988  0.10405  1.97824  1.13016   0.69249      NaN   \n",
       "723      NaN  12.82835  0.29127  0.15538      NaN   0.55252      NaN   \n",
       "759  0.46545   0.71132  0.04908  0.61554  0.26392   0.81657  2.06771   \n",
       "857  0.01562   0.48811  0.03495  1.75698  0.42159   1.21486  1.73595   \n",
       "791  0.01144   4.76577  0.03609      NaN  1.22909       NaN  2.40252   \n",
       "832  0.02380   0.26820  0.03583  2.44307  1.44195   4.17322  2.87235   \n",
       "771  0.01053   5.10908      NaN  0.83309  0.46398   0.50210  1.21030   \n",
       "807  0.84675   1.34073      NaN  0.61071  0.97467   1.09926      NaN   \n",
       "830  0.00359   6.31974  0.02361  0.45986  0.28121   0.02372  0.58563   \n",
       "839  0.01700   3.36213  0.01137  0.58911  0.30296   0.40801  0.00021   \n",
       "732  0.06224   3.50819  0.20594  1.14546  0.67621   0.34780  0.00940   \n",
       "876  0.01688   1.55130  0.03437  2.23532  0.68314   3.38963  0.20599   \n",
       "752  0.01213   0.51919  0.01708  0.55632  0.86241   0.16735  0.63342   \n",
       "823  0.04555   1.52632  0.01762      NaN  0.90976   2.43892  5.14338   \n",
       "847  0.09126   2.36342  0.02932  1.28123  0.33447   2.12795  2.23254   \n",
       "709  0.01408   4.19662  0.10975  2.11015      NaN       NaN  3.69445   \n",
       "836  0.02705   1.24907  2.86111  0.92703  0.66706   2.39532  1.38829   \n",
       "858  0.01607   2.77967  0.29588  2.14739  0.77604   0.09161  0.14588   \n",
       "821  0.24297   1.71018  0.23087  1.46301  1.09646   9.46870  0.20581   \n",
       "\n",
       "          329       333       341      408      410      512      609  \\\n",
       "868   0.77809   3.68838   3.34931  0.04667  0.01206  0.01894  0.40095   \n",
       "736   2.05840  24.05715   8.29215  0.02381  0.02490  0.33430  0.17264   \n",
       "869   2.34287  22.86243   5.64347  0.46603      NaN  0.06362  0.02620   \n",
       "864   1.69665   0.04387   0.62999  0.30132  0.09270  0.01189  0.04555   \n",
       "756   2.50411  27.12543   7.57322  0.32966  0.00982  0.21648  0.05139   \n",
       "737   0.29836  21.69010   7.57267  0.04062  0.04818  0.16694  0.36983   \n",
       "797   0.67918   0.63434   7.06480  0.02577  0.02745  0.14168  0.70556   \n",
       "746   0.17630   7.21582   8.95995  0.48772  0.04295  0.12646  0.36928   \n",
       "790   4.89462   5.14207   2.74218  0.18407  0.01817  0.16725  0.30772   \n",
       "863   2.23231   0.00046  13.19897  0.73339  0.01593  0.04562  0.41027   \n",
       "871   1.78076   0.35707   4.41223  0.05842  0.00992  0.02183  2.16896   \n",
       "802   1.26417  12.05048   3.28502  0.05837  0.01788  0.06615  1.33412   \n",
       "855   5.32448  33.60701   7.12865  0.32424  0.02416  0.09210  1.18006   \n",
       "778   0.72428   1.10377   2.09027  1.15650  0.03697  0.11980  0.10955   \n",
       "820   1.52370  25.50913   5.39667  0.30388  0.00290  0.00581  0.35655   \n",
       "744   2.23906  20.84720  13.50719  0.19307  0.11313  0.55639  0.04670   \n",
       "838   1.91330  27.26684   3.48436  0.35362  0.00452  0.54344  0.00676   \n",
       "818   1.66670   6.67624   4.35269  0.03119  0.01703  0.33269  0.03250   \n",
       "835   1.64202  14.69790  18.69431  0.06907  0.00858  0.38908  0.02894   \n",
       "753   5.08137   1.40578   4.80469  0.07142  0.12582  0.34505  0.02583   \n",
       "710   1.68496  16.68297  13.97481  0.07460  0.03235  0.37690  0.70773   \n",
       "874   2.60831  19.56838   4.72219  0.03778  0.01410  0.18734  0.02082   \n",
       "873   4.27759  11.23747   4.50273  0.16811  0.01327  0.11545  0.00453   \n",
       "800   2.96214  15.02368   2.36794  0.18042  0.00607  0.30132  0.57997   \n",
       "763   3.36600  16.24611   0.41096  0.01318  0.17176  0.87238  0.02532   \n",
       "865   0.72477   0.52252   4.42403  0.02257  0.01219  0.14477  0.05535   \n",
       "727   0.49263   1.39586   4.98916  0.20482      NaN  1.54838      NaN   \n",
       "860   1.11058  18.15777   5.92058  0.03307  0.02176  0.11454  0.08893   \n",
       "788   2.37425  36.33365   4.50121  0.03147  0.31067  1.20580  0.13381   \n",
       "811   5.67772  10.70089   0.45223  2.17698      NaN  0.00490      NaN   \n",
       "..        ...       ...       ...      ...      ...      ...      ...   \n",
       "741   0.33408   0.01420   4.19182  0.24957  0.03277  0.12110  0.29622   \n",
       "789   1.00464   5.16494  15.13262  0.00460      NaN  0.01749  0.12936   \n",
       "760   5.08439   6.16411   4.32149  0.83219  0.00962  0.30794  3.69777   \n",
       "711   1.55914   4.19636   3.50924  0.03163  0.01075  0.15572      NaN   \n",
       "720   0.25719   6.68537   7.69273  0.23592  0.03020  0.14550  0.83280   \n",
       "813   1.93078  15.56965   4.57963  0.06636  0.01258  0.01085  2.44015   \n",
       "872   0.77863  10.44293  22.68659  0.04099  0.00742  0.46007  0.37742   \n",
       "795   1.42560   8.47075   4.84051  0.05898  0.11941  0.24535  0.16572   \n",
       "719   1.14986  26.99858  10.99878  0.01134      NaN  0.08375  0.09414   \n",
       "745   2.65654   1.34007  11.82353  0.19012  0.02958  0.09458  2.42953   \n",
       "767   1.62854   0.01647   0.33365  0.06150  0.01449  0.11803  0.00511   \n",
       "750   2.54224   8.94439   1.41328  0.01424  0.03562  0.21590  0.38432   \n",
       "723   4.48736   5.68074   3.17288      NaN  0.02280  1.41932      NaN   \n",
       "759   2.04911   2.56256   2.77016  0.13958  0.01597  0.29486  0.16630   \n",
       "857   1.14603  14.42328   9.94085  0.02185  0.02559  0.26322      NaN   \n",
       "791   5.48350   0.00680  10.91227  0.04834  0.10115  1.12672      NaN   \n",
       "832   2.49513  11.74536   4.32097  0.02665  0.01190  0.02368  0.10148   \n",
       "771   1.26572   7.05694   3.25797  0.35984  0.00512  0.02447  2.63669   \n",
       "807   0.84026   9.39063       NaN  4.73216      NaN      NaN      NaN   \n",
       "830   0.74524  13.48167   3.30766  0.06387  0.04312  0.18389  0.78298   \n",
       "839   0.15581   0.19521   4.65349  0.01702  0.01691  0.16671  0.09885   \n",
       "732   3.40473  10.05673   6.67387  0.46322  0.04591  0.88254  0.08021   \n",
       "876   4.82273  10.26876   3.28365  0.42494  0.01901  0.14556  0.72336   \n",
       "752   3.28359  22.36562   5.21590  0.02148  0.04421  0.22354  0.31141   \n",
       "823   2.89245       NaN   4.20060  0.04156  0.03885  0.23893  0.47573   \n",
       "847   1.03966  14.67014  21.97238  0.04344  0.15905  0.16164  0.22130   \n",
       "709  13.62391   0.09351   3.22074  1.43591  0.34745      NaN  0.47881   \n",
       "836   1.08583   0.46673  10.95953  0.03546  0.00595  0.14353  0.31936   \n",
       "858   5.64159  17.49839   1.93098  0.79063  0.11130  0.73607  2.81358   \n",
       "821   3.39290  12.63603   8.59403  0.20624  0.00114  0.04914  0.52265   \n",
       "\n",
       "         611      625       629       690       702  \n",
       "868  1.93523  0.99138   1.46274   0.31146   3.03461  \n",
       "736  0.80085  0.57797   0.53797   0.01671  14.48115  \n",
       "869  0.90634  0.92651   1.53389   0.15329   6.77172  \n",
       "864  0.00038  0.48827   1.23912   0.15705  18.24170  \n",
       "756  0.52041  0.99326   1.66178   1.59456   1.86928  \n",
       "737  1.50510  1.39289   1.17217   0.12309   8.45865  \n",
       "797  0.33778  0.90136   0.75658   0.03949  10.02541  \n",
       "746  0.17894  1.94130   0.83185   0.20342   7.85651  \n",
       "790  0.76981  1.08495   1.21542   0.09419   8.04926  \n",
       "863  3.16531  1.82837   1.02263   0.20157  22.97987  \n",
       "871  0.41462  0.70009   0.34637   0.22988   9.98995  \n",
       "802  0.10335  0.45685   0.48942   0.00417   7.89301  \n",
       "855  0.42058  0.55031   2.05115   0.20938   1.94367  \n",
       "778  0.20712  6.08305  13.13537   0.17801  16.98043  \n",
       "820  2.29981  1.93435   0.99718   0.14217   8.22111  \n",
       "744  1.25082  0.21616   1.59506   0.21298   3.02060  \n",
       "838  0.31096  2.22112   1.69986   0.05988   6.43034  \n",
       "818  0.07011  0.63671   1.86806   2.94334  15.12912  \n",
       "835  0.38801  1.33992   0.89026   0.02777  10.35255  \n",
       "753  4.17462  0.51764   4.86192   0.21369   0.23934  \n",
       "710  4.29459  1.58893   2.74500   2.93867   1.56245  \n",
       "874  0.72723  1.58478   0.40223   0.43712   4.03261  \n",
       "873  2.72574  0.62829   1.64001   0.05581  14.53408  \n",
       "800  0.35730  1.49679   1.11217   0.23667  16.59995  \n",
       "763  0.10130  2.46159   9.67803   0.04572   3.43578  \n",
       "865  0.25596  1.82677   4.80256   0.04505  21.51340  \n",
       "727      NaN  0.67235   0.21902   0.07023   4.33134  \n",
       "860  0.59581  0.97051   0.72669   0.01245  10.45828  \n",
       "788  0.28491  2.11995   1.09618   0.07481  11.45324  \n",
       "811  0.18813  0.59429   4.38237   1.29294   2.84454  \n",
       "..       ...      ...       ...       ...       ...  \n",
       "741  0.00662  3.38619   0.39497   3.50237  13.62461  \n",
       "789  0.18708  0.57961   0.50270   0.28730   1.61595  \n",
       "760  0.09750  2.23211   3.89286   0.14833  25.01708  \n",
       "711  0.40091  1.57246   2.16862   0.65240   4.87138  \n",
       "720  1.06370  0.67356   0.50923   0.30366  16.02550  \n",
       "813  0.26985  2.06783   1.31885  10.29113   6.10536  \n",
       "872  0.47852  1.37475   1.27801   0.31825   3.65319  \n",
       "795  4.37312  0.65899   1.55107   0.03805   4.97474  \n",
       "719  4.49382  0.47883   0.78991   0.73725   5.19320  \n",
       "745  1.67899  1.03900   1.55533   0.53963  33.52076  \n",
       "767  0.02863  0.58703   1.86421   7.26994   3.08273  \n",
       "750  0.14875  3.36625   3.37624   0.06290  20.08717  \n",
       "723      NaN  0.28081   1.61811   0.09149   7.01696  \n",
       "759  0.02115  1.22595   1.74577   0.18296  28.73081  \n",
       "857  0.34612  1.23505   0.96376   0.34919  16.62813  \n",
       "791  0.01152  4.95653   3.36542   0.19886   3.95967  \n",
       "832  0.13649  0.53538   0.47335   3.94181  11.95528  \n",
       "771  0.04651  0.22349   0.45539   0.02902   4.65335  \n",
       "807      NaN  0.48609   2.18579  13.61501   0.54348  \n",
       "830  0.02916  0.30938   1.66913       NaN   5.82745  \n",
       "839  0.63858  0.20643   0.99758   0.04181   4.30869  \n",
       "732  0.43676  1.83387   5.91174   0.14928   6.57568  \n",
       "876  0.40821  2.48063   1.43761   1.16010   5.36634  \n",
       "752  0.00645  2.31461   4.21667       NaN  18.01843  \n",
       "823  0.29036  1.22212   1.26240   7.64148  13.89061  \n",
       "847  0.71209  0.65649   0.68682   0.30638   4.93792  \n",
       "709  0.00331  1.63606   2.93770   0.04890   2.76216  \n",
       "836  2.15708  1.11921   0.78324   0.31525  13.91351  \n",
       "858  0.92708  1.36068   3.39580   0.00346   4.43547  \n",
       "821  0.31644  5.83931   1.97836   0.97412   7.78451  \n",
       "\n",
       "[113 rows x 19 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

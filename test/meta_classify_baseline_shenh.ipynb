{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meata machine_learning baseline:\n",
    "\n",
    "1.针对二分类(后续加入多分类和回归)\n",
    "\n",
    "2.包含逻辑回归，决策树，随机森林，xgboost模型\n",
    "\n",
    "3.可根据需求自动调整超参数\n",
    "\n",
    "待改进：\n",
    "1.加入特征工程\n",
    "2.加入多分类分析\n",
    "3.加入回归分析\n",
    "4.加入神经网络方法(tensorflow框架)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以肿瘤病人预测为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_table(\"./austria.crc.crc_control.input.trans.profile.new2\",header=None)\n",
    "data1 = data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_df=data1.loc[1:,3:].drop(3,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_df=data1.loc[1:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelencoder=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat=labelencoder.fit_transform(y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_df=pd.DataFrame(x_df,dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_df,y_hat,test_size=0.2,random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_model= LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_prob = lr_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.where(y_prob > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.score(X_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.40      0.50      0.44         8\n",
      "          1       0.67      0.57      0.62        14\n",
      "\n",
      "avg / total       0.57      0.55      0.55        22\n",
      "\n",
      "0.5357142857142857\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,y_pred))\n",
    "print(metrics.roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参树调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "lr_model2= LogisticRegression()\n",
    "\n",
    "tuned_parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000,10000] ,\n",
    "              'penalty':['l1','l2']\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "LR= GridSearchCV(lr_model2, tuned_parameters,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000], 'penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "print(LR.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob = LR.predict_proba(X_test)[:,1]\n",
    "y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "LR.score(X_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.62      0.67         8\n",
      "          1       0.80      0.86      0.83        14\n",
      "\n",
      "avg / total       0.77      0.77      0.77        22\n",
      "\n",
      "0.7410714285714286\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,y_pred))\n",
    "print(metrics.roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.62      0.67         8\n",
      "          1       0.80      0.86      0.83        14\n",
      "\n",
      "avg / total       0.77      0.77      0.77        22\n",
      "\n",
      "0.7410714285714286\n"
     ]
    }
   ],
   "source": [
    "y_prob = tree_model.predict_proba(X_test)[:,1] \n",
    "y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "tree_model.score(X_test, y_pred)\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "metrics.roc_auc_score(y_test,y_pred)\n",
    "print(metrics.roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree_model2 = DecisionTreeClassifier()\n",
    "tuned_parameters= { 'max_features': [\"auto\",\"sqrt\",\"log2\"],\n",
    "                  'min_samples_leaf': range(1,10,1) , 'max_depth': range(1,10,1)\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TM = GridSearchCV(tree_model2, tuned_parameters,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_features': ['auto', 'sqrt', 'log2'], 'min_samples_leaf': range(1, 10), 'max_depth': range(1, 10)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 3}\n"
     ]
    }
   ],
   "source": [
    "print(TM.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      1.00      0.80         8\n",
      "          1       1.00      0.71      0.83        14\n",
      "\n",
      "avg / total       0.88      0.82      0.82        22\n",
      "\n",
      "0.8571428571428572\n"
     ]
    }
   ],
   "source": [
    "y_prob = TM.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \n",
    "y_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\n",
    "TM.score(X_test, y_pred)\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "print(metrics.roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shizhiluo/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rr_model=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.75      0.75         8\n",
      "          1       0.86      0.86      0.86        14\n",
      "\n",
      "avg / total       0.82      0.82      0.82        22\n",
      "\n",
      "0.8035714285714285\n"
     ]
    }
   ],
   "source": [
    "y_prob = rr_model.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \n",
    "y_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\n",
    "rr_model.score(X_test, y_pred)\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "print(metrics.roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RR = GridSearchCV(model_RR, tuned_parameters,cv=10)\n",
    "tuned_parameters = {'min_samples_leaf': range(10,100,10), 'n_estimators' : range(10,100,10),\n",
    "                    'max_features':['auto','sqrt','log2']\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'min_samples_leaf': range(10, 100, 10), 'n_estimators': range(10, 100, 10), 'max_features': ['auto', 'sqrt', 'log2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RR.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.75      0.67         8\n",
      "          1       0.83      0.71      0.77        14\n",
      "\n",
      "avg / total       0.75      0.73      0.73        22\n",
      "\n",
      "0.7321428571428572\n"
     ]
    }
   ],
   "source": [
    "y_prob = RR.predict_proba(X_test)[:,1]\n",
    "y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "RR.score(X_test, y_pred)\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "print(metrics.roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn import preprocessing\n",
    "#lbl = preprocessing.LabelEncoder()\n",
    "#train_x['acc_id1'] = lbl.fit_transform(train_x['acc_id1'].astype(str))#将提示的包含错误数据类型这一列进行转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "XGB_model=XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.62      0.67         8\n",
      "          1       0.80      0.86      0.83        14\n",
      "\n",
      "avg / total       0.77      0.77      0.77        22\n",
      "\n",
      "0.7410714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shizhiluo/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_prob = XGB_model.predict_proba(X_test)[:,1] \n",
    "y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "XGB_model.score(X_test, y_pred)\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "print(metrics.roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.0285297  0.         0.         0.         0.         0.00524615\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01947274 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01237625\n",
      " 0.00628219 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00274954 0.         0.\n",
      " 0.         0.         0.         0.0084788  0.         0.\n",
      " 0.00349184 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.03759316\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00712248\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01789715 0.         0.0040819  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.03201618 0.\n",
      " 0.         0.         0.02760276 0.         0.         0.\n",
      " 0.         0.01436509 0.04356159 0.         0.         0.\n",
      " 0.         0.         0.00519551 0.         0.         0.00925304\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01011682 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00386776 0.         0.         0.         0.00275225 0.\n",
      " 0.         0.01118955 0.         0.         0.         0.\n",
      " 0.         0.00132549 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.03308791 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01591424 0.         0.00250684 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.02298482 0.         0.         0.         0.\n",
      " 0.         0.00151387 0.         0.         0.00219606 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01088598 0.\n",
      " 0.         0.         0.         0.         0.00916769 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.11687651 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00962295 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00873401 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.00704312 0.\n",
      " 0.         0.         0.02000446 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00769228 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01052914 0.         0.         0.         0.\n",
      " 0.         0.02566922 0.         0.00309377 0.         0.\n",
      " 0.         0.         0.         0.03273491 0.         0.00915776\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.02985715 0.         0.         0.         0.         0.00738761\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.0078403  0.\n",
      " 0.         0.         0.00809816 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.02217373 0.         0.         0.         0.         0.\n",
      " 0.01730037 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.02910753\n",
      " 0.         0.         0.         0.         0.         0.00123029\n",
      " 0.         0.         0.         0.         0.0027687  0.\n",
      " 0.         0.         0.         0.         0.11254164 0.\n",
      " 0.02216806 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00374902 0.         0.07179386\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(model_XGB.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKgk6rgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10abae7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "from matplotlib import pyplot\n",
    "pyplot.bar(range(len(XGB_model.feature_importances_)), XGB_model.feature_importances_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11fb18d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot feature importance using built-in function\n",
    "from xgboost import plot_importance\n",
    "plot_importance(XGB_model)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function plot_importance in module xgboost.plotting:\n",
      "\n",
      "plot_importance(booster, ax=None, height=0.2, xlim=None, ylim=None, title='Feature importance', xlabel='F score', ylabel='Features', importance_type='weight', max_num_features=None, grid=True, show_values=True, **kwargs)\n",
      "    Plot importance based on fitted trees.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    booster : Booster, XGBModel or dict\n",
      "        Booster or XGBModel instance, or dict taken by Booster.get_fscore()\n",
      "    ax : matplotlib Axes, default None\n",
      "        Target axes instance. If None, new figure and axes will be created.\n",
      "    grid : bool, Turn the axes grids on or off.  Default is True (On).\n",
      "    importance_type : str, default \"weight\"\n",
      "        How the importance is calculated: either \"weight\", \"gain\", or \"cover\"\n",
      "    \n",
      "        * \"weight\" is the number of times a feature appears in a tree\n",
      "        * \"gain\" is the average gain of splits which use the feature\n",
      "        * \"cover\" is the average coverage of splits which use the feature\n",
      "          where coverage is defined as the number of samples affected by the split\n",
      "    max_num_features : int, default None\n",
      "        Maximum number of top features displayed on plot. If None, all features will be displayed.\n",
      "    height : float, default 0.2\n",
      "        Bar height, passed to ax.barh()\n",
      "    xlim : tuple, default None\n",
      "        Tuple passed to axes.xlim()\n",
      "    ylim : tuple, default None\n",
      "        Tuple passed to axes.ylim()\n",
      "    title : str, default \"Feature importance\"\n",
      "        Axes title. To disable, pass None.\n",
      "    xlabel : str, default \"F score\"\n",
      "        X axis title label. To disable, pass None.\n",
      "    ylabel : str, default \"Features\"\n",
      "        Y axis title label. To disable, pass None.\n",
      "    show_values : bool, default True\n",
      "        Show values on plot. To disable, pass False.\n",
      "    kwargs :\n",
      "        Other keywords passed to ax.barh()\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    ax : matplotlib Axes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(plot_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>...</th>\n",
       "      <th>517</th>\n",
       "      <th>518</th>\n",
       "      <th>519</th>\n",
       "      <th>520</th>\n",
       "      <th>521</th>\n",
       "      <th>522</th>\n",
       "      <th>523</th>\n",
       "      <th>524</th>\n",
       "      <th>525</th>\n",
       "      <th>526</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00485</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.09095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00368</td>\n",
       "      <td>0.00341</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05959</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25715</td>\n",
       "      <td>19.33382</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.33503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07911</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00039</td>\n",
       "      <td>0.04888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.51496</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.08936</td>\n",
       "      <td>0.04593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 523 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   4    5    6        7        8        9        10   11   12   13  ...   \\\n",
       "1  0.0  0.0  0.0  0.00485  0.00000  0.00000  0.00000  0.0  0.0  0.0 ...    \n",
       "2  0.0  0.0  0.0  0.00368  0.00341  0.00000  0.00565  0.0  0.0  0.0 ...    \n",
       "3  0.0  0.0  0.0  0.05959  0.00000  0.00000  0.00025  0.0  0.0  0.0 ...    \n",
       "4  0.0  0.0  0.0  0.00000  0.00000  0.00000  0.01150  0.0  0.0  0.0 ...    \n",
       "5  0.0  0.0  0.0  0.07911  0.00000  0.00039  0.04888  0.0  0.0  0.0 ...    \n",
       "\n",
       "       517       518      519      520  521      522  523  524  525  526  \n",
       "1  0.00000   0.00000  0.00000  0.09095  0.0  0.00000  0.0  0.0  0.0  0.0  \n",
       "2  0.00000   0.00000  0.00000  0.00043  0.0  0.00000  0.0  0.0  0.0  0.0  \n",
       "3  0.00000   0.00000  0.00000  0.05034  0.0  0.00000  0.0  0.0  0.0  0.0  \n",
       "4  0.25715  19.33382  0.00000  0.33503  0.0  0.00000  0.0  0.0  0.0  0.0  \n",
       "5  0.51496   0.00000  0.08936  0.04593  0.0  0.00041  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 523 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df=pd.DataFrame(x_df,dtype=np.float)\n",
    "x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>510</th>\n",
       "      <th>512</th>\n",
       "      <th>513</th>\n",
       "      <th>514</th>\n",
       "      <th>517</th>\n",
       "      <th>518</th>\n",
       "      <th>519</th>\n",
       "      <th>520</th>\n",
       "      <th>521</th>\n",
       "      <th>522</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00485</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11937</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.09095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00368</td>\n",
       "      <td>0.00341</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00022</td>\n",
       "      <td>0.00384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01157</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05959</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01080</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.79082</td>\n",
       "      <td>0.25715</td>\n",
       "      <td>19.33382</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.33503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07911</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00039</td>\n",
       "      <td>0.04888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04274</td>\n",
       "      <td>0.03359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09195</td>\n",
       "      <td>0.51496</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.08936</td>\n",
       "      <td>0.04593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 306 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   4    5    6        7        8        9        10   11   13      14   \\\n",
       "1  0.0  0.0  0.0  0.00485  0.00000  0.00000  0.00000  0.0  0.0  0.0000   \n",
       "2  0.0  0.0  0.0  0.00368  0.00341  0.00000  0.00565  0.0  0.0  0.0000   \n",
       "3  0.0  0.0  0.0  0.05959  0.00000  0.00000  0.00025  0.0  0.0  0.0000   \n",
       "4  0.0  0.0  0.0  0.00000  0.00000  0.00000  0.01150  0.0  0.0  0.0000   \n",
       "5  0.0  0.0  0.0  0.07911  0.00000  0.00039  0.04888  0.0  0.0  0.0019   \n",
       "\n",
       "    ...         510      512  513      514      517       518      519  \\\n",
       "1   ...     0.00000  0.00000  0.0  0.11937  0.00000   0.00000  0.00000   \n",
       "2   ...     0.00022  0.00384  0.0  0.01157  0.00000   0.00000  0.00000   \n",
       "3   ...     0.00000  0.00000  0.0  0.01080  0.00000   0.00000  0.00000   \n",
       "4   ...     0.00000  0.00299  0.0  0.79082  0.25715  19.33382  0.00000   \n",
       "5   ...     0.04274  0.03359  0.0  0.09195  0.51496   0.00000  0.08936   \n",
       "\n",
       "       520  521      522  \n",
       "1  0.09095  0.0  0.00000  \n",
       "2  0.00043  0.0  0.00000  \n",
       "3  0.05034  0.0  0.00000  \n",
       "4  0.33503  0.0  0.00000  \n",
       "5  0.04593  0.0  0.00041  \n",
       "\n",
       "[5 rows x 306 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df_drop95 = x_df.loc[:, (x_df == 0).sum(axis=0) / len(data1) < 0.95] # 某菌株在95%及以上的样本中表达量为0，则剔除该菌株\n",
    "x_df_drop95.head()\n",
    "#x_df_drop0.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>10</th>\n",
       "      <th>18</th>\n",
       "      <th>27</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>43</th>\n",
       "      <th>49</th>\n",
       "      <th>52</th>\n",
       "      <th>60</th>\n",
       "      <th>...</th>\n",
       "      <th>477</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>499</th>\n",
       "      <th>502</th>\n",
       "      <th>509</th>\n",
       "      <th>512</th>\n",
       "      <th>514</th>\n",
       "      <th>517</th>\n",
       "      <th>520</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00485</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.34827</td>\n",
       "      <td>1.77326</td>\n",
       "      <td>0.11415</td>\n",
       "      <td>2.56049</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.79781</td>\n",
       "      <td>0.04930</td>\n",
       "      <td>0.00331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49163</td>\n",
       "      <td>0.00113</td>\n",
       "      <td>0.88323</td>\n",
       "      <td>0.01551</td>\n",
       "      <td>0.00640</td>\n",
       "      <td>0.00833</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.11937</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.09095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00368</td>\n",
       "      <td>0.00565</td>\n",
       "      <td>0.94900</td>\n",
       "      <td>14.52639</td>\n",
       "      <td>0.00345</td>\n",
       "      <td>0.14303</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.25799</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14264</td>\n",
       "      <td>0.00112</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00927</td>\n",
       "      <td>0.00384</td>\n",
       "      <td>0.01157</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.05959</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>5.28096</td>\n",
       "      <td>3.12677</td>\n",
       "      <td>0.04317</td>\n",
       "      <td>1.38858</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>0.71169</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.74649</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02470</td>\n",
       "      <td>0.03396</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.84578</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01080</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01150</td>\n",
       "      <td>0.07418</td>\n",
       "      <td>0.79501</td>\n",
       "      <td>0.15282</td>\n",
       "      <td>0.52605</td>\n",
       "      <td>0.04702</td>\n",
       "      <td>0.60461</td>\n",
       "      <td>0.04329</td>\n",
       "      <td>0.10949</td>\n",
       "      <td>...</td>\n",
       "      <td>1.39704</td>\n",
       "      <td>0.00296</td>\n",
       "      <td>0.76824</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.12486</td>\n",
       "      <td>0.00299</td>\n",
       "      <td>0.79082</td>\n",
       "      <td>0.25715</td>\n",
       "      <td>0.33503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.07911</td>\n",
       "      <td>0.04888</td>\n",
       "      <td>0.02902</td>\n",
       "      <td>0.00114</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.23759</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.14101</td>\n",
       "      <td>0.09645</td>\n",
       "      <td>0.03103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.34831</td>\n",
       "      <td>0.02853</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00445</td>\n",
       "      <td>0.00128</td>\n",
       "      <td>0.03359</td>\n",
       "      <td>0.09195</td>\n",
       "      <td>0.51496</td>\n",
       "      <td>0.04593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       7        10       18        27       33       34       43       49   \\\n",
       "1  0.00485  0.00000  0.34827   1.77326  0.11415  2.56049  0.00000  0.79781   \n",
       "2  0.00368  0.00565  0.94900  14.52639  0.00345  0.14303  0.00000  1.25799   \n",
       "3  0.05959  0.00025  5.28096   3.12677  0.04317  1.38858  0.00195  0.71169   \n",
       "4  0.00000  0.01150  0.07418   0.79501  0.15282  0.52605  0.04702  0.60461   \n",
       "5  0.07911  0.04888  0.02902   0.00114  0.00000  0.23759  0.00000  0.14101   \n",
       "\n",
       "       52       60    ...         477      486      487      499      502  \\\n",
       "1  0.04930  0.00331   ...     0.49163  0.00113  0.88323  0.01551  0.00640   \n",
       "2  0.00000  0.00976   ...     0.14264  0.00112  0.00000  0.00000  0.00000   \n",
       "3  0.00000  0.00385   ...     0.74649  0.00000  0.02470  0.03396  0.00000   \n",
       "4  0.04329  0.10949   ...     1.39704  0.00296  0.76824  0.00000  0.00000   \n",
       "5  0.09645  0.03103   ...     0.34831  0.02853  0.00000  0.00000  0.00445   \n",
       "\n",
       "       509      512      514      517      520  \n",
       "1  0.00833  0.00000  0.11937  0.00000  0.09095  \n",
       "2  0.00927  0.00384  0.01157  0.00000  0.00043  \n",
       "3  0.84578  0.00000  0.01080  0.00000  0.05034  \n",
       "4  0.12486  0.00299  0.79082  0.25715  0.33503  \n",
       "5  0.00128  0.03359  0.09195  0.51496  0.04593  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df_drop50 = x_df.loc[:, (x_df == 0).sum(axis=0) / len(data1) < 0.5] # 某菌株在50%及以上的样本中表达量为0，则剔除该菌株\n",
    "x_df_drop50.head()\n",
    "#x_df_drop0.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat\n",
    "#len(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征选择操作对象：x_df_drop95.values (y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 基于树模型的特征选择\n",
    "可以采用随机森林的算法，通过树的模型训练可以计算每一个属性的重要性。重要性的值可以帮助我们选择出重要的特征。sklearn.ensemble模块包含了两种基于随机决策树的平均算法：RandomForest算法和Extra-Trees算法。这里使用Extra-Trees算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier #导入ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ExtraTreesClassifier()\n",
    "model.fit(x_df_drop95.values, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>16</th>\n",
       "      <th>18</th>\n",
       "      <th>22</th>\n",
       "      <th>25</th>\n",
       "      <th>27</th>\n",
       "      <th>31</th>\n",
       "      <th>43</th>\n",
       "      <th>...</th>\n",
       "      <th>474</th>\n",
       "      <th>476</th>\n",
       "      <th>481</th>\n",
       "      <th>484</th>\n",
       "      <th>494</th>\n",
       "      <th>496</th>\n",
       "      <th>499</th>\n",
       "      <th>509</th>\n",
       "      <th>512</th>\n",
       "      <th>520</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.34827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37097</td>\n",
       "      <td>1.77326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00125</td>\n",
       "      <td>0.01551</td>\n",
       "      <td>0.00833</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.09095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00341</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.32866</td>\n",
       "      <td>0.94900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>14.52639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40418</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00927</td>\n",
       "      <td>0.00384</td>\n",
       "      <td>0.00043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.28096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.12677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00138</td>\n",
       "      <td>0.03396</td>\n",
       "      <td>0.84578</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.07418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.79501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00946</td>\n",
       "      <td>0.08801</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.12486</td>\n",
       "      <td>0.00299</td>\n",
       "      <td>0.33503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00039</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.64515</td>\n",
       "      <td>0.00114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00128</td>\n",
       "      <td>0.03359</td>\n",
       "      <td>0.04593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   5        8        9        16       18   22       25        27   31   \\\n",
       "1  0.0  0.00000  0.00000  0.00000  0.34827  0.0  0.37097   1.77326  0.0   \n",
       "2  0.0  0.00341  0.00000  0.32866  0.94900  0.0  0.00000  14.52639  0.0   \n",
       "3  0.0  0.00000  0.00000  0.00000  5.28096  0.0  0.00000   3.12677  0.0   \n",
       "4  0.0  0.00000  0.00000  0.00000  0.07418  0.0  0.00000   0.79501  0.0   \n",
       "5  0.0  0.00000  0.00039  0.00000  0.02902  0.0  0.64515   0.00114  0.0   \n",
       "\n",
       "       43    ...        474  476  481  484      494      496      499  \\\n",
       "1  0.00000   ...     0.0000  0.0  0.0  0.0  0.00000  0.00125  0.01551   \n",
       "2  0.00000   ...     0.0000  0.0  0.0  0.0  0.40418  0.00000  0.00000   \n",
       "3  0.00195   ...     0.0000  0.0  0.0  0.0  0.00000  0.00138  0.03396   \n",
       "4  0.04702   ...     0.0037  0.0  0.0  0.0  0.00946  0.08801  0.00000   \n",
       "5  0.00000   ...     0.0000  0.0  0.0  0.0  0.00000  0.00000  0.00000   \n",
       "\n",
       "       509      512      520  \n",
       "1  0.00833  0.00000  0.09095  \n",
       "2  0.00927  0.00384  0.00043  \n",
       "3  0.84578  0.00000  0.05034  \n",
       "4  0.12486  0.00299  0.33503  \n",
       "5  0.00128  0.03359  0.04593  \n",
       "\n",
       "[5 rows x 73 columns]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df_drop_extra_trees = x_df_drop95.loc[:, model.feature_importances_ > model.feature_importances_.mean()*1.5]#*0.75]\n",
    "x_df_drop_extra_trees.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 递归特征消除法\n",
    "递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。它使用模型精度来识别哪些属性（和属性组合）对预测目标属性的贡献最大。可以通过sklearn库中的RFE来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE #导入RFE库\n",
    "from sklearn.linear_model import LogisticRegression #导入逻辑回归库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression() #设置算法为逻辑回归\n",
    "rfe = RFE(model, 50)  #选择50个最佳特征变量，并进行RFE\n",
    "fit = rfe.fit(x_df_drop95.values, y_hat) #进行RFE递归\n",
    "##print(fit.n_features_)  #打印最优特征变量数\n",
    "#print(fit.support_)  #打印选择的最优特征变量\n",
    "##print(fit.ranking_) #特征消除排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>15</th>\n",
       "      <th>25</th>\n",
       "      <th>33</th>\n",
       "      <th>43</th>\n",
       "      <th>49</th>\n",
       "      <th>60</th>\n",
       "      <th>77</th>\n",
       "      <th>108</th>\n",
       "      <th>122</th>\n",
       "      <th>...</th>\n",
       "      <th>445</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>454</th>\n",
       "      <th>457</th>\n",
       "      <th>470</th>\n",
       "      <th>474</th>\n",
       "      <th>480</th>\n",
       "      <th>514</th>\n",
       "      <th>518</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00485</td>\n",
       "      <td>2.73097</td>\n",
       "      <td>0.37097</td>\n",
       "      <td>0.11415</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.79781</td>\n",
       "      <td>0.00331</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.11551</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02410</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.48198</td>\n",
       "      <td>18.52238</td>\n",
       "      <td>0.79244</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.11937</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00368</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00345</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.25799</td>\n",
       "      <td>0.00976</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.17017</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00541</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>0.07378</td>\n",
       "      <td>7.40952</td>\n",
       "      <td>1.12321</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01157</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.05959</td>\n",
       "      <td>0.85651</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04317</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>0.71169</td>\n",
       "      <td>0.00385</td>\n",
       "      <td>0.00182</td>\n",
       "      <td>0.82452</td>\n",
       "      <td>0.02172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00626</td>\n",
       "      <td>0.03652</td>\n",
       "      <td>3.68212</td>\n",
       "      <td>0.01022</td>\n",
       "      <td>12.17483</td>\n",
       "      <td>1.88297</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01080</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.15282</td>\n",
       "      <td>0.04702</td>\n",
       "      <td>0.60461</td>\n",
       "      <td>0.10949</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.55321</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.60408</td>\n",
       "      <td>1.04129</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.17442</td>\n",
       "      <td>5.19832</td>\n",
       "      <td>1.02485</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.79082</td>\n",
       "      <td>19.33382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.07911</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.64515</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.14101</td>\n",
       "      <td>0.03103</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05492</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01045</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10671</td>\n",
       "      <td>0.91530</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.07651</td>\n",
       "      <td>0.09195</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       7        15       25       33       43       49       60       77   \\\n",
       "1  0.00485  2.73097  0.37097  0.11415  0.00000  0.79781  0.00331  0.00000   \n",
       "2  0.00368  0.00000  0.00000  0.00345  0.00000  1.25799  0.00976  0.00000   \n",
       "3  0.05959  0.85651  0.00000  0.04317  0.00195  0.71169  0.00385  0.00182   \n",
       "4  0.00000  0.00000  0.00000  0.15282  0.04702  0.60461  0.10949  0.00000   \n",
       "5  0.07911  0.00000  0.64515  0.00000  0.00000  0.14101  0.03103  0.00000   \n",
       "\n",
       "       108      122    ...         445      448      449      454       457  \\\n",
       "1  0.11551  0.00000    ...     0.02410  0.00000  0.00000  1.48198  18.52238   \n",
       "2  0.17017  0.00000    ...     0.00541  0.00000  0.08543  0.07378   7.40952   \n",
       "3  0.82452  0.02172    ...     0.00626  0.03652  3.68212  0.01022  12.17483   \n",
       "4  0.55321  0.00000    ...     0.60408  1.04129  0.00000  0.17442   5.19832   \n",
       "5  0.05492  0.00000    ...     0.01045  0.00000  0.00000  0.10671   0.91530   \n",
       "\n",
       "       470     474      480      514       518  \n",
       "1  0.79244  0.0000  0.00000  0.11937   0.00000  \n",
       "2  1.12321  0.0000  0.00000  0.01157   0.00000  \n",
       "3  1.88297  0.0000  0.00000  0.01080   0.00000  \n",
       "4  1.02485  0.0037  0.00000  0.79082  19.33382  \n",
       "5  0.00000  0.0000  0.07651  0.09195   0.00000  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df_drop_rfe = x_df_drop95.loc[:, fit.support_]\n",
    "x_df_drop_rfe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 树模型-特征选择结果测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_df_drop_extra_trees,y_hat,test_size=0.2,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "XGB_model=XGBClassifier()\n",
    "\n",
    "XGB_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89        10\n",
      "           1       0.86      1.00      0.92        12\n",
      "\n",
      "   micro avg       0.91      0.91      0.91        22\n",
      "   macro avg       0.93      0.90      0.91        22\n",
      "weighted avg       0.92      0.91      0.91        22\n",
      "\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "y_prob = XGB_model.predict_proba(X_test)[:,1] \n",
    "y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "XGB_model.score(X_test, y_pred)\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "print(metrics.roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rr_model=RandomForestClassifier()\n",
    "rr_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function argmax in module numpy.core.fromnumeric:\n",
      "\n",
      "argmax(a, axis=None, out=None)\n",
      "    Returns the indices of the maximum values along an axis.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Input array.\n",
      "    axis : int, optional\n",
      "        By default, the index is into the flattened array, otherwise\n",
      "        along the specified axis.\n",
      "    out : array, optional\n",
      "        If provided, the result will be inserted into this array. It should\n",
      "        be of the appropriate shape and dtype.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    index_array : ndarray of ints\n",
      "        Array of indices into the array. It has the same shape as `a.shape`\n",
      "        with the dimension along `axis` removed.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    ndarray.argmax, argmin\n",
      "    amax : The maximum value along a given axis.\n",
      "    unravel_index : Convert a flat index into an index tuple.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    In case of multiple occurrences of the maximum values, the indices\n",
      "    corresponding to the first occurrence are returned.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> a = np.arange(6).reshape(2,3)\n",
      "    >>> a\n",
      "    array([[0, 1, 2],\n",
      "           [3, 4, 5]])\n",
      "    >>> np.argmax(a)\n",
      "    5\n",
      "    >>> np.argmax(a, axis=0)\n",
      "    array([1, 1, 1])\n",
      "    >>> np.argmax(a, axis=1)\n",
      "    array([2, 2])\n",
      "    \n",
      "    Indexes of the maximal elements of a N-dimensional array:\n",
      "    \n",
      "    >>> ind = np.unravel_index(np.argmax(a, axis=None), a.shape)\n",
      "    >>> ind\n",
      "    (1, 2)\n",
      "    >>> a[ind]\n",
      "    5\n",
      "    \n",
      "    >>> b = np.arange(6)\n",
      "    >>> b[1] = 5\n",
      "    >>> b\n",
      "    array([0, 5, 2, 3, 4, 5])\n",
      "    >>> np.argmax(b)  # Only the first occurrence is returned.\n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80        10\n",
      "           1       0.83      0.83      0.83        12\n",
      "\n",
      "   micro avg       0.82      0.82      0.82        22\n",
      "   macro avg       0.82      0.82      0.82        22\n",
      "weighted avg       0.82      0.82      0.82        22\n",
      "\n",
      "0.8166666666666668\n"
     ]
    }
   ],
   "source": [
    "y_prob = rr_model.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \n",
    "y_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\n",
    "# y_prob = rr_model.predict_proba(X_test)\n",
    "# y_pred = np.argmax(axis=1)\n",
    "rr_model.score(X_test, y_pred)\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "print(metrics.roc_auc_score(y_test,y_pred))\n",
    "print(\"log_loss\",metrics.log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 递归特征消除-特征选择结果测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82        10\n",
      "           1       0.80      1.00      0.89        12\n",
      "\n",
      "   micro avg       0.86      0.86      0.86        22\n",
      "   macro avg       0.90      0.85      0.86        22\n",
      "weighted avg       0.89      0.86      0.86        22\n",
      "\n",
      "0.85\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_df_drop_rfe,y_hat,test_size=0.2,random_state=3)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(x_df_drop_rfe,y_hat,test_size=0.2)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "XGB_model=XGBClassifier()\n",
    "\n",
    "XGB_model.fit(X_train,y_train)\n",
    "\n",
    "y_prob = XGB_model.predict_proba(X_test)[:,1] \n",
    "y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "XGB_model.score(X_test, y_pred)\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "print(metrics.roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>15</th>\n",
       "      <th>25</th>\n",
       "      <th>33</th>\n",
       "      <th>43</th>\n",
       "      <th>49</th>\n",
       "      <th>60</th>\n",
       "      <th>77</th>\n",
       "      <th>108</th>\n",
       "      <th>122</th>\n",
       "      <th>...</th>\n",
       "      <th>445</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>454</th>\n",
       "      <th>457</th>\n",
       "      <th>470</th>\n",
       "      <th>474</th>\n",
       "      <th>480</th>\n",
       "      <th>514</th>\n",
       "      <th>518</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.16984</td>\n",
       "      <td>0.27085</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.12912</td>\n",
       "      <td>0.02177</td>\n",
       "      <td>2.11076</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>0.15438</td>\n",
       "      <td>0.01296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00345</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.09500</td>\n",
       "      <td>0.31965</td>\n",
       "      <td>0.82725</td>\n",
       "      <td>0.08917</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.56510</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.05177</td>\n",
       "      <td>0.10641</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10637</td>\n",
       "      <td>0.00156</td>\n",
       "      <td>4.84548</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.49320</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00700</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.27157</td>\n",
       "      <td>0.09257</td>\n",
       "      <td>2.44377</td>\n",
       "      <td>0.35008</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00140</td>\n",
       "      <td>0.10347</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.06540</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.28238</td>\n",
       "      <td>0.00083</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00620</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00152</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.28685</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01006</td>\n",
       "      <td>0.73573</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05342</td>\n",
       "      <td>0.44844</td>\n",
       "      <td>1.06048</td>\n",
       "      <td>0.03988</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00695</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.18278</td>\n",
       "      <td>0.18194</td>\n",
       "      <td>13.27147</td>\n",
       "      <td>2.07341</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00603</td>\n",
       "      <td>0.47008</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.00954</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.38299</td>\n",
       "      <td>0.21314</td>\n",
       "      <td>0.96489</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.17725</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.33288</td>\n",
       "      <td>0.35236</td>\n",
       "      <td>11.08944</td>\n",
       "      <td>1.11798</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01278</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.16424</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10657</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.31910</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01005</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.24068</td>\n",
       "      <td>0.04913</td>\n",
       "      <td>1.13743</td>\n",
       "      <td>0.01558</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.12459</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.43443</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.76124</td>\n",
       "      <td>0.03199</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.68083</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01136</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.87659</td>\n",
       "      <td>2.93204</td>\n",
       "      <td>1.01422</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.88332</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.00914</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.16567</td>\n",
       "      <td>0.00391</td>\n",
       "      <td>15.23092</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00585</td>\n",
       "      <td>0.00361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00746</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02198</td>\n",
       "      <td>0.01760</td>\n",
       "      <td>2.38908</td>\n",
       "      <td>0.49060</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.92949</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.80834</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.11968</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>2.38757</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00217</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.63634</td>\n",
       "      <td>0.04591</td>\n",
       "      <td>13.97461</td>\n",
       "      <td>3.86816</td>\n",
       "      <td>0.00614</td>\n",
       "      <td>0.13239</td>\n",
       "      <td>0.93357</td>\n",
       "      <td>0.00161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.15829</td>\n",
       "      <td>0.00318</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.07945</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00060</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00200</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03978</td>\n",
       "      <td>3.80088</td>\n",
       "      <td>2.56421</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03275</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.00243</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02987</td>\n",
       "      <td>0.41844</td>\n",
       "      <td>0.05890</td>\n",
       "      <td>0.00377</td>\n",
       "      <td>0.09580</td>\n",
       "      <td>0.00627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.77381</td>\n",
       "      <td>0.14293</td>\n",
       "      <td>0.26814</td>\n",
       "      <td>0.08763</td>\n",
       "      <td>4.66471</td>\n",
       "      <td>0.36857</td>\n",
       "      <td>0.06052</td>\n",
       "      <td>0.43057</td>\n",
       "      <td>0.01572</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.05097</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02841</td>\n",
       "      <td>0.29688</td>\n",
       "      <td>0.25346</td>\n",
       "      <td>0.00899</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.09467</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00486</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.69327</td>\n",
       "      <td>2.25559</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.36703</td>\n",
       "      <td>0.12385</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.34154</td>\n",
       "      <td>3.64034</td>\n",
       "      <td>0.15093</td>\n",
       "      <td>0.11626</td>\n",
       "      <td>0.60474</td>\n",
       "      <td>0.58841</td>\n",
       "      <td>0.60785</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.55105</td>\n",
       "      <td>0.03160</td>\n",
       "      <td>0.85810</td>\n",
       "      <td>4.61283</td>\n",
       "      <td>1.30254</td>\n",
       "      <td>1.21417</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.08986</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00070</td>\n",
       "      <td>0.03966</td>\n",
       "      <td>0.38638</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00454</td>\n",
       "      <td>0.05579</td>\n",
       "      <td>2.97709</td>\n",
       "      <td>0.24784</td>\n",
       "      <td>14.30710</td>\n",
       "      <td>0.54116</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.74437</td>\n",
       "      <td>0.05056</td>\n",
       "      <td>8.37144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.01250</td>\n",
       "      <td>0.22890</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01166</td>\n",
       "      <td>0.09455</td>\n",
       "      <td>0.25663</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06534</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00616</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.41341</td>\n",
       "      <td>6.48632</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04923</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>14.86122</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.16718</td>\n",
       "      <td>0.00372</td>\n",
       "      <td>0.43584</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02413</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.12929</td>\n",
       "      <td>6.77752</td>\n",
       "      <td>7.21551</td>\n",
       "      <td>0.03745</td>\n",
       "      <td>1.76506</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.03897</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00296</td>\n",
       "      <td>0.01385</td>\n",
       "      <td>0.26906</td>\n",
       "      <td>0.01442</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00382</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.39929</td>\n",
       "      <td>1.45331</td>\n",
       "      <td>1.71516</td>\n",
       "      <td>0.10711</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02332</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.00305</td>\n",
       "      <td>0.32364</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03190</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.20563</td>\n",
       "      <td>0.00354</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01065</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00720</td>\n",
       "      <td>4.87044</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.31600</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04701</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03382</td>\n",
       "      <td>0.01552</td>\n",
       "      <td>5.76791</td>\n",
       "      <td>0.00493</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00058</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00669</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.19592</td>\n",
       "      <td>0.07926</td>\n",
       "      <td>6.90223</td>\n",
       "      <td>4.09414</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.22597</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00166</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>7.03964</td>\n",
       "      <td>0.04049</td>\n",
       "      <td>0.00452</td>\n",
       "      <td>0.53514</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07448</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.11588</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.29283</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.91542</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.34383</td>\n",
       "      <td>0.20355</td>\n",
       "      <td>0.67696</td>\n",
       "      <td>0.00090</td>\n",
       "      <td>0.85541</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.00983</td>\n",
       "      <td>0.34807</td>\n",
       "      <td>1.05489</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02516</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>3.47063</td>\n",
       "      <td>0.00237</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.07826</td>\n",
       "      <td>0.00698</td>\n",
       "      <td>0.06297</td>\n",
       "      <td>0.00495</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06108</td>\n",
       "      <td>0.03266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05457</td>\n",
       "      <td>0.12042</td>\n",
       "      <td>0.92066</td>\n",
       "      <td>3.71338</td>\n",
       "      <td>4.37244</td>\n",
       "      <td>1.90750</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01544</td>\n",
       "      <td>0.02932</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         7         15       25       33       43        49       60       77   \\\n",
       "105  0.16984   0.27085  0.00000  0.12912  0.02177   2.11076  0.00100  0.00075   \n",
       "102  0.05177   0.10641  0.00000  0.10637  0.00156   4.84548  0.00000  0.00000   \n",
       "7    0.06540   0.00000  0.00000  0.00000  0.00000   1.28238  0.00083  0.00000   \n",
       "6    0.01006   0.73573  0.00000  0.05342  0.44844   1.06048  0.03988  0.00000   \n",
       "92   0.00954   0.00000  0.00000  0.38299  0.21314   0.96489  0.00000  0.00000   \n",
       "83   0.16424   0.00000  0.00000  0.10657  0.00000   1.31910  0.00000  0.00000   \n",
       "41   0.00000   0.00000  0.00000  0.43443  0.00000   0.76124  0.03199  0.00000   \n",
       "62   0.00914   0.00000  0.00000  0.16567  0.00391  15.23092  0.00000  0.00000   \n",
       "24   0.80834   0.00000  0.00000  0.11968  0.07458   2.38757  0.00000  0.00000   \n",
       "13   0.00000   0.00000  0.15829  0.00318  0.00000   2.07945  0.00049  0.00000   \n",
       "109  0.00243   0.00000  0.00000  0.00000  0.02987   0.41844  0.05890  0.00377   \n",
       "26   0.05097   0.00000  0.00000  0.02841  0.29688   0.25346  0.00899  0.00000   \n",
       "93   0.34154   3.64034  0.15093  0.11626  0.60474   0.58841  0.60785  0.00000   \n",
       "51   0.00000   0.00000  0.00000  0.00070  0.03966   0.38638  0.00000  0.00000   \n",
       "100  0.01250   0.22890  0.00000  0.01166  0.09455   0.25663  0.00000  0.00000   \n",
       "81   0.00000  14.86122  0.00000  0.16718  0.00372   0.43584  0.00000  0.02413   \n",
       "43   0.03897   0.00000  0.00000  0.00296  0.01385   0.26906  0.01442  0.00000   \n",
       "16   0.00305   0.32364  0.00000  0.03190  0.00000   4.20563  0.00354  0.00000   \n",
       "56   0.00000   0.04701  0.00000  0.03382  0.01552   5.76791  0.00493  0.00000   \n",
       "10   0.00166   0.00000  0.00000  0.00000  0.00000   7.03964  0.04049  0.00452   \n",
       "97   0.91542   0.00000  0.00000  0.34383  0.20355   0.67696  0.00090  0.85541   \n",
       "68   3.47063   0.00237  0.00000  0.07826  0.00698   0.06297  0.00495  0.00000   \n",
       "\n",
       "         108      122   ...         445      448      449      454       457  \\\n",
       "105  0.15438  0.01296   ...     0.00345  0.00000  0.09500  0.31965   0.82725   \n",
       "102  0.49320  0.00000   ...     0.00700  0.00000  0.27157  0.09257   2.44377   \n",
       "7    0.00000  0.00000   ...     0.00620  0.00000  0.00152  0.00000   0.00000   \n",
       "6    0.00000  0.00000   ...     0.00695  0.00000  0.18278  0.18194  13.27147   \n",
       "92   0.17725  0.00000   ...     0.00000  0.00000  2.33288  0.35236  11.08944   \n",
       "83   0.00000  0.00000   ...     0.01005  0.00000  0.24068  0.04913   1.13743   \n",
       "41   1.68083  0.00000   ...     0.01136  0.00000  0.00000  1.87659   2.93204   \n",
       "62   0.00585  0.00361   ...     0.00746  0.00000  0.02198  0.01760   2.38908   \n",
       "24   0.00000  0.00000   ...     0.00217  0.00000  0.63634  0.04591  13.97461   \n",
       "13   0.00060  0.00000   ...     0.00200  0.00000  0.00000  0.03978   3.80088   \n",
       "109  0.09580  0.00627   ...     0.77381  0.14293  0.26814  0.08763   4.66471   \n",
       "26   0.09467  0.00000   ...     0.00486  0.00000  0.00000  0.00000   4.69327   \n",
       "93   0.00100  0.00000   ...     2.55105  0.03160  0.85810  4.61283   1.30254   \n",
       "51   0.00000  0.00000   ...     0.00454  0.05579  2.97709  0.24784  14.30710   \n",
       "100  0.06534  0.00000   ...     0.00616  0.00000  0.00000  0.41341   6.48632   \n",
       "81   0.00000  0.00000   ...     0.00000  0.00000  2.12929  6.77752   7.21551   \n",
       "43   0.00000  0.00000   ...     0.00382  0.00000  0.39929  1.45331   1.71516   \n",
       "16   0.00000  0.00000   ...     0.01065  0.00000  0.00000  0.00720   4.87044   \n",
       "56   0.00058  0.00000   ...     0.00669  0.00000  0.19592  0.07926   6.90223   \n",
       "10   0.53514  0.00000   ...     0.07448  0.00000  0.00000  0.00000   0.11588   \n",
       "97   0.00000  0.00000   ...     0.00000  0.00000  0.13890  0.00983   0.34807   \n",
       "68   0.06108  0.03266   ...     0.05457  0.12042  0.92066  3.71338   4.37244   \n",
       "\n",
       "         470      474      480      514      518  \n",
       "105  0.08917  0.00000  0.00000  0.56510  0.00000  \n",
       "102  0.35008  0.00000  0.00140  0.10347  0.00000  \n",
       "7    0.00000  0.00000  0.00000  0.28685  0.00000  \n",
       "6    2.07341  0.00000  0.00603  0.47008  0.00000  \n",
       "92   1.11798  0.00000  0.00000  0.01278  0.00000  \n",
       "83   0.01558  0.00000  0.00000  0.12459  0.00000  \n",
       "41   1.01422  0.00000  0.00000  0.88332  0.00000  \n",
       "62   0.49060  0.00000  0.00000  0.92949  0.00000  \n",
       "24   3.86816  0.00614  0.13239  0.93357  0.00161  \n",
       "13   2.56421  0.00000  0.00000  0.03275  0.00000  \n",
       "109  0.36857  0.06052  0.43057  0.01572  0.00000  \n",
       "26   2.25559  0.00000  3.36703  0.12385  0.00000  \n",
       "93   1.21417  0.00000  0.00000  0.08986  0.00000  \n",
       "51   0.54116  0.00000  0.74437  0.05056  8.37144  \n",
       "100  0.00000  0.00000  0.00000  0.04923  0.00000  \n",
       "81   0.03745  1.76506  0.00000  0.00000  0.00000  \n",
       "43   0.10711  0.00000  0.00000  0.02332  0.00000  \n",
       "16   0.00000  0.00000  0.00000  1.31600  0.00000  \n",
       "56   4.09414  0.00000  0.00000  0.22597  0.00000  \n",
       "10   0.00000  0.00000  0.00000  0.29283  0.00000  \n",
       "97   1.05489  0.00000  0.00000  0.02516  0.00000  \n",
       "68   1.90750  0.00000  0.01544  0.02932  0.00000  \n",
       "\n",
       "[22 rows x 50 columns]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rr_model=RandomForestClassifier()\n",
    "rr_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86        10\n",
      "           1       0.91      0.83      0.87        12\n",
      "\n",
      "   micro avg       0.86      0.86      0.86        22\n",
      "   macro avg       0.86      0.87      0.86        22\n",
      "weighted avg       0.87      0.86      0.86        22\n",
      "\n",
      "0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "y_prob = rr_model.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \n",
    "y_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\n",
    "rr_model.score(X_test, y_pred)\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "print(metrics.roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 95% 0 drop-特征选择结果测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_df_drop95,y_hat,test_size=0.2,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82        10\n",
      "           1       0.80      1.00      0.89        12\n",
      "\n",
      "   micro avg       0.86      0.86      0.86        22\n",
      "   macro avg       0.90      0.85      0.86        22\n",
      "weighted avg       0.89      0.86      0.86        22\n",
      "\n",
      "0.85\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "XGB_model=XGBClassifier()\n",
    "\n",
    "XGB_model.fit(X_train,y_train)\n",
    "\n",
    "y_prob = XGB_model.predict_proba(X_test)[:,1] \n",
    "y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "XGB_model.score(X_test, y_pred)\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "print(metrics.roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95        10\n",
      "           1       1.00      0.92      0.96        12\n",
      "\n",
      "   micro avg       0.95      0.95      0.95        22\n",
      "   macro avg       0.95      0.96      0.95        22\n",
      "weighted avg       0.96      0.95      0.95        22\n",
      "\n",
      "0.9583333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftwareOrTools\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rr_model=RandomForestClassifier()\n",
    "rr_model.fit(X_train,y_train)\n",
    "\n",
    "y_prob = rr_model.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \n",
    "y_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\n",
    "rr_model.score(X_test, y_pred)\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "print(metrics.roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50% 0 drop-特征选择结果测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_df_drop50,y_hat,test_size=0.2,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84        10\n",
      "           1       0.85      0.92      0.88        12\n",
      "\n",
      "   micro avg       0.86      0.86      0.86        22\n",
      "   macro avg       0.87      0.86      0.86        22\n",
      "weighted avg       0.87      0.86      0.86        22\n",
      "\n",
      "0.8583333333333333\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "XGB_model=XGBClassifier()\n",
    "\n",
    "XGB_model.fit(X_train,y_train)\n",
    "\n",
    "y_prob = XGB_model.predict_proba(X_test)[:,1] \n",
    "y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "XGB_model.score(X_test, y_pred)\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "print(metrics.roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
